## BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

Prelude:

The major benefit we can gain from language modelling is pretraining: "can 
we leverage huge amounts of raw text, which aren't labeled for any specific"
task to help us train better models for other tasks? That is treat the problem as a transfer learning and fine-tuning problem. The task is an unsupervised learning task thus we need a task to be able to calculate gradients to train our reprensentation. For LM, this task is approximating some density function that being auto-regressively predict the next word given previous words. A shift has come from learning unconditional word vectors (where the word's representation is the same globally) to contextualised ones where the representation is dependent on the sentence context it's found in. GPT uses self-attention Transformer to predict the next word given an attention-weighted representation of all prior words. This is done by multiply a query vector with every word in a sequence, then putting this into a softmax to give you a weight distribution. ELMO uses a deep bi-LSTM to gather contexts of a work k in both directions.

BERT differs by representing a word with full context before and after the word in each of its layers. It uses a bi-directional Transformer unlike OpenAI GPT who's LM context is constrained to its left. 

The LM randomly masks some tokens (Masked Language Model, MLM) from the input and the joint task is to predict the original vocabulary id of the masked word based only on the context (left-to-right, right-to-left). Standard conditional LMs can only be trained left-to-right or right-to-left since "bidirectional would allow each word to indirectly see itself". This is overcome by masking some of the input at random (15% of the time) and predicting only those tokens. The final hidden vectors of the masked tokens are fed into a softmax to create a distribution over the vocabulary. The downside is using this for pre-training, since the [MASK] token is never seen during fine-tuning. This is mitigated by rather than always replacing a random word with [MASK], replace the word with a random word 10% of the time and keep the word unchanged 10% of the time. In my opinion this has a regularising effect on the LM by forcing the transformer to keep a "distributional contextual representation" of each word. 

In addition, BERT tries to do next sentence prediction (NSP). This means capturing the relationship between sentence A and B. 

My Take:

The encoder and decoder are 6 stacked layers but don't share weights. Each encoder is broken down into 2 sub-layers. 

The encoder's input goes through a self-attention layer (which is a layer that helps the encoder look at other words in the input. The outputs are then fed to a FFNN, the same FFNN is independently applied to each position. Each word in each position flows through its own path (dependently within the self-attention layer) independently in the FFNN so this can be run in parallel. 

Self-attention works by allowing the encoder to look at other positions in the input sequence that might better help encode the current word. For each word we create a Query, Key and Value vector. These are created by multiplying the embedding by three matrices that are learnable parameters. Then scores are calculated from all words against the current, which determines how much attention to place on other parts of the input sequence. The score is calculated by taking the dot product of the query vector with the key vector and we do this for every word (so we have scores for q1 . k1, q1 . k2,...,q1 . kn). Then divide the scores by the square root of the dimension of the key vector to stabilise the gradients. Then pass the result through softmax to create a distribution which higlights which word to pay attention to. Then multiply each softmax score with the value vector (which acts as function to select the actual value of the word we want to attend to where the key acts as the index to the value and the query poses the question what is important here). Then the weighted value vectors are summed which produces the output of self-attention at this position. This resulting vector is passed through the FFNN indepdently. In practice this is done in matrix form Q, K, V. 
The matrix form is the same process: create our Q, K, V by multiplying our embedding matrix with our WQ, WK, WV matrices. 

So our summed output vector z = softmax(Q x K.Transpose/sqrt dk) * V.

Self-attention is expanded since it's encoding is dominated by the current word. They use multi-head attention which allows the attention layer to have multiple representation subspaces. With multi-head attention we have multiple sets of Q, K, V matrices (8 attention heads are used so 8 sets for both encoder/decoder). Each set is used to project the input embeddings into different representation subspaces. If we do the same self-attention computation 8 times we end up with 8 different z matrices which the FFNN is not expecting (rather it wants a single matrix, a vector for each word). We concatenate the matrices then multiply them by an additional weight matrix WO, to create a Z matrix.

To account for word order of the input the transformer adds a vector to each input embedding. It learns to determine the position of each word (positional encoding) or the distance between different words in a sequence. By adding these values to embeddings provides meaningful distances between the embedding vectors once they're projected into Q, K, V and during the dot-product attention. The positional encoding follows a pattern. Each row is a word and columns are embedding dimensions. It splits the encoding in half since the left half is generated using sine and right using cosine and they're concatenated to form each vector of the positional encoding.

Each sub-layer that is self-attention then FFNN, has residual connections around it followed by layer normalization. So we add this residual connection, X matrix embedding without SA, to the Z matrix then apply layer norm then pass it through the feed forward (but we also have residual connections around the FFNN). 

The output of the top encoder is transformed into a set of attention vectors K and V. These will be used by each decoder in its encoder-decoder attention layer. The output of each step of decoding is fed to the bottom decoder. We do the same for the decoder as encoder with embeddings and positional encodings. The decoder self-attention is different since it can only attend to earlier positions in the output. This is done by masking future positions (set to negative infinity) before the softmax calculation. 

The encoder-decoder attention works like multi-head SA but it creates its query matrix from the layer below it and takes K's and V's matrix from the output of the encoder. The decoder stack outputs a vector which we pass through a linear layer then softmax. The linear layer is a FC layer that projects the vector of outputs from the decoder into a larger logits vector which is the size of the vocab (probability distribution over vocab). The loss is cross-entropy loss. The model produces outputs one at a time, the model selects the word with the highest probability from the probability distribution and throwing away the rest (greed decoding). Or we can use beam search.
