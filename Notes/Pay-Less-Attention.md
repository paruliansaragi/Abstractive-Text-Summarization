Self-attention is a useful mechanism to build generative models for language and images. It 
determines the importance of context elements by comparing each element to the current time step. 
In this paper, we show that a very lightweight convolution can perform competitively to the best 
reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more 
efficient than self-attention. We predict separate convolution kernels based solely on the current 
time-step in order to determine the importance of context elements. The number of operations required 
by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments 
on large-scale machine translation, language modeling and abstractive summarization show that dynamic 
convolutions improve over strong self-attention models. On the WMT’14 English-German test set dynamic 
convolutions achieve a new state of the art of 29.7 BLEU.1

RNNs integrate context information by updating a hidden state at every time-step, CNNs summarize a 
fixed size context through multiple layers, while as self-attention directly summarizes all context.
Attention assigns context elements attention weights which define a weighted sum over context 
representations (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Chorowski et al., 2015; Luong et al., 2015). 
Source-target attention summarizes information from another sequence such as in machine translation 
while as self-attention operates over the current sequence. Self-attention has been formulated as 
content-based where attention weights are computed by comparing the current time-step to all elements 
in the context (Figure 1a). The ability to compute comparisons over such unrestricted context sizes 
are seen as a key characteristic of self-attention (Vaswani et al., 2017).

However, the ability of self-attention to model long-range dependencies has recently come into 
question (Tang et al., 2018) and the unlimited context size is computationally very challenging 
due to the quadratic complexity in the input length. Furthermore, in practice long sequences 
require the introduction of hierarchies (Liu et al., 2018).

In this paper, we introduce lightweight convolutions which are 
depth-wise separable (Chollet, 2017; Kaiser et al., 2017), softmax-normalized and share weights over 
the channel dimension. The result is a convolution with several orders of magnitude fewer weights 
than a standard non-separable convolution. Different to self-attention, lightweight convolutions reuse 
the same weights for context elements, regardless of the current time-step. Dynamic convolutions build 
on lightweight convolutions by predicting a different convolution kernel at every time-step. 
The kernel is a function of the current time-step only as opposed to the entire context as in 
self-attention. Dynamic convolutions are similar to locally connected 
layers in the sense that the weights change at every position, however, the 
difference is that weights are dynamically generated by the model rather than fixed 
after training (LeCun et al., 1998; Taigman et al., 2014; Chen et al., 2015). 

Our approach also bears similarity to location-based attention which does not access the context to 
determine attention weights, however, we do not directly take the attention weights from the previous 
time-step into account (Chorowski et al., 2015; Luong et al., 2015). Shen et al. (2018b) reduce 
complexity by performing attention within blocks of the input sequence and Shen et al. (2017; 2018c) 
perform more fine-grained attention over each feature. Shen et al. (2018a) and Gong et al. (2018) use 
input-dependent filters for text classification tasks. Our experiments show that lightweight convolutions 
perform competitively to strong self-attention results and that dynamic convolutions can perform even better. 
On WMT English-German translation dynamic convolutions achieve a new state of the art of 29.7 BLEU, 
on WMT English-French they match the best reported result in the literature, and on 
IWSLT German-English dynamic convolutions outperform self-attention by 0.8 BLEU. 
Dynamic convolutions achieve 20% faster runtime than a highly-optimized self-attention baseline. 
For language modeling on the Billion word benchmark dynamic convolutions perform as well as or better than 
self-attention and on CNN-DailyMail abstractive document summarization we outperform a strong self-attention model.

We first outline sequence to sequence learning and self-attention. Our work builds on non-separable 
convolutions as well as depthwise separable convolutions. Sequence to sequence learning maps a source 
sequence to a target sequence via two separate networks such as in machine translation (Sutskever et al., 2014). 
The encoder network computes representations for the source sequence such as an English sentence and the decoder 
network autoregressively generates a target sequence based on the encoder output. 
The self-attention module of Vaswani et al. (2017) applies three projections to the input X ∈ R n×d to 
obtain key (K), query (Q), and value (V) representations, where n is the number of time steps, 
d the input/output dimension (Figure 2a). It also defines a number of heads H where each head can 
learn separate attention weights over dk features and attend to different positions. The module 
computes dot-products between key/query pairs, scales to stabilize training, and then softmax 
normalizes the result. Finally, it computes a weighted sum using the output of the value projection (V): 
Attention(Q, K, V ) = softmax(QKT √ dk )V 

Depthwise convolutions perform a convolution independently over every channel. 
The number of parameters can be reduced from d 2k to dk where k is the kernel width. 
The output O ∈ R n×d of a depthwise convolution with weight W ∈ R d×k for element i and 
output dimension c is defined as: 
Oi,c = DepthwiseConv(X, Wc,: , i, c) = X k j=1 Wc,j · X(i+j−d k+1 2 e),c 
3 LIGHTWEIGHT CONVOLUTIONS In this section, we introduce LightConv, a depthwise convolution 
which shares certain output channels and whose weights are normalized across the 
temporal dimension using a softmax. Compared to...
