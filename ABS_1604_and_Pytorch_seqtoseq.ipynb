{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ABS-1604-and-Pytorch-seqtoseq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paruliansaragi/Abstractive-Text-Summarization/blob/master/ABS_1604_and_Pytorch_seqtoseq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRUJ74V3Aub5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip './bbc-news-summary.zip'\n",
        "!unrar x 'BBC News Summary.rar'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcIpGYc8SnVt",
        "colab_type": "code",
        "outputId": "0bba3e99-158b-4c9e-a8cf-139ba339e0d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-17 07:58:33--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-04-17 07:58:33--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M   884KB/s    in 14m 14s \n",
            "\n",
            "2019-04-17 08:12:48 (986 KB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga1YWkXpKpw4",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Turn to csv and train test split { form-width: \"5px\" }\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def read_lines(file):\n",
        "    \"\"\"Gets the lines from a file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        the lines of text of the input file\n",
        "    \"\"\"\n",
        "    with open(file, 'r', encoding=\"utf-8\", errors=\"replace\") as fd:\n",
        "        first_lines = fd.readlines()\n",
        "    return first_lines\n",
        "\n",
        "def merge_art_sum(art_folder_path, sum_folder_path, category):\n",
        "    \"\"\"Merges first lines of text files in one folder, and\n",
        "    writes combined lines into new output file\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    folder_path : str\n",
        "        String representation of the folder path containing the text files.\n",
        "    output_filename : str\n",
        "        Name of the output file the merged lines will be written to.\n",
        "    \"\"\"\n",
        "    # get all text files\n",
        "    art_txt_files = glob.glob(art_folder_path + \"*.txt\")\n",
        "    sum_txt_files = glob.glob(sum_folder_path + \"*.txt\")\n",
        "    # get first lines; map to each text file (sorted)\n",
        "    art_arr = []\n",
        "    sum_arr = []\n",
        "    for fle in art_txt_files:\n",
        "      # open the file and then call .read() to get the text\n",
        "      art_arr.append(read_lines(fle))\n",
        "    for fl in sum_txt_files:\n",
        "      sum_arr.append(read_lines(fl))\n",
        "    # create dir\n",
        "    #if not os.path.exists(output_dir) : os.makedirs(output_dir)\n",
        "    # return dataframe\n",
        "    art_df = pd.DataFrame({'article':art_arr})\n",
        "    sum_df = pd.DataFrame({'summary':sum_arr})\n",
        "    df = pd.merge(sum_df,art_df,left_index=True,right_index=True)\n",
        "    return df.to_csv(category+'.csv')\n",
        "  \n",
        "categories = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
        "\n",
        "def art_or_sum(category):\n",
        "  artpath = './BBC News Summary/News Articles/' + category +'/'\n",
        "  sumpath = './BBC News Summary/Summaries/' + category +'/'\n",
        "  return artpath, sumpath\n",
        "\n",
        "for i in categories:\n",
        "  art, summ = art_or_sum(i)\n",
        "  merge_art_sum(art, summ, i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_rSQlUaF8dz",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Train test test split { form-width: \"1px\" }\n",
        "\n",
        "pol = pd.read_csv('politics.csv')\n",
        "biz = pd.read_csv('business.csv')\n",
        "ent = pd.read_csv('entertainment.csv')\n",
        "tech = pd.read_csv('tech.csv')\n",
        "sport = pd.read_csv('sport.csv')\n",
        "\n",
        "#@title\n",
        "def strip_sum_art(df):\n",
        "  df = df.drop('Unnamed: 0', 1)\n",
        "  df.article = df.article.astype(str).str.replace('\\[|\\]|\\'', '')\n",
        "  df.summary = df.summary.astype(str).str.replace('\\[|\\]|\\'', '')\n",
        "  df = df.replace(r'\\\\n','', regex=True)\n",
        "  df.article = df.article.replace(',', '')\n",
        "  columns_headers = [\"article\", \"summary\"]\n",
        "  df = df.reindex(columns=columns_headers)\n",
        "  return df\n",
        "\n",
        "#@title\n",
        "pol = strip_sum_art(pol)\n",
        "biz = strip_sum_art(biz)\n",
        "ent = strip_sum_art(ent)\n",
        "tech = strip_sum_art(tech)\n",
        "sport = strip_sum_art(sport)\n",
        "\n",
        "#@title\n",
        "df = pol.merge(biz, how='outer')\n",
        "df = df.merge(ent, how='outer')\n",
        "df = df.merge(tech, how='outer')\n",
        "df = df.merge(sport, how='outer')\n",
        "\n",
        "#@title\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "trn, val = train_test_split(df, test_size=0.2)\n",
        "trn, test = train_test_split(trn, test_size=0.2)\n",
        "\n",
        "#@title\n",
        "trn.to_csv('train.csv', index=False)\n",
        "val.to_csv('valid.csv', index=False)\n",
        "test.to_csv('test.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfSAhegiAxBJ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Imports \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import NamedTuple, List, Callable, Dict, Tuple, Optional, Collection\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "import pandas as pd\n",
        "from random import shuffle\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import gzip\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab16uepltIhr",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Params.py { form-width: \"35px\" }\n",
        "from typing import Optional, Union, List\n",
        "\n",
        "\n",
        "class Params:\n",
        "  # Model architecture\n",
        "  vocab_size: int = 30000\n",
        "  hidden_size: int = 150  # of the encoder; default decoder size is doubled if encoder is bidi\n",
        "  dec_hidden_size: Optional[int] = 200  # if set, a matrix will transform enc state into dec state\n",
        "  embed_size: int = 100\n",
        "  enc_bidi: bool = True\n",
        "  enc_attn: bool = True  # decoder has attention over encoder states?\n",
        "  dec_attn: bool = False  # decoder has attention over previous decoder states?\n",
        "  pointer: bool = True  # use pointer network (copy mechanism) in addition to word generator?\n",
        "  out_embed_size: Optional[int] = None  # if set, use an additional layer before decoder output\n",
        "  tie_embed: bool = True  # tie the decoder output layer to the input embedding layer?\n",
        "\n",
        "  # Coverage (to turn on/off, change both `enc_attn_cover` and `cover_loss`)\n",
        "  enc_attn_cover: bool = True  # provide coverage as input when computing enc attn?\n",
        "  cover_func: str = 'max'  # how to aggregate previous attention distributions? sum or max\n",
        "  cover_loss: float = 1  # add coverage loss if > 0; weight of coverage loss as compared to NLLLoss\n",
        "  show_cover_loss: bool = False  # include coverage loss in the loss shown in the progress bar?\n",
        "\n",
        "  # Regularization\n",
        "  enc_rnn_dropout: float = 0\n",
        "  dec_in_dropout: float = 0\n",
        "  dec_rnn_dropout: float = 0\n",
        "  dec_out_dropout: float = 0\n",
        "\n",
        "  # Training\n",
        "  optimizer: str = 'adam'  # adam or adagrad\n",
        "  lr: float = 0.001  # learning rate\n",
        "  adagrad_accumulator: float = 0.1\n",
        "  lr_decay_step: int = 5  # decay lr every how many epochs?\n",
        "  lr_decay: Optional[float] = None  # decay lr by multiplying this factor\n",
        "  #batch_size: int = 32\n",
        "  batch_size: int = 8\n",
        "  #n_batches: int = 1000  # how many batches per epoch\n",
        "  n_batches: int = 250\n",
        "  #val_batch_size: int = 32\n",
        "  val_batch_size: int = 8\n",
        "  n_val_batches: int = 100  # how many validation batches per epoch\n",
        "  #n_epochs: int = 75\n",
        "  n_epochs: int = 5\n",
        "  pack_seq: bool = True  # use packed sequence to skip PAD inputs?\n",
        "  forcing_ratio: float = 0.75  # initial percentage of using teacher forcing\n",
        "  partial_forcing: bool = True  # in a seq, can some steps be teacher forced and some not?\n",
        "  forcing_decay_type: Optional[str] = 'exp'  # linear, exp, sigmoid, or None\n",
        "  forcing_decay: float = 0.9999\n",
        "  sample: bool = True  # are non-teacher forced inputs based on sampling or greedy selection?\n",
        "  grad_norm: float = 1  # use gradient clipping if > 0; max gradient norm\n",
        "  # note: enabling reinforcement learning can significantly slow down training\n",
        "  rl_ratio: float = 0  # use mixed objective if > 0; ratio of RL in the loss function\n",
        "  rl_ratio_power: float = 1  # increase rl_ratio by **= rl_ratio_power after each epoch; (0, 1]\n",
        "  rl_start_epoch: int = 1  # start RL at which epoch (later start can ensure a strong baseline)?\n",
        "\n",
        "  # Data\n",
        "  embed_file: Optional[str] = './glove.6B.50d.txt'  # use pre-trained embeddings\n",
        "  data_path: str = './sentences_aa.txt'\n",
        "  val_data_path: Optional[str] = './sentences_ab.txt'\n",
        "  max_src_len: int = 400  # exclusive of special tokens such as EOS\n",
        "  max_tgt_len: int = 100  # exclusive of special tokens such as EOS\n",
        "  truncate_src: bool = True  # truncate to max_src_len? if false, drop example if too long\n",
        "  truncate_tgt: bool = True  # truncate to max_tgt_len? if false, drop example if too long\n",
        "\n",
        "  # Saving model automatically during training\n",
        "  model_path_prefix: Optional[str] = './checkpoints/m05'\n",
        "  keep_every_epoch: bool = False  # save all epochs, or only the best and the latest one?\n",
        "\n",
        "  # Testing\n",
        "  beam_size: int = 4\n",
        "  min_out_len: int = 60\n",
        "  max_out_len: Optional[int] = 100\n",
        "  out_len_in_words: bool = False\n",
        "  #test_data_path: str = 'data/cnndm.test.gz'\n",
        "  test_sample_ratio: float = 1  # what portion of the test data is used? (1 for all data)\n",
        "  test_save_results: bool = False\n",
        "\n",
        "  def update(self, cmd_args: List[str]):\n",
        "    \"\"\"Update configuration by a list of command line arguments\"\"\"\n",
        "    arg_name = None\n",
        "    for arg_text in cmd_args:\n",
        "      if arg_name is None:\n",
        "        assert arg_text.startswith('--')  # the arg name has to start with \"--\"\n",
        "        arg_name = arg_text[2:]\n",
        "      else:\n",
        "        arg_curr_value = getattr(self, arg_name)\n",
        "        if arg_text.lower() == 'none':\n",
        "          arg_new_value = None\n",
        "        elif arg_text.lower() == 'true':\n",
        "          arg_new_value = True\n",
        "        elif arg_text.lower() == 'false':\n",
        "          arg_new_value = False\n",
        "        else:\n",
        "          arg_type = self.__annotations__[arg_name]\n",
        "          if type(arg_type) is not type:  # support only Optional[T], where T is a basic type\n",
        "            assert arg_type.__origin__ is Union\n",
        "            arg_types = [t for t in arg_type.__args__ if t is not type(None)]\n",
        "            assert len(arg_types) == 1\n",
        "            arg_type = arg_types[0]\n",
        "            assert type(arg_type) is type\n",
        "          arg_new_value = arg_type(arg_text)\n",
        "        setattr(self, arg_name, arg_new_value)\n",
        "        print(\"Hyper-parameter %s = %s (was %s)\" % (arg_name, arg_new_value, arg_curr_value))\n",
        "        arg_name = None\n",
        "    if arg_name is not None:\n",
        "      print(\"Warning: Argument %s lacks a value and is ignored.\" % arg_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxUENHSSb63j",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Vocab Class { form-width: \"40px\" }\n",
        "class Vocab(object):\n",
        "\n",
        "  PAD = 0\n",
        "  SOS = 1\n",
        "  EOS = 2\n",
        "  UNK = 3\n",
        "\n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.word2count = Counter()\n",
        "    self.reserved = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "    self.index2word = self.reserved[:]\n",
        "    self.embeddings = None\n",
        "\n",
        "  def add_words(self, words: List[str]):\n",
        "    for word in words:\n",
        "      if word not in self.word2index:\n",
        "        self.word2index[word] = len(self.index2word)\n",
        "        self.index2word.append(word)\n",
        "    self.word2count.update(words)\n",
        "\n",
        "  def trim(self, *, vocab_size: int=None, min_freq: int=1):\n",
        "    if min_freq <= 1 and (vocab_size is None or vocab_size >= len(self.word2index)):\n",
        "      return\n",
        "    ordered_words = sorted(((c, w) for (w, c) in self.word2count.items()), reverse=True)\n",
        "    if vocab_size:\n",
        "      ordered_words = ordered_words[:vocab_size]\n",
        "    self.word2index = {}\n",
        "    self.word2count = Counter()\n",
        "    self.index2word = self.reserved[:]\n",
        "    for count, word in ordered_words:\n",
        "      if count < min_freq: break\n",
        "      self.word2index[word] = len(self.index2word)\n",
        "      self.word2count[word] = count\n",
        "      self.index2word.append(word)\n",
        "\n",
        "  def load_embeddings(self, file_path: str, dtype=np.float32) -> int:\n",
        "    num_embeddings = 0\n",
        "    vocab_size = len(self)\n",
        "    with open(file_path, 'rb') as f:\n",
        "      for line in f:\n",
        "        line = line.split()\n",
        "        word = line[0].decode('utf-8')\n",
        "        idx = self.word2index.get(word)\n",
        "        if idx is not None:\n",
        "          vec = np.array(line[1:], dtype=dtype)\n",
        "          if self.embeddings is None:\n",
        "            n_dims = len(vec)\n",
        "            self.embeddings = np.random.normal(np.zeros((vocab_size, n_dims))).astype(dtype)\n",
        "            self.embeddings[self.PAD] = np.zeros(n_dims)\n",
        "          self.embeddings[idx] = vec\n",
        "          num_embeddings += 1\n",
        "    return num_embeddings\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    if type(item) is int:\n",
        "      return self.index2word[item]\n",
        "    return self.word2index.get(item, self.UNK)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.index2word)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJRZ1bKRhku7",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "c61e2b94-31ac-4c48-f9a4-0928e263be48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#@title nltk_tokenizer(doc) { form-width: \"40px\" }\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def nltk_tokenizer(doc):\n",
        "  return nltk.word_tokenize(doc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR50sbZmhiFk",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Create Vocab \n",
        "\n",
        "Voc = Vocab()\n",
        "\n",
        "trn = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "val = pd.read_csv('valid.csv')\n",
        "\n",
        "for i in trn.values:\n",
        "  Voc.add_words(nltk_tokenizer(i[0])), Voc.add_words(nltk_tokenizer(i[1]))\n",
        "\n",
        "for i in test.values:\n",
        "  Voc.add_words(nltk_tokenizer(i[0])), Voc.add_words(nltk_tokenizer(i[1]))\n",
        "\n",
        "for i in val.values:\n",
        "  Voc.add_words(nltk_tokenizer(i[0])), Voc.add_words(nltk_tokenizer(i[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09tvYOO3UvwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Voc.load_embeddings('./glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUpTh5delK7I",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Example Class { form-width: \"40px\" }\n",
        "class Example(NamedTuple):\n",
        "  src: List[str]\n",
        "  tgt: List[str]\n",
        "  src_len: int  # inclusive of EOS, so that it corresponds to tensor shape\n",
        "  tgt_len: int  # inclusive of EOS, so that it corresponds to tensor shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFXV9KhOmIRx",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title OOV Dictionary Class { form-width: \"40px\" }\n",
        "\n",
        "class OOVDict(object):\n",
        "\n",
        "  def __init__(self, base_oov_idx):\n",
        "    self.word2index = {}  # type: Dict[Tuple[int, str], int]\n",
        "    self.index2word = {}  # type: Dict[Tuple[int, int], str]\n",
        "    self.next_index = {}  # type: Dict[int, int]\n",
        "    self.base_oov_idx = base_oov_idx\n",
        "    self.ext_vocab_size = base_oov_idx\n",
        "\n",
        "  def add_word(self, idx_in_batch, word) -> int:\n",
        "    key = (idx_in_batch, word)\n",
        "    index = self.word2index.get(key)\n",
        "    if index is not None: return index\n",
        "    index = self.next_index.get(idx_in_batch, self.base_oov_idx)\n",
        "    self.next_index[idx_in_batch] = index + 1\n",
        "    self.word2index[key] = index\n",
        "    self.index2word[(idx_in_batch, index)] = word\n",
        "    self.ext_vocab_size = max(self.ext_vocab_size, index + 1)\n",
        "    return index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzT4eP4tl2_k",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Batch { form-width: \"40px\" }\n",
        "\n",
        "class Batch(NamedTuple):\n",
        "  examples: List[Example]\n",
        "  input_tensor: Optional[torch.Tensor]\n",
        "  target_tensor: Optional[torch.Tensor]\n",
        "  input_lengths: Optional[List[int]]\n",
        "  oov_dict: Optional[OOVDict]\n",
        "\n",
        "  @property\n",
        "  def ext_vocab_size(self):\n",
        "    if self.oov_dict is not None:\n",
        "      return self.oov_dict.ext_vocab_size\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USH8_e19mUfn",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Dataset Class { form-width: \"40px\" }\n",
        "\n",
        "def simple_tokenizer(text: str, lower: bool=False, newline: str=None) -> List[str]:\n",
        "  \"\"\"Split an already tokenized input `text`.\"\"\"\n",
        "  if lower:\n",
        "    text = text.lower()\n",
        "  if newline is not None:  # replace newline by a token\n",
        "    text = text.replace('\\n', ' ' + newline + ' ')\n",
        "  return text.split()\n",
        "\n",
        "class Dataset(object):\n",
        "\n",
        "  def __init__(self, filename: Optional[str], dataframe: Optional[pd.core.frame.DataFrame], tokenize: Callable=simple_tokenizer, max_src_len: int=None,\n",
        "               max_tgt_len: int=None, truncate_src: bool=False, truncate_tgt: bool=False):\n",
        "    \n",
        "    if isinstance(dataframe, pd.DataFrame):\n",
        "      print(\"Reading dataframe ...\")\n",
        "    else:\n",
        "      print(\"Reading dataset %s...\" % filename, end=' ', flush=True)\n",
        "    self.filename = filename\n",
        "    self.pairs = []\n",
        "    self.src_len = 0\n",
        "    self.tgt_len = 0\n",
        "      #requires a file that splits src and tgt by a tab \\t \n",
        "    if filename:\n",
        "      with open(filename, 'rt', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "          pair = line.strip().split('\\t') # pair = (src, tgt)\n",
        "          if len(pair) != 2:\n",
        "            print(\"Line %d of %s is malformed.\" % (i, filename))\n",
        "            continue\n",
        "          src = tokenize(pair[0])\n",
        "          if max_src_len and len(src) > max_src_len:\n",
        "            if truncate_src:\n",
        "              src = src[:max_src_len]\n",
        "            else:\n",
        "              continue\n",
        "          tgt = tokenize(pair[1])\n",
        "          if max_tgt_len and len(tgt) > max_tgt_len:\n",
        "            if truncate_tgt:\n",
        "              tgt = tgt[:max_tgt_len]\n",
        "            else:\n",
        "              continue\n",
        "          src_len = len(src) + 1  # EOS\n",
        "          tgt_len = len(tgt) + 1  # EOS\n",
        "          self.src_len = max(self.src_len, src_len)\n",
        "          self.tgt_len = max(self.tgt_len, tgt_len)\n",
        "          self.pairs.append(Example(src, tgt, src_len, tgt_len))\n",
        "      print(\"%d pairs.\" % len(self.pairs))\n",
        "    else:#from list of strings in separate src, tgt instead of file\n",
        "      for i in dataframe.values:\n",
        "        src = nltk_tokenizer(i[0])\n",
        "        if max_src_len and len(src) > max_src_len:\n",
        "            if truncate_src:\n",
        "              src = src[:max_src_len]\n",
        "            else:\n",
        "              continue\n",
        "              \n",
        "        tgt = nltk_tokenizer(i[1])\n",
        "        if max_tgt_len and len(tgt) > max_tgt_len:\n",
        "            if truncate_tgt:\n",
        "              tgt = tgt[:max_tgt_len]\n",
        "            else:\n",
        "              continue\n",
        "        src_len = len(src) + 1\n",
        "        tgt_len = len(tgt) + 1\n",
        "        self.src_len = max(self.src_len, src_len)\n",
        "        self.tgt_len = max(self.tgt_len, tgt_len)\n",
        "        self.pairs.append(Example(src, tgt, src_len, tgt_len))\n",
        "      print(\"%d pairs.\" % len(self.pairs))\n",
        "\n",
        "  def build_vocab(self, ttv: str, vocab_size: int=None, src: bool=True, tgt: bool=True,\n",
        "                  embed_file: str=None) -> Vocab:\n",
        "    if self.filename:  \n",
        "      filename, _ = os.path.splitext(self.filename)\n",
        "      if vocab_size:\n",
        "        filename += \".%d\" % vocab_size\n",
        "      filename += '.vocab'\n",
        "      if os.path.isfile(filename):\n",
        "        vocab = torch.load(filename)\n",
        "        print(\"Vocabulary loaded, %d words.\" % len(vocab))\n",
        "    else:\n",
        "      print(\"Building vocabulary...\", end=' ', flush=True)\n",
        "      vocab = Vocab()\n",
        "      filename = \"{}\".format(ttv)\n",
        "      filename += \".%d\" % vocab_size\n",
        "      for example in self.pairs:\n",
        "        if src:\n",
        "          vocab.add_words(example.src)\n",
        "        if tgt:\n",
        "          vocab.add_words(example.tgt)\n",
        "      vocab.trim(vocab_size=vocab_size)\n",
        "      print(\"%d words.\" % len(vocab))\n",
        "      torch.save(vocab, filename)\n",
        "    if embed_file:\n",
        "      count = vocab.load_embeddings(embed_file)\n",
        "      print(\"%d pre-trained embeddings loaded.\" % count)\n",
        "    return vocab\n",
        "\n",
        "  def generator(self, batch_size: int, src_vocab: Vocab=None, tgt_vocab: Vocab=None,\n",
        "                ext_vocab: bool=False):\n",
        "    ptr = len(self.pairs)  # make sure to shuffle at first run\n",
        "    if ext_vocab:\n",
        "      assert src_vocab is not None\n",
        "      base_oov_idx = len(src_vocab)\n",
        "    while True:\n",
        "      if ptr + batch_size > len(self.pairs):\n",
        "        shuffle(self.pairs)  # shuffle inplace to save memory\n",
        "        ptr = 0\n",
        "      examples = self.pairs[ptr:ptr + batch_size]\n",
        "      ptr += batch_size\n",
        "      src_tensor, tgt_tensor = None, None\n",
        "      lengths, oov_dict = None, None\n",
        "      if src_vocab or tgt_vocab:\n",
        "        # initialize tensors\n",
        "        if src_vocab:\n",
        "          examples.sort(key=lambda x: -x.src_len)\n",
        "          lengths = [x.src_len for x in examples]\n",
        "          max_src_len = lengths[0]\n",
        "          src_tensor = torch.zeros(max_src_len, batch_size, dtype=torch.long)\n",
        "          if ext_vocab:\n",
        "            oov_dict = OOVDict(base_oov_idx)\n",
        "        if tgt_vocab:\n",
        "          max_tgt_len = max(x.tgt_len for x in examples)\n",
        "          tgt_tensor = torch.zeros(max_tgt_len, batch_size, dtype=torch.long)\n",
        "        # fill up tensors by word indices\n",
        "        for i, example in enumerate(examples):\n",
        "          if src_vocab:\n",
        "            for j, word in enumerate(example.src):\n",
        "              idx = src_vocab[word]\n",
        "              if ext_vocab and idx == src_vocab.UNK:\n",
        "                idx = oov_dict.add_word(i, word)\n",
        "              src_tensor[j, i] = idx\n",
        "            src_tensor[example.src_len - 1, i] = src_vocab.EOS\n",
        "          if tgt_vocab:\n",
        "            for j, word in enumerate(example.tgt):\n",
        "              idx = tgt_vocab[word]\n",
        "              if ext_vocab and idx == src_vocab.UNK:\n",
        "                idx = oov_dict.word2index.get((i, word), idx)\n",
        "              tgt_tensor[j, i] = idx\n",
        "            tgt_tensor[example.tgt_len - 1, i] = tgt_vocab.EOS\n",
        "      yield Batch(examples, src_tensor, tgt_tensor, lengths, oov_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A7rk5GntRaH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Hypothesis Class \n",
        "\n",
        "class Hypothesis(object):\n",
        "\n",
        "  def __init__(self, tokens, log_probs, dec_hidden, dec_states, enc_attn_weights, num_non_words):\n",
        "    self.tokens = tokens  # type: List[int]\n",
        "    self.log_probs = log_probs  # type: List[float]\n",
        "    self.dec_hidden = dec_hidden  # shape: (1, 1, hidden_size)\n",
        "    self.dec_states = dec_states  # list of dec_hidden\n",
        "    self.enc_attn_weights = enc_attn_weights  # list of shape: (1, 1, src_len)\n",
        "    self.num_non_words = num_non_words  # type: int\n",
        "\n",
        "  def __repr__(self):\n",
        "    return repr(self.tokens)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tokens) - self.num_non_words\n",
        "\n",
        "  @property\n",
        "  def avg_log_prob(self):\n",
        "    return sum(self.log_probs) / len(self.log_probs)\n",
        "\n",
        "  def create_next(self, token, log_prob, dec_hidden, add_dec_states, enc_attn, non_word):\n",
        "    return Hypothesis(tokens=self.tokens + [token], log_probs=self.log_probs + [log_prob],\n",
        "                      dec_hidden=dec_hidden, dec_states=\n",
        "                      self.dec_states + [dec_hidden] if add_dec_states else self.dec_states,\n",
        "                      enc_attn_weights=self.enc_attn_weights + [enc_attn]\n",
        "                      if enc_attn is not None else self.enc_attn_weights,\n",
        "                      num_non_words=self.num_non_words + 1 if non_word else self.num_non_words)\n",
        "\n",
        "\n",
        "def show_plot(loss, step=1, val_loss=None, val_metric=None, val_step=1, file_prefix=None):\n",
        "  plt.figure()\n",
        "  fig, ax = plt.subplots(figsize=(12, 8))\n",
        "  # this locator puts ticks at regular intervals\n",
        "  loc = ticker.MultipleLocator(base=0.2)\n",
        "  ax.yaxis.set_major_locator(loc)\n",
        "  ax.set_ylabel('Loss', color='b')\n",
        "  ax.set_xlabel('Batch')\n",
        "  plt.plot(range(step, len(loss) * step + 1, step), loss, 'b')\n",
        "  if val_loss:\n",
        "    plt.plot(range(val_step, len(val_loss) * val_step + 1, val_step), val_loss, 'g')\n",
        "  if val_metric:\n",
        "    ax2 = ax.twinx()\n",
        "    ax2.plot(range(val_step, len(val_metric) * val_step + 1, val_step), val_metric, 'r')\n",
        "    ax2.set_ylabel('ROUGE', color='r')\n",
        "  if file_prefix:\n",
        "    plt.savefig(file_prefix + '.png')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def show_attention_map(src_words, pred_words, attention, pointer_ratio=None):\n",
        "  fig, ax = plt.subplots(figsize=(16, 4))\n",
        "  im = plt.pcolormesh(np.flipud(attention), cmap=\"GnBu\")\n",
        "  # set ticks and labels\n",
        "  ax.set_xticks(np.arange(len(src_words)) + 0.5)\n",
        "  ax.set_xticklabels(src_words, fontsize=14)\n",
        "  ax.set_yticks(np.arange(len(pred_words)) + 0.5)\n",
        "  ax.set_yticklabels(reversed(pred_words), fontsize=14)\n",
        "  if pointer_ratio is not None:\n",
        "    ax1 = ax.twinx()\n",
        "    ax1.set_yticks(np.concatenate([np.arange(0.5, len(pred_words)), [len(pred_words)]]))\n",
        "    ax1.set_yticklabels('%.3f' % v for v in np.flipud(pointer_ratio))\n",
        "    ax1.set_ylabel('Copy probability', rotation=-90, va=\"bottom\")\n",
        "  # let the horizontal axes labelling appear on top\n",
        "  ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
        "  # rotate the tick labels and set their alignment\n",
        "  plt.setp(ax.get_xticklabels(), rotation=-45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "\n",
        "non_word_char_in_word = re.compile(r\"(?<=\\w)\\W(?=\\w)\")\n",
        "not_for_output = {'<PAD>', '<SOS>', '<EOS>', '<UNK>'}\n",
        "\n",
        "def format_tokens(tokens: List[str], newline: str= '<P>', for_rouge: bool=False) -> str:\n",
        "  \"\"\"Join output `tokens` for ROUGE evaluation.\"\"\"\n",
        "  tokens = filter(lambda t: t not in not_for_output, tokens)\n",
        "  if for_rouge:\n",
        "    tokens = [non_word_char_in_word.sub(\"\", t) for t in tokens]  # \"n't\" => \"nt\"\n",
        "  if newline is None:\n",
        "    s = ' '.join(tokens)\n",
        "  else:  # replace newline tokens by newlines\n",
        "    lines, line = [], []\n",
        "    for tok in tokens:\n",
        "      if tok == newline:\n",
        "        if line: lines.append(\" \".join(line))\n",
        "        line = []\n",
        "      else:\n",
        "        line.append(tok)\n",
        "    if line: lines.append(\" \".join(line))\n",
        "    s = '\\n'.join(lines)\n",
        "  return s\n",
        "\n",
        "def format_rouge_scores(rouge_result: Dict[str, float]) -> str:\n",
        "  lines = []\n",
        "  line, prev_metric = [], None\n",
        "  for key in sorted(rouge_result.keys()):\n",
        "    metric = key.rsplit(\"_\", maxsplit=1)[0]\n",
        "    if metric != prev_metric and prev_metric is not None:\n",
        "      lines.append(\"\\t\".join(line))\n",
        "      line = []\n",
        "    line.append(\"%s %s\" % (key, rouge_result[key]))\n",
        "    prev_metric = metric\n",
        "  lines.append(\"\\t\".join(line))\n",
        "  return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "rouge_pattern = re.compile(rb\"(\\d+) ROUGE-(.+) Average_([RPF]): ([\\d.]+) \"\n",
        "                           rb\"\\(95%-conf\\.int\\. ([\\d.]+) - ([\\d.]+)\\)\")\n",
        "\n",
        "def rouge(target: List[List[str]], *predictions: List[List[str]]) -> List[Dict[str, float]]:\n",
        "  \"\"\"Perform single-reference ROUGE evaluation of one or more systems' predictions.\"\"\"\n",
        "  results = [dict() for _ in range(len(predictions))]  # e.g. 0 => 'su4_f' => 0.35\n",
        "  print('Why are we skipping this??')\n",
        "  with TemporaryDirectory() as folder:  # on my server, /tmp is a RAM disk\n",
        "    # write SPL files\n",
        "    eval_entries = []\n",
        "    for i, tgt_tokens in enumerate(target):\n",
        "      sys_entries = []\n",
        "      for j, pred_docs in enumerate(predictions):\n",
        "        sys_file = 'sys%d_%d.spl' % (j, i)\n",
        "        sys_entries.append('\\n    <P ID=\"%d\">%s</P>' % (j, sys_file))\n",
        "        with open(os.path.join(folder, sys_file), 'wt') as f:\n",
        "          f.write(format_tokens(pred_docs[i], for_rouge=True))\n",
        "      ref_file = 'ref_%d.spl' % i\n",
        "      with open(os.path.join(folder, ref_file), 'wt') as f:\n",
        "        f.write(format_tokens(tgt_tokens, for_rouge=True))\n",
        "      eval_entry = \"\"\"\n",
        "<EVAL ID=\"{1}\">\n",
        "  <PEER-ROOT>{0}</PEER-ROOT>\n",
        "  <MODEL-ROOT>{0}</MODEL-ROOT>\n",
        "  <INPUT-FORMAT TYPE=\"SPL\"></INPUT-FORMAT>\n",
        "  <PEERS>{2}\n",
        "  </PEERS>\n",
        "  <MODELS>\n",
        "    <M ID=\"A\">{3}</M>\n",
        "  </MODELS>\n",
        "</EVAL>\"\"\".format(folder, i, ''.join(sys_entries), ref_file)\n",
        "      eval_entries.append(eval_entry)\n",
        "    # write config file\n",
        "    xml = '<ROUGE-EVAL version=\"1.0\">{0}\\n</ROUGE-EVAL>'.format(\"\".join(eval_entries))\n",
        "    config_path = os.path.join(folder, 'task.xml')\n",
        "    #ROUGE-eval-config-file: Specify the evaluation setup. Three files come with the ROUGE \n",
        "            #evaluation package, i.e. ROUGE-test.xml, verify.xml, and verify-spl.xml are \n",
        "            #good examples.\n",
        "    with open(config_path, 'wt') as f:\n",
        "      f.write(xml)\n",
        "      print('Written config for rouge...{}'.format(config_path))\n",
        "    # run ROUGE\n",
        "    out = subprocess.check_output('./ROUGE-1.5.5.pl -e data -a -n 2 -2 4 -u ' + config_path,\n",
        "                                  shell=True, cwd=os.path.join(this_dir, 'data'))\n",
        "  # parse ROUGE output\n",
        "  for line in out.split(b'\\n'):\n",
        "    match = rouge_pattern.match(line)\n",
        "    if match:\n",
        "      sys_id, metric, rpf, value, low, high = match.groups()\n",
        "      results[int(sys_id)][(metric + b'_' + rpf).decode('utf-8').lower()] = float(value)\n",
        "  return results\n",
        "\n",
        "\n",
        "def rouge_single(example: List[List[str]]) -> List[Dict[str, float]]:\n",
        "  \"\"\"Helper for `rouge_parallel()`.\"\"\"\n",
        "  return rouge(*example)\n",
        "\n",
        "\n",
        "def rouge_parallel(target: List[List[str]], *predictions: List[List[str]]) \\\n",
        "        -> List[List[Dict[str, float]]]:\n",
        "  \"\"\"\n",
        "  Run ROUGE tests in parallel (by Python multi-threading, i.e. multiprocessing.dummy) to obtain\n",
        "  per-document scores. Depending on batch size and hardware, this may be slower or faster than\n",
        "  `rouge()`.\n",
        "  \"\"\"\n",
        "  with Pool() as p:\n",
        "    return p.map(rouge_single, zip(target, *predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATh1wfdig51x",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title To dataframe ~ deprecated ~ keep for reference { form-width: \"5px\" }\n",
        "\n",
        "df = pd.DataFrame({'article': article, 'summary': summary})\n",
        "df.article = df.article.apply(' '.join)\n",
        "df.summary = df.summary.apply(' '.join)\n",
        "df = df.replace('\\n','', regex=True)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "trn, val = train_test_split(df, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Uv2wg4pkXas",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Test.py { form-width: \"5px\" }\n",
        "\n",
        "import torch\n",
        "import tarfile\n",
        "from io import BytesIO\n",
        "from typing import Dict, Tuple, List, Union, Optional\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def decode_batch_output(decoded_tokens, vocab: Vocab, oov_dict: OOVDict) -> List[List[str]]:\n",
        "  \"\"\"Convert word indices to strings.\"\"\"\n",
        "  decoded_batch = []\n",
        "  if not isinstance(decoded_tokens, list):\n",
        "    decoded_tokens = decoded_tokens.transpose(0, 1).tolist()\n",
        "  for i, doc in enumerate(decoded_tokens):\n",
        "    decoded_doc = []\n",
        "    for word_idx in doc:\n",
        "      if word_idx >= len(vocab):\n",
        "        word = oov_dict.index2word.get((i, word_idx), '<UNK>')\n",
        "      else:\n",
        "        word = vocab[word_idx]\n",
        "      decoded_doc.append(word)\n",
        "      if word_idx == vocab.EOS:\n",
        "        break\n",
        "    decoded_batch.append(decoded_doc)\n",
        "  return decoded_batch\n",
        "\n",
        "\n",
        "def decode_batch(batch: Batch, model: Seq2Seq, vocab: Vocab, criterion=None, *, pack_seq=True,\n",
        "                 show_cover_loss=False) -> Tuple[List[List[str]], Seq2SeqOutput]:\n",
        "  \"\"\"Test the `model` on the `batch`, return the decoded textual tokens and the Seq2SeqOutput.\"\"\"\n",
        "  if not pack_seq:\n",
        "    input_lengths = None\n",
        "  else:\n",
        "    input_lengths = batch.input_lengths\n",
        "  with torch.no_grad():\n",
        "    input_tensor = batch.input_tensor.to(DEVICE)\n",
        "    if batch.target_tensor is None or criterion is None:\n",
        "      target_tensor = None\n",
        "    else:\n",
        "      target_tensor = batch.target_tensor.to(DEVICE)\n",
        "    out = model(input_tensor, target_tensor, input_lengths, criterion,\n",
        "                ext_vocab_size=batch.ext_vocab_size, include_cover_loss=show_cover_loss)\n",
        "    decoded_batch = decode_batch_output(out.decoded_tokens, vocab, batch.oov_dict)\n",
        "  target_length = batch.target_tensor.size(0)\n",
        "  out.loss_value /= target_length\n",
        "  return decoded_batch, out\n",
        "\n",
        "\n",
        "def decode_one(*args, **kwargs):\n",
        "  \"\"\"\n",
        "  Same as `decode_batch()` but because batch size is 1, the batch dim in visualization data is\n",
        "  eliminated.\n",
        "  \"\"\"\n",
        "  decoded_batch, out = decode_batch(*args, **kwargs)\n",
        "  decoded_doc = decoded_batch[0]\n",
        "  if out.enc_attn_weights is not None:\n",
        "    out.enc_attn_weights = out.enc_attn_weights[:len(decoded_doc), 0, :]\n",
        "  if out.ptr_probs is not None:\n",
        "    out.ptr_probs = out.ptr_probs[:len(decoded_doc), 0]\n",
        "  return decoded_doc, out\n",
        "\n",
        "\n",
        "def eval_batch(batch: Batch, model: Seq2Seq, vocab: Vocab, criterion=None, *, pack_seq=True,\n",
        "               show_cover_loss=False) -> Tuple[float, float]:\n",
        "  \"\"\"Test the `model` on the `batch`, return the ROUGE score and the loss.\"\"\"\n",
        "  decoded_batch, out = decode_batch(batch, model, vocab, criterion=criterion, pack_seq=pack_seq,\n",
        "                                    show_cover_loss=show_cover_loss)\n",
        "  examples = batch[0]\n",
        "  gold_summaries = [ex.tgt for ex in examples]\n",
        "  scores = rouge(gold_summaries, decoded_batch)\n",
        "  return out.loss_value, scores[0]['l_f']\n",
        "\n",
        "\n",
        "def eval_batch_output(tgt_tensor_or_tokens: Union[torch.Tensor, List[List[str]]], vocab: Vocab,\n",
        "                      oov_dict: OOVDict, *pred_tensors: torch.Tensor) -> List[Dict[str, float]]:\n",
        "  \"\"\"\n",
        "  :param tgt_tensor_or_tokens: the gold standard, either as indices or textual tokens\n",
        "  :param vocab: the fixed-size vocab\n",
        "  :param oov_dict: out-of-vocab dict\n",
        "  :param pred_tensors: one or more systems' prediction (output tensors)\n",
        "  :return: two-level score lookup (system index => ROUGE metric => value)\n",
        "  Evaluate one or more systems' output.\n",
        "  \"\"\"\n",
        "  decoded_batch = [decode_batch_output(pred_tensor, vocab, oov_dict)\n",
        "                   for pred_tensor in pred_tensors]\n",
        "  if isinstance(tgt_tensor_or_tokens, torch.Tensor):\n",
        "    gold_summaries = decode_batch_output(tgt_tensor_or_tokens, vocab, oov_dict)\n",
        "  else:\n",
        "    gold_summaries = tgt_tensor_or_tokens\n",
        "  scores = rouge(gold_summaries, *decoded_batch)\n",
        "  return scores\n",
        "\n",
        "\n",
        "def eval_bs_batch(batch: Batch, model: Seq2Seq, vocab: Vocab, *, pack_seq=True, beam_size=4,\n",
        "                  min_out_len=1, max_out_len=None, len_in_words=True, best_only=True,\n",
        "                  details: bool=True) -> Tuple[Optional[List[Dict[str, float]]], Optional[str]]:\n",
        "  \"\"\"\n",
        "  :param batch: a test batch of a single example\n",
        "  :param model: a trained summarizer\n",
        "  :param vocab: vocabulary of the trained summarizer\n",
        "  :param pack_seq: currently has no effect as batch size is 1\n",
        "  :param beam_size: the beam size\n",
        "  :param min_out_len: required minimum output length\n",
        "  :param max_out_len: required maximum output length (if None, use the model's own value)\n",
        "  :param len_in_words: if True, count output length in words instead of tokens (i.e. do not count\n",
        "                       punctuations)\n",
        "  :param best_only: if True, run ROUGE only on the best hypothesis instead of all `beam size` many\n",
        "  :param details: if True, also return a string containing the result of this document\n",
        "  :return: two-level score lookup (hypothesis index => ROUGE metric => value)\n",
        "  Test a trained summarizer on a document using the beam search decoder.\n",
        "  \"\"\"\n",
        "  assert len(batch.examples) == 1\n",
        "  with torch.no_grad():\n",
        "    input_tensor = batch.input_tensor.to(DEVICE)\n",
        "    hypotheses = model.beam_search(input_tensor, batch.input_lengths if pack_seq else None,\n",
        "                                   batch.ext_vocab_size, beam_size, min_out_len=min_out_len,\n",
        "                                   max_out_len=max_out_len, len_in_words=len_in_words)\n",
        "  if best_only:\n",
        "    to_decode = [hypotheses[0].tokens]\n",
        "  else:\n",
        "    to_decode = [h.tokens for h in hypotheses]\n",
        "  decoded_batch = decode_batch_output(to_decode, vocab, batch.oov_dict)\n",
        "  if details:\n",
        "    file_content = \"[System Summary]\\n\" + format_tokens(decoded_batch[0])\n",
        "  else:\n",
        "    file_content = None\n",
        "  if batch.examples[0].tgt is not None:  # run ROUGE if gold standard summary exists\n",
        "  #  gold_summaries = [batch.examples[0].tgt for _ in range(len(decoded_batch))]\n",
        "  #  scores = rouge(gold_summaries, decoded_batch)\n",
        "    if details:\n",
        "      file_content += \"\\n\\n\\n[Reference Summary]\\n\" + format_tokens(batch.examples[0].tgt)\n",
        "  #    file_content += \"\\n\\n\\n[ROUGE Scores]\\n\" + format_rouge_scores(scores[0]) + \"\\n\"\n",
        "  else:\n",
        "    scores = None\n",
        "  if details:\n",
        "    file_content += \"\\n\\n\\n[Source Text]\\n\" + format_tokens(batch.examples[0].src)\n",
        "  return scores, file_content\n",
        "\n",
        "\n",
        "def eval_bs(test_set: Dataset, vocab: Vocab, model: Seq2Seq, params: Params):\n",
        "  test_gen = test_set.generator(1, vocab, None, True if params.pointer else False)\n",
        "  n_samples = int(params.test_sample_ratio * len(test_set.pairs))\n",
        "\n",
        "  if params.test_save_results and params.model_path_prefix:\n",
        "    result_file = tarfile.open(params.model_path_prefix + \".results.tgz\", 'w:gz')\n",
        "  else:\n",
        "    result_file = None\n",
        "\n",
        "  model.eval()\n",
        "  r1, r2, rl, rsu4 = 0, 0, 0, 0\n",
        "  prog_bar = tqdm(range(1, n_samples + 1))\n",
        "  for i in prog_bar:\n",
        "    batch = next(test_gen)\n",
        "    scores, file_content = eval_bs_batch(batch, model, vocab, pack_seq=params.pack_seq,\n",
        "                                         beam_size=params.beam_size,\n",
        "                                         min_out_len=params.min_out_len,\n",
        "                                         max_out_len=params.max_out_len,\n",
        "                                         len_in_words=params.out_len_in_words,\n",
        "                                         details=result_file is not None)\n",
        "    if file_content:\n",
        "      file_content = file_content.encode('utf-8')\n",
        "      file_info = tarfile.TarInfo(name='%06d.txt' % i)\n",
        "      file_info.size = len(file_content)\n",
        "      result_file.addfile(file_info, fileobj=BytesIO(file_content))\n",
        "    if scores:\n",
        "      r1 += scores[0]['1_f']\n",
        "      r2 += scores[0]['2_f']\n",
        "      rl += scores[0]['l_f']\n",
        "      rsu4 += scores[0]['su4_f']\n",
        "      prog_bar.set_postfix(R1='%.4g' % (r1 / i * 100), R2='%.4g' % (r2 / i * 100),\n",
        "                           RL='%.4g' % (rl / i * 100), RSU4='%.4g' % (rsu4 / i * 100))\n",
        "\n",
        "test_flag=\"nogo\"\n",
        "if test_flag == \"go\":\n",
        "  import argparse\n",
        "  import os.path\n",
        "\n",
        "  #parser = argparse.ArgumentParser(description='Evaluate a summarization model.')\n",
        "  #parser.add_argument('--model', type=str, metavar='M', help='path to the model to be evaluated')\n",
        "  #args, unknown_args = parser.parse_known_args()\n",
        "  \n",
        "  p = Params()\n",
        "  #if unknown_args:  # allow command line args to override params.py\n",
        "  #  p.update(unknown_args)\n",
        "\n",
        "  if args.model:  # evaluate a specific model\n",
        "    filename = args.model\n",
        "  else:  # evaluate the best model\n",
        "    train_status = torch.load(p.model_path_prefix + \".train.pt\")\n",
        "    filename = '%s.%02d.pt' % (p.model_path_prefix, train_status['best_epoch_so_far'])\n",
        "\n",
        "  print(\"Evaluating %s...\" % filename)\n",
        "  m = torch.load(filename)  # use map_location='cpu' if you are testing a CUDA model using CPU\n",
        "\n",
        "  m.encoder.gru.flatten_parameters()\n",
        "  m.decoder.gru.flatten_parameters()\n",
        "\n",
        "  if hasattr(m, 'vocab'):\n",
        "    v = m.vocab\n",
        "  else:  # fixes for models trained by a previous version of the summarizer\n",
        "    filename, _ = os.path.splitext(p.data_path)\n",
        "    if p.vocab_size:\n",
        "      filename += \".%d\" % p.vocab_size\n",
        "    filename += '.vocab'\n",
        "    v = torch.load(filename)\n",
        "    m.vocab = v\n",
        "    m.max_dec_steps = m.max_output_length\n",
        "\n",
        "  d = Dataset(p.test_data_path)\n",
        "  eval_bs(d, v, m, p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRwRVSe1re2m",
        "colab_type": "code",
        "outputId": "66aff6d1-a50a-485d-8696-6f3cc690fb9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "trn_ds = Dataset(filename=None, dataframe=trn, max_src_len=400, \n",
        "                 max_tgt_len=100, truncate_src=True, truncate_tgt=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading dataframe ...\n",
            "1424 pairs.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLS9UJGLWkf2",
        "colab_type": "code",
        "outputId": "837e5018-d118-4c06-d099-ce5733989a22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "trn_ds.build_vocab(ttv='train',vocab_size=Par.vocab_size, embed_file='./glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building vocabulary... 30004 words.\n",
            "15052 pre-trained embeddings loaded.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Vocab at 0x7f49968907f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq93HGv0u1hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples = trn_ds.pairs[0:0 + 1]\n",
        "examples.sort(key=lambda x: -x.src_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT4aNFfqx9Dy",
        "colab_type": "text"
      },
      "source": [
        "Todo, create batches that we can yield"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs1JR-iowAWi",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "\n",
        "src_tensor = torch.zeros(400, 1, dtype=torch.long)\n",
        "max_tgt_len = max(x.tgt_len for x in examples)\n",
        "tgt_tensor = torch.zeros(max_tgt_len, 1, dtype=torch.long)\n",
        "lengths = [x.src_len for x in examples]\n",
        "for i, example in enumerate(examples):\n",
        "  for j, word in enumerate(example.src):\n",
        "    \n",
        "    idx = Voc[word]\n",
        "    src_tensor[j, i] = idx\n",
        "    tgt_tensor[j, i] = idx\n",
        "  src_tensor[example.src_len - 1, i] = src_vocab.EOS\n",
        "  tgt_tensor[example.tgt_len - 1, i] = tgt_vocab.EOS\n",
        "yield Batch(examples, src_tensor, tgt_tensor, lengths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXVD2bilscOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_gen = trn_ds.generator(batch_size=8, src_vocab=Voc, tgt_vocab=Voc, ext_vocab=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1jqXvIutB8w",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Model.py { form-width: \"15px\" }\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import random\n",
        "#from params import Params\n",
        "#from utils import Vocab, Hypothesis, word_detector\n",
        "from typing import Union, List\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "eps = 1e-31\n",
        "\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_size, hidden_size, bidi=True, *, rnn_drop: float=0):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_directions = 2 if bidi else 1\n",
        "    self.gru = nn.GRU(embed_size, hidden_size, bidirectional=bidi, dropout=rnn_drop)\n",
        "\n",
        "  def forward(self, embedded, hidden, input_lengths=None):\n",
        "    \"\"\"\n",
        "    :param embedded: (src seq len, batch size, embed size)\n",
        "    :param hidden: (num directions, batch size, encoder hidden size)\n",
        "    :param input_lengths: list containing the non-padded length of each sequence in this batch;\n",
        "                          if set, we use `PackedSequence` to skip the PAD inputs and leave the\n",
        "                          corresponding encoder states as zeros\n",
        "    :return: (src seq len, batch size, hidden size * num directions = decoder hidden size)\n",
        "    Perform multi-step encoding.\n",
        "    \"\"\"\n",
        "    if input_lengths is not None:\n",
        "      embedded = pack_padded_sequence(embedded, input_lengths)\n",
        "\n",
        "    output, hidden = self.gru(embedded, hidden)\n",
        "\n",
        "    if input_lengths is not None:\n",
        "      output, _ = pad_packed_sequence(output)\n",
        "\n",
        "    if self.num_directions > 1:\n",
        "      # hidden: (num directions, batch, hidden) => (1, batch, hidden * 2)\n",
        "      batch_size = hidden.size(1)\n",
        "      hidden = hidden.transpose(0, 1).contiguous().view(1, batch_size,\n",
        "                                                        self.hidden_size * self.num_directions)\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    return torch.zeros(self.num_directions, batch_size, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embed_size, hidden_size, *, enc_attn=True, dec_attn=True,\n",
        "               enc_attn_cover=True, pointer=True, tied_embedding=None, out_embed_size=None,\n",
        "               in_drop: float=0, rnn_drop: float=0, out_drop: float=0, enc_hidden_size=None):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.combined_size = self.hidden_size\n",
        "    self.enc_attn = enc_attn\n",
        "    self.dec_attn = dec_attn\n",
        "    self.enc_attn_cover = enc_attn_cover\n",
        "    self.pointer = pointer\n",
        "    self.out_embed_size = out_embed_size\n",
        "    if tied_embedding is not None and self.out_embed_size and embed_size != self.out_embed_size:\n",
        "      print(\"Warning: Output embedding size %d is overriden by its tied embedding size %d.\"\n",
        "            % (self.out_embed_size, embed_size))\n",
        "      self.out_embed_size = embed_size\n",
        "\n",
        "    self.in_drop = nn.Dropout(in_drop) if in_drop > 0 else None\n",
        "    self.gru = nn.GRU(embed_size, self.hidden_size, dropout=rnn_drop)\n",
        "\n",
        "    if enc_attn:\n",
        "      if not enc_hidden_size: enc_hidden_size = self.hidden_size\n",
        "      self.enc_bilinear = nn.Bilinear(self.hidden_size, enc_hidden_size, 1)\n",
        "      self.combined_size += enc_hidden_size\n",
        "      if enc_attn_cover:\n",
        "        self.cover_weight = nn.Parameter(torch.rand(1))\n",
        "\n",
        "    if dec_attn:\n",
        "      self.dec_bilinear = nn.Bilinear(self.hidden_size, self.hidden_size, 1)\n",
        "      self.combined_size += self.hidden_size\n",
        "\n",
        "    self.out_drop = nn.Dropout(out_drop) if out_drop > 0 else None\n",
        "    if pointer:\n",
        "      self.ptr = nn.Linear(self.combined_size, 1)\n",
        "\n",
        "    if tied_embedding is not None and embed_size != self.combined_size:\n",
        "      # use pre_out layer if combined size is different from embedding size\n",
        "      self.out_embed_size = embed_size\n",
        "\n",
        "    if self.out_embed_size:  # use pre_out layer\n",
        "      self.pre_out = nn.Linear(self.combined_size, self.out_embed_size)\n",
        "      size_before_output = self.out_embed_size\n",
        "    else:  # don't use pre_out layer\n",
        "      size_before_output = self.combined_size\n",
        "\n",
        "    self.out = nn.Linear(size_before_output, vocab_size)\n",
        "    if tied_embedding is not None:\n",
        "      self.out.weight = tied_embedding.weight\n",
        "\n",
        "  def forward(self, embedded, hidden, encoder_states=None, decoder_states=None, coverage_vector=None, *,\n",
        "              encoder_word_idx=None, ext_vocab_size: int=None, log_prob: bool=True):\n",
        "    \"\"\"\n",
        "    :param embedded: (batch size, embed size)\n",
        "    :param hidden: (1, batch size, decoder hidden size)\n",
        "    :param encoder_states: (src seq len, batch size, hidden size), for attention mechanism\n",
        "    :param decoder_states: (past dec steps, batch size, hidden size), for attention mechanism\n",
        "    :param encoder_word_idx: (src seq len, batch size), for pointer network\n",
        "    :param ext_vocab_size: the dynamic vocab size, determined by the max num of OOV words contained\n",
        "                           in any src seq in this batch, for pointer network\n",
        "    :param log_prob: return log probability instead of probability\n",
        "    :return: tuple of four things:\n",
        "             1. word prob or log word prob, (batch size, dynamic vocab size);\n",
        "             2. RNN hidden state after this step, (1, batch size, decoder hidden size);\n",
        "             3. attention weights over encoder states, (batch size, src seq len);\n",
        "             4. prob of copying by pointing as opposed to generating, (batch size, 1)\n",
        "    Perform single-step decoding.\n",
        "    \"\"\"\n",
        "    batch_size = embedded.size(0)\n",
        "    combined = torch.zeros(batch_size, self.combined_size, device=DEVICE)\n",
        "\n",
        "    if self.in_drop: embedded = self.in_drop(embedded)\n",
        "\n",
        "    output, hidden = self.gru(embedded.unsqueeze(0), hidden)  # unsqueeze and squeeze are necessary\n",
        "    combined[:, :self.hidden_size] = output.squeeze(0)        # as RNN expects a 3D tensor (step=1)\n",
        "    offset = self.hidden_size\n",
        "    enc_attn, prob_ptr = None, None  # for visualization\n",
        "\n",
        "    if self.enc_attn or self.pointer:\n",
        "      # energy and attention: (num encoder states, batch size, 1)\n",
        "      num_enc_steps = encoder_states.size(0)\n",
        "      enc_total_size = encoder_states.size(2)\n",
        "      enc_energy = self.enc_bilinear(hidden.expand(num_enc_steps, batch_size, -1).contiguous(),\n",
        "                                     encoder_states)\n",
        "      if self.enc_attn_cover and coverage_vector is not None:\n",
        "        enc_energy += self.cover_weight * torch.log(coverage_vector.transpose(0, 1).unsqueeze(2) + eps)\n",
        "      # transpose => (batch size, num encoder states, 1)\n",
        "      enc_attn = F.softmax(enc_energy, dim=0).transpose(0, 1)\n",
        "      if self.enc_attn:\n",
        "        # context: (batch size, encoder hidden size, 1)\n",
        "        enc_context = torch.bmm(encoder_states.permute(1, 2, 0), enc_attn)\n",
        "        combined[:, offset:offset+enc_total_size] = enc_context.squeeze(2)\n",
        "        offset += enc_total_size\n",
        "      enc_attn = enc_attn.squeeze(2)\n",
        "\n",
        "    if self.dec_attn:\n",
        "      if decoder_states is not None and len(decoder_states) > 0:\n",
        "        dec_energy = self.dec_bilinear(hidden.expand_as(decoder_states).contiguous(),\n",
        "                                       decoder_states)\n",
        "        dec_attn = F.softmax(dec_energy, dim=0).transpose(0, 1)\n",
        "        dec_context = torch.bmm(decoder_states.permute(1, 2, 0), dec_attn)\n",
        "        combined[:, offset:offset + self.hidden_size] = dec_context.squeeze(2)\n",
        "      offset += self.hidden_size\n",
        "\n",
        "    if self.out_drop: combined = self.out_drop(combined)\n",
        "\n",
        "    # generator\n",
        "    if self.out_embed_size:\n",
        "      out_embed = self.pre_out(combined)\n",
        "    else:\n",
        "      out_embed = combined\n",
        "    logits = self.out(out_embed)  # (batch size, vocab size)\n",
        "\n",
        "    # pointer\n",
        "    if self.pointer:\n",
        "      output = torch.zeros(batch_size, ext_vocab_size, device=DEVICE)\n",
        "      # distribute probabilities between generator and pointer\n",
        "      prob_ptr = F.sigmoid(self.ptr(combined))  # (batch size, 1)\n",
        "      #prob_ptr = torch.sigmoid(self.ptr(combined))\n",
        "      prob_gen = 1 - prob_ptr\n",
        "      # add generator probabilities to output\n",
        "      gen_output = F.softmax(logits, dim=1)  # can't use log_softmax due to adding probabilities\n",
        "      output[:, :self.vocab_size] = prob_gen * gen_output\n",
        "      # add pointer probabilities to output\n",
        "      ptr_output = enc_attn\n",
        "      output.scatter_add_(1, encoder_word_idx.transpose(0, 1), prob_ptr * ptr_output)\n",
        "      if log_prob: output = torch.log(output + eps)\n",
        "    else:\n",
        "      if log_prob: output = F.log_softmax(logits, dim=1)\n",
        "      else: output = F.softmax(logits, dim=1)\n",
        "\n",
        "    return output, hidden, enc_attn, prob_ptr\n",
        "\n",
        "\n",
        "class Seq2SeqOutput(object):\n",
        "\n",
        "  def __init__(self, encoder_outputs: torch.Tensor, encoder_hidden: torch.Tensor,\n",
        "               decoded_tokens: torch.Tensor, loss: Union[torch.Tensor, float]=0,\n",
        "               loss_value: float=0, enc_attn_weights: torch.Tensor=None,\n",
        "               ptr_probs: torch.Tensor=None):\n",
        "    self.encoder_outputs = encoder_outputs\n",
        "    self.encoder_hidden = encoder_hidden\n",
        "    self.decoded_tokens = decoded_tokens  # (out seq len, batch size)\n",
        "    self.loss = loss  # scalar\n",
        "    self.loss_value = loss_value  # float value, excluding coverage loss\n",
        "    self.enc_attn_weights = enc_attn_weights  # (out seq len, batch size, src seq len)\n",
        "    self.ptr_probs = ptr_probs  # (out seq len, batch size)\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab: Vocab, params: Params, max_dec_steps=None):\n",
        "    \"\"\"\n",
        "    :param vocab: mainly for info about special tokens and vocab size\n",
        "    :param params: model hyper-parameters\n",
        "    :param max_dec_steps: max num of decoding steps (only effective at test time, as during\n",
        "                          training the num of steps is determined by the `target_tensor`); it is\n",
        "                          safe to change `self.max_dec_steps` as the network architecture is\n",
        "                          independent of src/tgt seq lengths\n",
        "    Create the seq2seq model; its encoder and decoder will be created automatically.\n",
        "    \"\"\"\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.vocab = vocab\n",
        "    self.vocab_size = len(vocab)\n",
        "    if vocab.embeddings is not None:\n",
        "      self.embed_size = vocab.embeddings.shape[1]\n",
        "      if params.embed_size is not None and self.embed_size != params.embed_size:\n",
        "        print(\"Warning: Model embedding size %d is overriden by pre-trained embedding size %d.\"\n",
        "              % (params.embed_size, self.embed_size))\n",
        "      embedding_weights = torch.from_numpy(vocab.embeddings)\n",
        "    else:\n",
        "      self.embed_size = params.embed_size\n",
        "      embedding_weights = None\n",
        "    self.max_dec_steps = params.max_tgt_len + 1 if max_dec_steps is None else max_dec_steps\n",
        "    self.enc_attn = params.enc_attn\n",
        "    self.enc_attn_cover = params.enc_attn_cover\n",
        "    self.dec_attn = params.dec_attn\n",
        "    self.pointer = params.pointer\n",
        "    self.cover_loss = params.cover_loss\n",
        "    self.cover_func = params.cover_func\n",
        "    enc_total_size = params.hidden_size * 2 if params.enc_bidi else params.hidden_size\n",
        "    if params.dec_hidden_size:\n",
        "      dec_hidden_size = params.dec_hidden_size\n",
        "      self.enc_dec_adapter = nn.Linear(enc_total_size, dec_hidden_size)\n",
        "    else:\n",
        "      dec_hidden_size = enc_total_size\n",
        "      self.enc_dec_adapter = None\n",
        "\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.embed_size, padding_idx=vocab.PAD,\n",
        "                                  _weight=embedding_weights)\n",
        "    self.encoder = EncoderRNN(self.embed_size, params.hidden_size, params.enc_bidi,\n",
        "                              rnn_drop=params.enc_rnn_dropout)\n",
        "    self.decoder = DecoderRNN(self.vocab_size, self.embed_size, dec_hidden_size,\n",
        "                              enc_attn=params.enc_attn, dec_attn=params.dec_attn,\n",
        "                              pointer=params.pointer, out_embed_size=params.out_embed_size,\n",
        "                              tied_embedding=self.embedding if params.tie_embed else None,\n",
        "                              in_drop=params.dec_in_dropout, rnn_drop=params.dec_rnn_dropout,\n",
        "                              out_drop=params.dec_out_dropout, enc_hidden_size=enc_total_size)\n",
        "\n",
        "  def filter_oov(self, tensor, ext_vocab_size):\n",
        "    \"\"\"Replace any OOV index in `tensor` with UNK\"\"\"\n",
        "    if ext_vocab_size and ext_vocab_size > self.vocab_size:\n",
        "      result = tensor.clone()\n",
        "      result[tensor >= self.vocab_size] = self.vocab.UNK\n",
        "      return result\n",
        "    return tensor\n",
        "\n",
        "  def get_coverage_vector(self, enc_attn_weights):\n",
        "    \"\"\"Combine the past attention weights into one vector\"\"\"\n",
        "    if self.cover_func == 'max':\n",
        "      coverage_vector, _ = torch.max(torch.cat(enc_attn_weights), dim=0)\n",
        "    elif self.cover_func == 'sum':\n",
        "      coverage_vector = torch.sum(torch.cat(enc_attn_weights), dim=0)\n",
        "    else:\n",
        "      raise ValueError('Unrecognized cover_func: ' + self.cover_func)\n",
        "    return coverage_vector\n",
        "\n",
        "  def forward(self, input_tensor, target_tensor=None, input_lengths=None, criterion=None, *,\n",
        "              forcing_ratio=0, partial_forcing=True, ext_vocab_size=None, sample=False,\n",
        "              saved_out: Seq2SeqOutput=None, visualize: bool=None, include_cover_loss: bool=False)\\\n",
        "          -> Seq2SeqOutput:\n",
        "    \"\"\"\n",
        "    :param input_tensor: tensor of word indices, (src seq len, batch size)\n",
        "    :param target_tensor: tensor of word indices, (tgt seq len, batch size)\n",
        "    :param input_lengths: see explanation in `EncoderRNN`\n",
        "    :param criterion: the loss function; if set, loss will be returned\n",
        "    :param forcing_ratio: see explanation in `Params` (requires `target_tensor`, training only)\n",
        "    :param partial_forcing: see explanation in `Params` (training only)\n",
        "    :param ext_vocab_size: see explanation in `DecoderRNN`\n",
        "    :param sample: if True, the returned `decoded_tokens` will be based on random sampling instead\n",
        "                   of greedily selecting the token of the highest probability at each step\n",
        "    :param saved_out: the output of this function in a previous run; if set, the encoding step will\n",
        "                      be skipped and we reuse the encoder states saved in this object\n",
        "    :param visualize: whether to return data for attention and pointer visualization; if None,\n",
        "                      return if no `criterion` is provided\n",
        "    :param include_cover_loss: whether to include coverage loss in the returned `loss_value`\n",
        "    Run the seq2seq model for training or testing.\n",
        "    \"\"\"\n",
        "    input_length = input_tensor.size(0)\n",
        "    batch_size = input_tensor.size(1)\n",
        "    log_prob = not (sample or self.decoder.pointer)  # don't apply log too soon in these cases\n",
        "    if visualize is None:\n",
        "      visualize = criterion is None\n",
        "    if visualize and not (self.enc_attn or self.pointer):\n",
        "      visualize = False  # nothing to visualize\n",
        "\n",
        "    if target_tensor is None:\n",
        "      target_length = self.max_dec_steps\n",
        "    else:\n",
        "      target_length = target_tensor.size(0)\n",
        "\n",
        "    if forcing_ratio == 1:\n",
        "      # if fully teacher-forced, it may be possible to eliminate the for-loop over decoder steps\n",
        "      # for generality, this optimization is not investigated\n",
        "      use_teacher_forcing = True\n",
        "    elif forcing_ratio > 0:\n",
        "      if partial_forcing:\n",
        "        use_teacher_forcing = None  # decide later individually in each step\n",
        "      else:\n",
        "        use_teacher_forcing = random.random() < forcing_ratio\n",
        "    else:\n",
        "      use_teacher_forcing = False\n",
        "\n",
        "    if saved_out:  # reuse encoder states of a previous run\n",
        "      encoder_outputs = saved_out.encoder_outputs\n",
        "      encoder_hidden = saved_out.encoder_hidden\n",
        "      assert input_length == encoder_outputs.size(0)\n",
        "      assert batch_size == encoder_outputs.size(1)\n",
        "    else:  # run the encoder\n",
        "      encoder_hidden = self.encoder.init_hidden(batch_size)\n",
        "      # encoder_embedded: (input len, batch size, embed size)\n",
        "      encoder_embedded = self.embedding(self.filter_oov(input_tensor, ext_vocab_size))\n",
        "      encoder_outputs, encoder_hidden = \\\n",
        "        self.encoder(encoder_embedded, encoder_hidden, input_lengths)\n",
        "\n",
        "    # initialize return values\n",
        "    r = Seq2SeqOutput(encoder_outputs, encoder_hidden,\n",
        "                      torch.zeros(target_length, batch_size, dtype=torch.long))\n",
        "    if visualize:#Visualize attention\n",
        "      r.enc_attn_weights = torch.zeros(target_length, batch_size, input_length)\n",
        "      if self.pointer:\n",
        "        r.ptr_probs = torch.zeros(target_length, batch_size)\n",
        "\n",
        "    decoder_input = torch.tensor([self.vocab.SOS] * batch_size, device=DEVICE)\n",
        "    if self.enc_dec_adapter is None:\n",
        "      decoder_hidden = encoder_hidden\n",
        "    else:\n",
        "      decoder_hidden = self.enc_dec_adapter(encoder_hidden)\n",
        "    decoder_states = []\n",
        "    enc_attn_weights = []\n",
        "\n",
        "    for di in range(target_length):\n",
        "      decoder_embedded = self.embedding(self.filter_oov(decoder_input, ext_vocab_size))\n",
        "      if enc_attn_weights:\n",
        "        coverage_vector = self.get_coverage_vector(enc_attn_weights)\n",
        "      else:\n",
        "        coverage_vector = None\n",
        "      decoder_output, decoder_hidden, dec_enc_attn, dec_prob_ptr = \\\n",
        "        self.decoder(decoder_embedded, decoder_hidden, encoder_outputs,\n",
        "                     torch.cat(decoder_states) if decoder_states else None, coverage_vector,\n",
        "                     encoder_word_idx=input_tensor, ext_vocab_size=ext_vocab_size,\n",
        "                     log_prob=log_prob)\n",
        "      if self.dec_attn:\n",
        "        decoder_states.append(decoder_hidden)\n",
        "      # save the decoded tokens\n",
        "      if not sample:\n",
        "        _, top_idx = decoder_output.data.topk(1)  # top_idx shape: (batch size, k=1)\n",
        "      else:\n",
        "        prob_distribution = torch.exp(decoder_output) if log_prob else decoder_output\n",
        "        top_idx = torch.multinomial(prob_distribution, 1)\n",
        "      top_idx = top_idx.squeeze(1).detach()  # detach from history as input\n",
        "      r.decoded_tokens[di] = top_idx\n",
        "      # compute loss\n",
        "      if criterion:\n",
        "        if target_tensor is None:\n",
        "          gold_standard = top_idx  # for sampling\n",
        "        else:\n",
        "          gold_standard = target_tensor[di]\n",
        "        if not log_prob:\n",
        "          decoder_output = torch.log(decoder_output + eps)  # necessary for NLLLoss\n",
        "        nll_loss = criterion(decoder_output, gold_standard)\n",
        "        r.loss += nll_loss\n",
        "        r.loss_value += nll_loss.item()\n",
        "      # update attention history and compute coverage loss\n",
        "      if self.enc_attn_cover or (criterion and self.cover_loss > 0):\n",
        "        if coverage_vector is not None and criterion and self.cover_loss > 0:\n",
        "          coverage_loss = torch.sum(torch.min(coverage_vector, dec_enc_attn)) / batch_size \\\n",
        "                          * self.cover_loss\n",
        "          r.loss += coverage_loss\n",
        "          if include_cover_loss: r.loss_value += coverage_loss.item()\n",
        "        enc_attn_weights.append(dec_enc_attn.unsqueeze(0))\n",
        "      # save data for visualization\n",
        "      if visualize:\n",
        "        r.enc_attn_weights[di] = dec_enc_attn.data\n",
        "        if self.pointer:\n",
        "          r.ptr_probs[di] = dec_prob_ptr.squeeze(1).data\n",
        "      # decide the next input\n",
        "      if use_teacher_forcing or (use_teacher_forcing is None and random.random() < forcing_ratio):\n",
        "        decoder_input = target_tensor[di]  # teacher forcing\n",
        "      else:\n",
        "        decoder_input = top_idx\n",
        "    \n",
        "    return r\n",
        "\n",
        "  def beam_search(self, input_tensor, input_lengths=None, ext_vocab_size=None, beam_size=4, *,\n",
        "                  min_out_len=1, max_out_len=None, len_in_words=True) -> List[Hypothesis]:\n",
        "    \"\"\"\n",
        "    :param input_tensor: tensor of word indices, (src seq len, batch size); for now, batch size has\n",
        "                         to be 1\n",
        "    :param input_lengths: see explanation in `EncoderRNN`\n",
        "    :param ext_vocab_size: see explanation in `DecoderRNN`\n",
        "    :param beam_size: the beam size\n",
        "    :param min_out_len: required minimum output length\n",
        "    :param max_out_len: required maximum output length (if None, use the model's own value)\n",
        "    :param len_in_words: if True, count output length in words instead of tokens (i.e. do not count\n",
        "                         punctuations)\n",
        "    :return: list of the best decoded sequences, in descending order of probability\n",
        "    Use beam search to generate summaries.\n",
        "    \"\"\"\n",
        "    batch_size = input_tensor.size(1)\n",
        "    assert batch_size == 1\n",
        "    if max_out_len is None:\n",
        "      max_out_len = self.max_dec_steps - 1  # max_out_len doesn't count EOS\n",
        "\n",
        "    # encode\n",
        "    encoder_hidden = self.encoder.init_hidden(batch_size)\n",
        "    # encoder_embedded: (input len, batch size, embed size)\n",
        "    encoder_embedded = self.embedding(self.filter_oov(input_tensor, ext_vocab_size))\n",
        "    encoder_outputs, encoder_hidden = \\\n",
        "      self.encoder(encoder_embedded, encoder_hidden, input_lengths)\n",
        "    if self.enc_dec_adapter is None:\n",
        "      decoder_hidden = encoder_hidden\n",
        "    else:\n",
        "      decoder_hidden = self.enc_dec_adapter(encoder_hidden)\n",
        "    # turn batch size from 1 to beam size (by repeating)\n",
        "    # if we want dynamic batch size, the following must be created for all possible batch sizes\n",
        "    encoder_outputs = encoder_outputs.expand(-1, beam_size, -1).contiguous()\n",
        "    input_tensor = input_tensor.expand(-1, beam_size).contiguous()\n",
        "\n",
        "    # decode\n",
        "    hypos = [Hypothesis([self.vocab.SOS], [], decoder_hidden, [], [], 1)]\n",
        "    results, backup_results = [], []\n",
        "    step = 0\n",
        "    while hypos and step < 2 * max_out_len:  # prevent infinitely generating punctuations\n",
        "      # make batch size equal to beam size (n_hypos <= beam size)\n",
        "      n_hypos = len(hypos)\n",
        "      if n_hypos < beam_size:\n",
        "        hypos.extend(hypos[-1] for _ in range(beam_size - n_hypos))\n",
        "      # assemble existing hypotheses into a batch\n",
        "      decoder_input = torch.tensor([h.tokens[-1] for h in hypos], device=DEVICE)\n",
        "      decoder_hidden = torch.cat([h.dec_hidden for h in hypos], 1)\n",
        "      if self.dec_attn and step > 0:  # dim 0 is decoding step, dim 1 is beam batch\n",
        "        decoder_states = torch.cat([torch.cat(h.dec_states, 0) for h in hypos], 1)\n",
        "      else:\n",
        "        decoder_states = None\n",
        "      if self.enc_attn_cover:\n",
        "        enc_attn_weights = [torch.cat([h.enc_attn_weights[i] for h in hypos], 1)\n",
        "                            for i in range(step)]\n",
        "      else:\n",
        "        enc_attn_weights = []\n",
        "      if enc_attn_weights:\n",
        "        coverage_vector = self.get_coverage_vector(enc_attn_weights)  # shape: (beam size, src len)\n",
        "      else:\n",
        "        coverage_vector = None\n",
        "      # run the decoder over the assembled batch\n",
        "      decoder_embedded = self.embedding(self.filter_oov(decoder_input, ext_vocab_size))\n",
        "      decoder_output, decoder_hidden, dec_enc_attn, dec_prob_ptr = \\\n",
        "        self.decoder(decoder_embedded, decoder_hidden, encoder_outputs,\n",
        "                     decoder_states, coverage_vector,\n",
        "                     encoder_word_idx=input_tensor, ext_vocab_size=ext_vocab_size)\n",
        "      top_v, top_i = decoder_output.data.topk(beam_size)  # shape of both: (beam size, beam size)\n",
        "      # create new hypotheses\n",
        "      new_hypos = []\n",
        "      for in_idx in range(n_hypos):\n",
        "        for out_idx in range(beam_size):\n",
        "          new_tok = top_i[in_idx][out_idx].item()\n",
        "          new_prob = top_v[in_idx][out_idx].item()\n",
        "          if len_in_words:\n",
        "            non_word = not self.vocab.is_word(new_tok)\n",
        "          else:\n",
        "            non_word = new_tok == self.vocab.EOS  # only SOS & EOS don't count\n",
        "          new_hypo = hypos[in_idx].create_next(new_tok, new_prob,\n",
        "                                               decoder_hidden[0][in_idx].unsqueeze(0).unsqueeze(0),\n",
        "                                               self.dec_attn,\n",
        "                                               dec_enc_attn[in_idx].unsqueeze(0).unsqueeze(0)\n",
        "                                               if dec_enc_attn is not None else None, non_word)\n",
        "          new_hypos.append(new_hypo)\n",
        "      # process the new hypotheses\n",
        "      new_hypos = sorted(new_hypos, key=lambda h: -h.avg_log_prob)\n",
        "      hypos = []\n",
        "      new_complete_results, new_incomplete_results = [], []\n",
        "      for nh in new_hypos:\n",
        "        length = len(nh)\n",
        "        if nh.tokens[-1] == self.vocab.EOS:  # a complete hypothesis\n",
        "          if len(new_complete_results) < beam_size and min_out_len <= length <= max_out_len:\n",
        "            new_complete_results.append(nh)\n",
        "        elif len(hypos) < beam_size and length < max_out_len:  # an incomplete hypothesis\n",
        "          hypos.append(nh)\n",
        "        elif length == max_out_len and len(new_incomplete_results) < beam_size:\n",
        "          new_incomplete_results.append(nh)\n",
        "      if new_complete_results:\n",
        "        results.extend(new_complete_results)\n",
        "      elif new_incomplete_results:\n",
        "        backup_results.extend(new_incomplete_results)\n",
        "      step += 1\n",
        "    if not results:  # if no sequence ends with EOS within desired length, fallback to sequences\n",
        "      results = backup_results  # that are \"truncated\" at the end to max_out_len\n",
        "    return sorted(results, key=lambda h: -h.avg_log_prob)[:beam_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exVwUciRtaRo",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Train.py { form-width: \"5px\" }\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import os\n",
        "from torch import optim\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_batch(batch: Batch, model: Seq2Seq, criterion, optimizer, *,\n",
        "                pack_seq=True, forcing_ratio=0.5, partial_forcing=True, sample=False,\n",
        "                rl_ratio: float=0, vocab=None, grad_norm: float=0, show_cover_loss=False):\n",
        "  if not pack_seq:\n",
        "    input_lengths = None\n",
        "  else:\n",
        "    input_lengths = batch.input_lengths\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  input_tensor = batch.input_tensor.to(DEVICE)\n",
        "  target_tensor = batch.target_tensor.to(DEVICE)\n",
        "  ext_vocab_size = batch.ext_vocab_size\n",
        "\n",
        "  out = model(input_tensor, target_tensor, input_lengths, criterion,\n",
        "              forcing_ratio=forcing_ratio, partial_forcing=partial_forcing, sample=sample,\n",
        "              ext_vocab_size=ext_vocab_size, include_cover_loss=show_cover_loss)\n",
        "\n",
        "  if rl_ratio > 0:\n",
        "    assert vocab is not None\n",
        "    sample_out = model(input_tensor, saved_out=out, criterion=criterion, sample=True,\n",
        "                       ext_vocab_size=ext_vocab_size)\n",
        "    baseline_out = model(input_tensor, saved_out=out, visualize=False,\n",
        "                         ext_vocab_size=ext_vocab_size)\n",
        "    scores = eval_batch_output([ex.tgt for ex in batch.examples], vocab, batch.oov_dict,\n",
        "                               sample_out.decoded_tokens, baseline_out.decoded_tokens)\n",
        "    greedy_rouge = scores[1]['l_f']\n",
        "    neg_reward = greedy_rouge - scores[0]['l_f']\n",
        "    # if sample > baseline, the reward is positive (i.e. good exploration), rl_loss is negative\n",
        "    rl_loss = neg_reward * sample_out.loss\n",
        "    rl_loss_value = neg_reward * sample_out.loss_value\n",
        "    loss = (1 - rl_ratio) * out.loss + rl_ratio * rl_loss\n",
        "    loss_value = (1 - rl_ratio) * out.loss_value + rl_ratio * rl_loss_value\n",
        "  else:\n",
        "    loss = out.loss\n",
        "    loss_value = out.loss_value\n",
        "    greedy_rouge = None\n",
        "\n",
        "  loss.backward()\n",
        "  if grad_norm > 0:\n",
        "    clip_grad_norm_(model.parameters(), grad_norm)\n",
        "  optimizer.step()\n",
        "\n",
        "  target_length = target_tensor.size(0)\n",
        "  return loss_value / target_length, greedy_rouge\n",
        "\n",
        "\n",
        "def train(train_generator, vocab: Vocab, model: Seq2Seq, params: Params, valid_generator=None,\n",
        "          saved_state: dict=None):\n",
        "  # variables for plotting\n",
        "  plot_points_per_epoch = max(math.log(params.n_batches, 1.6), 1.)\n",
        "  plot_every = round(params.n_batches / plot_points_per_epoch)\n",
        "  plot_losses, cached_losses = [], []\n",
        "  plot_val_losses, plot_val_metrics = [], []\n",
        "  #count number of parameters of the model\n",
        "  total_parameters = sum(parameter.numel() for parameter in model.parameters()\n",
        "                         if parameter.requires_grad)\n",
        "  print(\"Training %d trainable parameters...\" % total_parameters)\n",
        "  model.to(DEVICE)\n",
        "  \n",
        "  if saved_state is None:\n",
        "    if params.optimizer == 'adagrad':\n",
        "      optimizer = optim.Adagrad(model.parameters(), lr=params.lr,\n",
        "                                initial_accumulator_value=params.adagrad_accumulator)\n",
        "    else:\n",
        "      optimizer = optim.Adam(model.parameters(), lr=params.lr)\n",
        "    past_epochs = 0\n",
        "    total_batch_count = 0\n",
        "  else:\n",
        "    optimizer = saved_state['optimizer']\n",
        "    past_epochs = saved_state['epoch']\n",
        "    total_batch_count = saved_state['total_batch_count']\n",
        "  if params.lr_decay:\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, params.lr_decay_step, params.lr_decay,\n",
        "                                             past_epochs - 1)\n",
        "  criterion = nn.NLLLoss(ignore_index=vocab.PAD)\n",
        "  best_avg_loss, best_epoch_id = float(\"inf\"), None\n",
        "\n",
        "  for epoch_count in range(1 + past_epochs, params.n_epochs + 1):\n",
        "    if params.lr_decay:\n",
        "      lr_scheduler.step()\n",
        "    rl_ratio = params.rl_ratio if epoch_count >= params.rl_start_epoch else 0\n",
        "    epoch_loss, epoch_metric = 0, 0\n",
        "    epoch_avg_loss, valid_avg_loss, valid_avg_metric = None, None, None\n",
        "    prog_bar = tqdm(range(1, params.n_batches + 1), desc='Epoch %d' % epoch_count)\n",
        "    model.train()\n",
        "\n",
        "    for batch_count in prog_bar:  # training batches\n",
        "      if params.forcing_decay_type:\n",
        "        if params.forcing_decay_type == 'linear':\n",
        "          forcing_ratio = max(0, params.forcing_ratio - params.forcing_decay * total_batch_count)\n",
        "        elif params.forcing_decay_type == 'exp':\n",
        "          forcing_ratio = params.forcing_ratio * (params.forcing_decay ** total_batch_count)\n",
        "        elif params.forcing_decay_type == 'sigmoid':\n",
        "          forcing_ratio = params.forcing_ratio * params.forcing_decay / (\n",
        "                  params.forcing_decay + math.exp(total_batch_count / params.forcing_decay))\n",
        "        else:\n",
        "          raise ValueError('Unrecognized forcing_decay_type: ' + params.forcing_decay_type)\n",
        "      else:\n",
        "        forcing_ratio = params.forcing_ratio\n",
        "\n",
        "      batch = next(train_generator)\n",
        "      loss, metric = train_batch(batch, model, criterion, optimizer, pack_seq=params.pack_seq,\n",
        "                                 forcing_ratio=forcing_ratio,\n",
        "                                 partial_forcing=params.partial_forcing, sample=params.sample,\n",
        "                                 rl_ratio=rl_ratio, vocab=vocab, grad_norm=params.grad_norm,\n",
        "                                 show_cover_loss=params.show_cover_loss)\n",
        "\n",
        "      epoch_loss += float(loss)\n",
        "      epoch_avg_loss = epoch_loss / batch_count\n",
        "      if metric is not None:  # print ROUGE as well if reinforcement learning is enabled\n",
        "        epoch_metric += metric\n",
        "        epoch_avg_metric = epoch_metric / batch_count\n",
        "        prog_bar.set_postfix(loss='%g' % epoch_avg_loss, rouge='%.4g' % (epoch_avg_metric * 100))\n",
        "      else:\n",
        "        prog_bar.set_postfix(loss='%g' % epoch_avg_loss)\n",
        "\n",
        "      cached_losses.append(loss)\n",
        "      total_batch_count += 1\n",
        "      if total_batch_count % plot_every == 0:\n",
        "        period_avg_loss = sum(cached_losses) / len(cached_losses)\n",
        "        plot_losses.append(period_avg_loss)\n",
        "        cached_losses = []\n",
        "\n",
        "    if valid_generator is not None:  # validation batches\n",
        "      valid_loss, valid_metric = 0, 0\n",
        "      prog_bar = tqdm(range(1, params.n_val_batches + 1), desc='Valid %d' % epoch_count)\n",
        "      model.eval()\n",
        "\n",
        "      for batch_count in prog_bar:\n",
        "        batch = next(valid_generator)\n",
        "        loss, metric = eval_batch(batch, model, vocab, criterion, pack_seq=params.pack_seq,\n",
        "                                  show_cover_loss=params.show_cover_loss)\n",
        "        valid_loss += loss\n",
        "        valid_metric += metric\n",
        "        valid_avg_loss = valid_loss / batch_count\n",
        "        valid_avg_metric = valid_metric / batch_count\n",
        "        prog_bar.set_postfix(loss='%g' % valid_avg_loss, rouge='%.4g' % (valid_avg_metric * 100))\n",
        "\n",
        "      plot_val_losses.append(valid_avg_loss)\n",
        "      plot_val_metrics.append(valid_avg_metric)\n",
        "\n",
        "      metric_loss = -valid_avg_metric  # choose the best model by ROUGE instead of loss\n",
        "      if metric_loss < best_avg_loss:\n",
        "        best_epoch_id = epoch_count\n",
        "        best_avg_loss = metric_loss\n",
        "\n",
        "    else:  # no validation, \"best\" is defined by training loss\n",
        "      if epoch_avg_loss < best_avg_loss:\n",
        "        best_epoch_id = epoch_count\n",
        "        best_avg_loss = epoch_avg_loss\n",
        "\n",
        "    if params.model_path_prefix:\n",
        "      # save model\n",
        "      filename = '%s.%02d.pt' % (params.model_path_prefix, epoch_count)\n",
        "      torch.save(model, filename)\n",
        "      if not params.keep_every_epoch:  # clear previously saved models\n",
        "        for epoch_id in range(1 + past_epochs, epoch_count):\n",
        "          if epoch_id != best_epoch_id:\n",
        "            try:\n",
        "              prev_filename = '%s.%02d.pt' % (params.model_path_prefix, epoch_id)\n",
        "              os.remove(prev_filename)\n",
        "            except FileNotFoundError:\n",
        "              pass\n",
        "      # save training status\n",
        "      torch.save({\n",
        "        'epoch': epoch_count,\n",
        "        'total_batch_count': total_batch_count,\n",
        "        'train_avg_loss': epoch_avg_loss,\n",
        "        'valid_avg_loss': valid_avg_loss,\n",
        "        'valid_avg_metric': valid_avg_metric,\n",
        "        'best_epoch_so_far': best_epoch_id,\n",
        "        'params': params,\n",
        "        'optimizer': optimizer\n",
        "      }, '%s.train.pt' % params.model_path_prefix)\n",
        "\n",
        "    if rl_ratio > 0:\n",
        "      params.rl_ratio **= params.rl_ratio_power\n",
        "\n",
        "    show_plot(plot_losses, plot_every, plot_val_losses, plot_val_metrics, params.n_batches,\n",
        "              params.model_path_prefix)\n",
        "\n",
        "isayso ='no'\n",
        "flaggy='no'\n",
        "resume_from = ''\n",
        "if flaggy == \"go\":\n",
        "  import argparse\n",
        "\n",
        "  #parser = argparse.ArgumentParser(description='Train the seq2seq abstractive summarizer.')\n",
        "  #parser.add_argument('--resume_from', type=str, metavar='R',\n",
        "  #                    help='path to a saved training status (*.train.pt)')\n",
        "  #args, unknown_args = parser.parse_known_args()\n",
        "\n",
        "  if resume_from:\n",
        "    print(\"Resuming from %s...\" % resume_from)\n",
        "    train_status = torch.load(resume_from)\n",
        "    m = torch.load('%s.%02d.pt' % (resume_from[:-9], train_status['epoch']))\n",
        "    p = train_status['params']\n",
        "  else:\n",
        "    p = Params()\n",
        "    m = None\n",
        "    train_status = None\n",
        "\n",
        "  #if unknown_args:  # allow command line args to override params.py\n",
        "  #  p.update(unknown_args)\n",
        "\n",
        "  dataset = Dataset(p.data_path, max_src_len=p.max_src_len, max_tgt_len=p.max_tgt_len,\n",
        "                    truncate_src=p.truncate_src, truncate_tgt=p.truncate_tgt)\n",
        "  if m is None:\n",
        "    v = dataset.build_vocab(p.vocab_size, embed_file=p.embed_file)\n",
        "    m = Seq2Seq(v, p)\n",
        "  else:\n",
        "    v = dataset.build_vocab(p.vocab_size)\n",
        "\n",
        "  train_gen = dataset.generator(p.batch_size, v, v, True if p.pointer else False)\n",
        "  #if p.val_data_path and isayso=='go':\n",
        "  #  val_dataset = Dataset(p.val_data_path, max_src_len=p.max_src_len, max_tgt_len=p.max_tgt_len,\n",
        "  #                        truncate_src=p.truncate_src, truncate_tgt=p.truncate_tgt)\n",
        "  #  val_gen = val_dataset.generator(p.val_batch_size, v, v, True if p.pointer else False)\n",
        "  #  print('Validation data path exists... {}'.format(p.val_data_path))\n",
        "  #else:\n",
        "  #  val_gen = None\n",
        "  \n",
        "\n",
        "  train(train_gen, v, m, p, val_gen, train_status)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J5Wen_ptnUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Par = Params()\n",
        "m = Seq2Seq(Voc, Par)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyksgnDKTWFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir checkpoints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po3f68OXtus5",
        "colab_type": "code",
        "outputId": "a004df57-3ae0-4d14-d0b4-fec11f3c378e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "train_status = None\n",
        "train(trn_gen, Voc, m, Par, train_status)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch 1:   0%|          | 0/250 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training 5966244 trainable parameters...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "Epoch 1: 100%|██████████| 250/250 [05:36<00:00,  1.34s/it, loss=2.17137]\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type EncoderRNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type DecoderRNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "Epoch 2: 100%|██████████| 250/250 [05:36<00:00,  1.36s/it, loss=1.92768]\n",
            "Epoch 3: 100%|██████████| 250/250 [05:36<00:00,  1.34s/it, loss=1.83374]\n",
            "Epoch 4: 100%|██████████| 250/250 [05:35<00:00,  1.34s/it, loss=1.70055]\n",
            "Epoch 5: 100%|██████████| 250/250 [05:36<00:00,  1.34s/it, loss=1.59219]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MDpPKjDirtE",
        "colab_type": "code",
        "outputId": "c818630a-559d-4d11-baa9-b1a91c2bfc02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "torch.save(m, 'abs.2.train.pt')\n",
        "sm = torch.load('abs.2.train.pt')\n",
        "sm.state_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type EncoderRNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type DecoderRNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.state_dict of Seq2Seq(\n",
              "  (enc_dec_adapter): Linear(in_features=300, out_features=200, bias=True)\n",
              "  (embedding): Embedding(53341, 100, padding_idx=0)\n",
              "  (encoder): EncoderRNN(\n",
              "    (gru): GRU(100, 150, bidirectional=True)\n",
              "  )\n",
              "  (decoder): DecoderRNN(\n",
              "    (gru): GRU(100, 200)\n",
              "    (enc_bilinear): Bilinear(in1_features=200, in2_features=300, out_features=1, bias=True)\n",
              "    (ptr): Linear(in_features=500, out_features=1, bias=True)\n",
              "    (pre_out): Linear(in_features=500, out_features=100, bias=True)\n",
              "    (out): Linear(in_features=100, out_features=53341, bias=True)\n",
              "  )\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NKZjFnojBJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_ds = Dataset(filename=None, dataframe=test, max_src_len=400, \n",
        "                 max_tgt_len=100, truncate_src=True, truncate_tgt=True)\n",
        "\n",
        "test_gen = test_ds.generator(batch_size=8, src_vocab=Voc, tgt_vocab=Voc, ext_vocab=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weaBXWCQkza8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next_gen = next(test_gen)\n",
        "examp, src_tens, targ_tens, lens, oovs = next_gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWIhC60FjthS",
        "colab_type": "code",
        "outputId": "3af0fd4d-fda7-4cc0-8f37-8dda5ad3534c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "pred_src = examp[0][0]\n",
        "pred_src = \" \".join(pred_src)\n",
        "pred_tgt = examp[0][1]\n",
        "pred_tgt = \" \".join(pred_tgt)\n",
        "\n",
        "dec_batch, out = decode_batch(next_gen, sm, Voc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMvcMXT3keLo",
        "colab_type": "code",
        "outputId": "1f338861-2baf-4ad8-d222-b5e459d7c057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "decoded = \" \".join(dec_batch[0])\n",
        "decoded, pred_tgt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Mr Bush\\\\s first task was to `` rebuild a sense of domestic purpose '' within the US president was significant for the world but particularly so for Britain because of its special relationship , he added . <EOS>\",\n",
              " \"Mr Bush\\\\s re-election came at a crucial time for a world that was `` fractured , divided and uncertain '' , Mr Blair said.Lib Dem foreign affairs spokesman Menzies Campbell said a win by Mr Kerry would have given Mr Blair the chance of a fresh start , adding it was almost as if there was an `` umbilical cord '' between Mr Bush and the UK premier.Mr Bush\\\\s first task was to `` rebuild a sense of domestic purpose '' within the US , he said.Mr Blair said states had to work with the US to fight global terrorism.Even\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2MRsHl6RZdL",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ~~~~~~~~~~~~~~~~~~~~Todo~~~~~~~~~~~~~~~~~~~~"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ugFuaTnqFPk",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/A%20-%20Using%20TorchText%20with%20Your%20Own%20Datasets.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W189Q9LWdGbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Torchtext { form-width: \"5px\" }\n",
        "\n",
        "#get data\n",
        "TEXT = data.Field(sequential=True, tokenize = 'spacy', init_token='<sos>',\n",
        "                  eos_token='<eos>', lower=True)\n",
        "MAX_VOCAB_SIZE = 8000\n",
        "\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(path='./',\n",
        "                                                              train='train.csv',\n",
        "                                                              validation='valid.csv',\n",
        "                                                              test='test.csv',\n",
        "                                                              format='csv',\n",
        "                                                              fields=[('article', TEXT), \n",
        "                                                                      ('summary', TEXT)])\n",
        "\n",
        "TEXT.build_vocab(train_data, vectors='glove.6B.50d', max_size = MAX_VOCAB_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6E6dlM0r1Tw",
        "colab_type": "text"
      },
      "source": [
        "When we pass data into a neural network, we want the data to be padded to be the same length so that we can process them in batch. The BucketIterator groups sequences of similar lengths together for each batch to minimize padding. Handy, right?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33unolUgpDXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    sort = False, #don't sort test/validation data\n",
        "    batch_size=8,\n",
        "    device=DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDBXDJNZdeXm",
        "colab_type": "text"
      },
      "source": [
        "Our Encoder: will consist of an embedding layer, an RNN, and a linear layer. The RNN will take a dense vector and the previous hidden state. The linear layer takes a final hidden state and feeds it through a fully connected layer. The forward is called when we feed examples to our model. Each batch, `text` is a tensor of size [sentence_length, batch_size]. That is a batch of sentences, each having each word converted into a one-hot vector. Pytorch nicely stores a one-hot vector as it's index value, i.e. the tensor representing a sentence is just a tensor of indexes for each token in that sentence. The act of converting a list of tokens to a list of indexes is called numericalisation. The input batch is passed through the embedding layer to get `embedded` which gives us a dense vector representation of our sentences. `embedded` is a tensor of size [sentence_length, batch_size, embedding_dim]. `embedded` is fed into the RNN, in some frameworks you have to feed the initial hiddn state into the RNN but in PyTorch if no initial hidden state is passed it defaults to a tensor of all zeros. The RNN returns 2 tensors, `output` of size [sentence_length, batch_size, hidden_dim] and `hidden` of size [1, batch_size, hidden_dim]. `output` is the concatenation of the hidden state from every time step, whereas `hidden` is simply the final hidden state. We verify this by assert, squeeze removes a dimension of size 1. Then feed the last hidden state `hidden` through the linear layer to produce a prediction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBX63sSscvlU",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb\n",
        "\n",
        "Next, we have to build a vocabulary. This is a effectively a look up table where every unique word in your data set has a corresponding index (an integer).\n",
        "\n",
        "We do this as our machine learning model cannot operate on strings, only numbers. Each index is used to construct a one-hot vector for each word. A one-hot vector is a vector where all of the elements are 0, except one, which is 1, and dimensionality is the total number of unique words in your vocabulary, commonly denoted by $V$.\n",
        "\n",
        "![alt text](https://github.com/bentrevett/pytorch-sentiment-analysis/raw/61093d819c960368a02500bb4ff6f8881731abd3/assets/sentiment5.png)\n",
        "\n",
        "The number of unique words in our training set is over 100,000, which means that our one-hot vectors will have over 100,000 dimensions! This will make training slow and possibly won't fit onto your GPU (if you're using one).\n",
        "\n",
        "There are two ways effectively cut down our vocabulary, we can either only take the top $n$ most common words or ignore words that appear less than $m$ times. We'll do the former, only keeping the top 25,000 words.\n",
        "\n",
        "What do we do with words that appear in examples but we have cut from the vocabulary? We replace them with a special unknown or <unk> token. For example, if the sentence was \"This film is great and I love it\" but the word \"love\" was not in the vocabulary, it would become \"This film is great and I <unk> it\".\n",
        "\n",
        "The following builds the vocabulary, only keeping the most common max_size tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oro7QIDMhgp6",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Simple Encoder { form-width: \"5px\" }\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, dropout=0.2):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.hid_dim = hid_dim\n",
        "        self.rnn = nn.RNN(emb_dim, hid_dim)\n",
        "        self.LSTM = nn.LSTM(emb_dim, hid_dim, dropout = dropout)\n",
        "        ####\n",
        "        self.dropout = dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        #self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        ###\n",
        "    def forward(self, text):\n",
        "        #source sentence, $X$, which is converted into dense vectors \n",
        "        #using the embedding layer, and then dropout is applied.\n",
        "        #text = [sent len, batch size]\n",
        "        embedded = self.embedding(text)\n",
        "        embedded = self.dropout(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        #The RNN returns: outputs (the top-layer hidden state for each time-step), \n",
        "        #hidden (the final hidden state for each layer, h_T, stacked on top of each other) \n",
        "        #and cell (the final cell state for each layer, c_T, stacked on top of each other).\n",
        "        output, (hidden, cell) = self.LSTM(embedded)\n",
        "        return hidden, cell\n",
        "        #output = [sent len, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jgqnw7nQ6Y_n",
        "colab_type": "text"
      },
      "source": [
        "The Decoder class does a single step of decoding. The first layer will receive a hidden and cell state from the previous time-step, $(s_{t-1}^1, c_{t-1}^1)$, and feed it through the LSTM with the current token, $y_t$, to produce a new hidden and cell state, $(s_t^1, c_t^1)$. The subsequent layers will use the hidden state from the layer below, $s_t^{l-1}$, and the previous hidden and cell states from their layer, $(s_{t-1}^l, c_{t-1}^l)$. \n",
        "\n",
        "We then pass the hidden state from the top layer of the RNN, $s_t^L$, through a linear layer, $f$, to make a prediction of what the next token in the target (output) sequence should be, $\\hat{y}_{t+1}$.\n",
        "\n",
        "The arguments and initialization are similar to the Encoder class, except we now have an output_dim which is the size of the one-hot vectors that will be input to the decoder. These are equal to the vocabulary size of the output/target. There is also the addition of the Linear layer, used to make the predictions from the top layer hidden state.\n",
        "\n",
        "Within the forward method, we accept a batch of input tokens, previous hidden states and previous cell states. We unsqueeze the input tokens to add a sentence length dimension of 1. Then, similar to the encoder, we pass through an embedding layer and apply dropout. This batch of embedded tokens is then passed into the RNN with the previous hidden and cell states. This produces an output (hidden state from the top layer of the RNN), a new hidden state (one for each layer, stacked on top of each other) and a new cell state (also one per layer, stacked on top of each other). We then pass the output (after getting rid of the sentence length dimension) through the linear layer to receive our prediction. We then return the prediction, the new hidden state and the new cell state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeRjIp9ywYLb",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title simple Decoder { form-width: \"5px\" }\n",
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, hid_dim, dropout = 0.3):\n",
        "    super().__init__()\n",
        "    self.emb_dim = emb_dim\n",
        "    #hid_dim is the dimensionality of the hidden and cell states.\n",
        "    self.hid_dim = hid_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.dropout = dropout\n",
        "    \n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    \n",
        "    self.LSTM = nn.LSTM(emb_dim, hid_dim, dropout=dropout)\n",
        "    self.RNN = nn.RNN(emb_dim, hid_dim, dropout=dropout)\n",
        "    self.GRU = nn.GRU(emb_dim, hid_dim, dropout=dropout)\n",
        "    self.fc = nn.Linear(hid_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "  def forward(self, input, hidden, cell):\n",
        "    #input = [batch size]\n",
        "    #hidden = [n layers * n directions, batch size, hid dim]\n",
        "    #cell = [n layers * n directions, batch size, hid dim]\n",
        "    #n directions in the decoder will both always be 1, therefore:\n",
        "    #hidden = [n layers, batch size, hid dim]\n",
        "    #context = [n layers, batch size, hid dim]\n",
        "    input = input.unsqueeze(0)\n",
        "    #input = [1, batch size]\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    #embedded = [1, batch size, emb dim]\n",
        "    output, (hidden, cell) = self.LSTM(embedded, (hidden, cell))\n",
        "    #output = [sent len, batch size, hid dim * n directions]\n",
        "    #hidden = [n layers * n directions, batch size, hid dim]\n",
        "    #cell = [n layers * n directions, batch size, hid dim] \n",
        "    #sent len and n directions will always be 1 in the decoder, therefore:\n",
        "    #output = [1, batch size, hid dim]\n",
        "    #hidden = [n layers, batch size, hid dim]\n",
        "    #cell = [n layers, batch size, hid dim]\n",
        "    prediction = self.fc(output.squeeze(0))\n",
        "    #prediction = [batch size, output dim]\n",
        "    return prediction, hidden, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT0odMKh9RP-",
        "colab_type": "text"
      },
      "source": [
        "###Seq2Seq\n",
        "This will handle\n",
        "- receiving the input/source sentence\n",
        "- using the encoder to produce the context vectors\n",
        "- using the decoder to produce the predicted output/target sentence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk9L1FMQ9o69",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title simple Seq2Seq { form-width: \"5px\" }\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "    \n",
        "    assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "  def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "    #src = [src sent len, batch size]\n",
        "    #trg = [trg sent len, batch size]\n",
        "    #teacher_forcing_ratio is probability to use teacher forcing\n",
        "    #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "    batch_size = trg.shape[1]\n",
        "    max_len = trg.shape[0]\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "    #tensor to store decoder outputs\n",
        "    outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "    #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "    hidden, cell = self.encoder(src)\n",
        "    #first input to the decoder is the <sos> tokens\n",
        "    input = trg[0,:]\n",
        "    \n",
        "    for t in range(1, max_len):\n",
        "      output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "      outputs[t] = output\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "      top1 = output.max(1)[1]\n",
        "      ##use topi, topk\n",
        "      input = (trg[t] if teacher_force else top1)\n",
        "    \n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kTCFxXH_db-",
        "colab_type": "code",
        "outputId": "2bfbe9f3-34d5-406e-a01e-ea7bebf32b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "#@title Parameters 2 { form-width: \"5px\" }\n",
        "\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "OUTPUT_DIM = len(TEXT.vocab)\n",
        "ENC_EMB_DIM = 50\n",
        "DEC_EMB_DIM = 50\n",
        "HID_DIM = 50\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDxFCD2-NzCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc = EncoderRNN(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = DecoderRNN(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "model2 = Seq2Seq(enc, dec, DEVICE).to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr_675RayqFv",
        "colab_type": "text"
      },
      "source": [
        "https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1200/1*kLIkXgfeGRdi1Mds5CV5xA.png)\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1200/1*DQ_mD_mIN3M6gpVoe2NALA.png)\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1200/1*Ht2-sUJHi65wDwnR276k3A.png)\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1200/1*2zXEI3nbVV5mqSoDrVYscA.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iozskZww0aUV",
        "colab_type": "text"
      },
      "source": [
        "###When to use GRU vs RNN vs LSTM\n",
        "\n",
        "So, LSTM gives us the most Control-ability and thus, Better Results. But also comes with more Complexity and Operating Cost.\n",
        "\n",
        "GRU is related to LSTM as both are utilizing different way if gating information to prevent vanishing gradient problem. Here are some pin-points about GRU vs LSTM-\n",
        "\n",
        "The GRU unit controls the flow of information like the LSTM unit, but without having to use a memory unit. It just exposes the full hidden content without any control.\n",
        "GRU is relatively new, and from my perspective, the performance is on par with LSTM, but computationally more efficient (less complex structure as pointed out). So we are seeing it being used more and more.\n",
        "\n",
        "###They perform roughly the same \n",
        "https://arxiv.org/pdf/1412.3555.pdf\n",
        "\n",
        "Further Reading:\n",
        "\n",
        "http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/\n",
        "\n",
        "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRnGLGtkhv-F",
        "colab_type": "text"
      },
      "source": [
        "Now create an instance of the RNN. The input dimension is the dimension of one-hot vectors which is equal to vocabulary size. \n",
        "\n",
        "We can exclude the output dimension if we want to decode since we dont want to predict but just encode and take the outputs/final hidden states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMAFs3NOsBoU",
        "colab_type": "text"
      },
      "source": [
        "Currently, the iterator returns a custom datatype called torchtext.data.Batch. This makes code reuse difficult (since each time the column names change, we need to modify the code), and makes torchtext hard to use with other libraries for some use cases (like torchsample and fastai). Concretely, we'll convert the batch to a tuple in the form (x, y) where x is the independent variable (the input to the model) and y is the dependent variable (the supervision data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olwpjTOYsgcM",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/pytorch/text\n",
        "\n",
        "https://medium.com/@sonicboom8/sentiment-analysis-torchtext-55fb57b1fab8\n",
        "\n",
        "Note: BucketIterator returns a Batch object instead of text index and labels. Also Batch object is not iterable like pytorch Dataloader. A single Batch object contains the data of one batch .The text and labels can be accessed via column names. This is one of the small hiccups in torchtext. But this can be easily overcome in two ways. Either write some extra code in the training loop for getting the data out of the Batch object or write a iterable wrapper around Batch Object that returns the desired data.\n",
        "\n",
        "Here's two examples below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V8_P4CcsIYm",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Batch Generator { form-width: \"5px\" }\n",
        "\n",
        "class BatchWrapper:\n",
        "    def __init__(self, dl, x_var, y_vars):\n",
        "        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars \n",
        "        # we pass in the list of attributes for x and y\n",
        "    \n",
        "    def __iter__(self):\n",
        "        for batch in self.dl:\n",
        "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
        "            \n",
        "            if self.y_vars is not None: # we will concatenate y into a single tensor\n",
        "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
        "            else:\n",
        "                y = torch.zeros((1))\n",
        "\n",
        "            yield (x, y)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "      #We will use Batch Generator\n",
        "class BatchGenerator:\n",
        "    def __init__(self, dl, x_field, y_field):\n",
        "        self.dl, self.x_field, self.y_field = dl, x_field, y_field\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "    \n",
        "    def __iter__(self):\n",
        "        for batch in self.dl:\n",
        "            X = getattr(batch, self.x_field)\n",
        "            y = getattr(batch, self.y_field)\n",
        "            yield (X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmpyycrON9ZS",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title 1 Sample decode batch { form-width: \"5px\" }\n",
        "def sample_dec_batch(decoded_tokens, vocab: Vocab, oov_dict: OOVDict) -> List[List[str]]:\n",
        "  \"\"\"Convert word indices to strings.\"\"\"\n",
        "  decoded_batch = []\n",
        "  if not isinstance(decoded_tokens, list):\n",
        "    decoded_tokens = decoded_tokens.transpose(0, 1).tolist()\n",
        "  for i, doc in enumerate(decoded_tokens):\n",
        "    decoded_doc = []\n",
        "    for word_idx in doc:\n",
        "      for j in word_idx:\n",
        "        decoded_doc.append(vocab.index2word[j])\n",
        "        if word_idx == vocab.EOS:\n",
        "          break\n",
        "    decoded_batch.append(decoded_doc)\n",
        "  return decoded_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iMvbx2jsRUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dl = next(iter(train_iterator))\n",
        "trn_src_dl = train_dl.article\n",
        "trn_tgt_dl = train_dl.summary\n",
        "#out, hid = enc(trn_src_dl)\n",
        "outs = model2(trn_src_dl, trn_tgt_dl)\n",
        "top_k, top_idx = outs.topk(1)\n",
        "transposed_top_idx = top_idx.transpose(0, 1).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9VcD2MTTHBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_doc = []\n",
        "dec_idxs=[]\n",
        "for i, doc in enumerate(transposed_top_idx):\n",
        "  for word_idx in doc:\n",
        "    dec_idxs.append(word_idx)\n",
        "    if word_idx == 2:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHr0vRL9XKXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in dec_idxs:\n",
        "  for j in i:\n",
        "    print(Voc.index2word[j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRGgQQ_TL110",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dec_batch = sample_dec_batch(slam, Voc, Voc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWdtkLU8WiDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\" \".join(dec_batch[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20O2pSx3Yg1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_tokens = trn_tgt_dl.transpose(0, 1).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76KuQiZ8ZQCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in target_tokens:\n",
        "  for j in i:\n",
        "    print(Voc.index2word[j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvMitzKQtTjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch_it = BatchGenerator(train_iterator, 'article', 'summary')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yVreBTDqhZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output, hidden = model(train_dl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scPq8xGBvQJ3",
        "colab_type": "code",
        "outputId": "0b1ed775-3bb6-4a71-a277-3759de29133b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "hidden.shape, output.shape, train_dl.shape\n",
        "\n",
        "#hidden = [1, batch size, hid dim],\n",
        "\n",
        "#output = [sent len, batch size, hid dim],\n",
        "\n",
        "#article = [sent len, batch size]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 8, 256]), torch.Size([854, 8, 256]), torch.Size([854, 8]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChYq2F_Jl9-H",
        "colab_type": "code",
        "outputId": "fe18593c-b94e-4b04-943f-7e2b833cde94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title Count Parameters of Model { form-width: \"5px\" }\n",
        "#Useful function to tell us how many trainable params our model has.\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model2):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 3,841,602 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkDAHk-GmEdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model2 = model2.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDHaSo7Scba6",
        "colab_type": "code",
        "outputId": "2731b656-5ceb-4cb3-9742-a19559c645f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_batch_it.x_field"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'article'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 393
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl3fICqNcTIn",
        "colab_type": "code",
        "outputId": "b32a3aa0-aa73-4f03-e796-fc0616c6c833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "for i, batch in enumerate(train_iterator):\n",
        "  print(batch.article)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[14512,     6,  2672,  ...,  2061,     6,     6],\n",
            "        [ 7110,   650,  7538,  ...,  4531,  6865,     0],\n",
            "        [   73,   101, 10832,  ...,   552,    30,  1790],\n",
            "        ...,\n",
            "        [    1,     1,     1,  ...,     1,  4849,     1],\n",
            "        [    1,     1,     1,  ...,     1,     4,     1],\n",
            "        [    1,     1,     1,  ...,     1,     6,     1]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9q7Myl8bT5B",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)#@title Train 2 { form-width: \"5px\" }\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.article\n",
        "        trg = batch.summary\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg sent len, batch size]\n",
        "        #output = [trg sent len, batch size, output dim]\n",
        "        \n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg sent len - 1) * batch size]\n",
        "        #output = [(trg sent len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI8G7z1HdvoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tgt = trn_tgt_dl[1:].view(-1)\n",
        "out_reshaped = outs[1:].view(-1, outs.shape[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1lqzjbjdVCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = criterion(outs, tgt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovd-Aea0dzxD",
        "colab_type": "code",
        "outputId": "548e0fe5-e0d6-4c6d-bd9f-70222b05fe86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tgt.shape, out_reshaped.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2264]), torch.Size([2264, 25002]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 406
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb5ciWiGiB-H",
        "colab_type": "code",
        "outputId": "97cd367d-f47a-482a-d1bb-4bb6d00cf402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model2.apply(init_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): EncoderRNN(\n",
              "    (embedding): Embedding(25002, 50)\n",
              "    (rnn): RNN(50, 50)\n",
              "    (LSTM): LSTM(50, 50, dropout=0.2)\n",
              "    (dropout): Dropout(p=0.2)\n",
              "  )\n",
              "  (decoder): DecoderRNN(\n",
              "    (embedding): Embedding(25002, 50)\n",
              "    (LSTM): LSTM(50, 50, dropout=0.5)\n",
              "    (RNN): RNN(50, 50, dropout=0.5)\n",
              "    (GRU): GRU(50, 50, dropout=0.5)\n",
              "    (fc): Linear(in_features=50, out_features=25002, bias=True)\n",
              "    (dropout): Dropout(p=0.5)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 419
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozNgnGkUby9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPOCHS = 3\n",
        "CLIP = 1\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "  print('Epoch....{}'.format(epoch))\n",
        "  train_loss = train(model2, train_iterator, optimizer, criterion, CLIP)\n",
        "  \n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JyI7rs3eN5P",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Learning Phrase Repre { form-width: \"5px\" }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5E-Jh5GlqFw",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Phrase Encoder\n",
        "class EncoderRNN2(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.input_dim = input_dim\n",
        "    self.emb_dim = emb_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.dropout = dropout\n",
        "    \n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    \n",
        "    self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "    \n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "  def forward(self, src):\n",
        "    \n",
        "    #src = [src sent len, batch size]\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "    #embedded = [src sent len, batch size, emb dim]\n",
        "    outputs, hidden = self.rnn(embedded)\n",
        "    #outputs = [src sent len, batch size, hid dim * n directions]\n",
        "    #hidden = [n layers * n directions, batch size, hid dim]\n",
        "    #outputs are always from the top hidden layer\n",
        "    return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIZTwh_XrNDT",
        "colab_type": "text"
      },
      "source": [
        "The decoder is where the implementation differs significantly from the previous model and we alleviate some of the information compression.\n",
        "\n",
        "Instead of the GRU in the decoder taking just the target token, $y_t$ and the previous hidden state $s_{t-1}$ as inputs, it also takes the context vector $z$.\n",
        "\n",
        "Note how this context vector, $z$, does not have a $t$ subscript, meaning we re-use the same context vector returned by the encoder for every time-step in the decoder.\n",
        "\n",
        "Before, we predicted the next token, $\\hat{y}_{t+1}$, with the linear layer, $f$, only using the top-layer decoder hidden state at that time-step, $s_t$, as $\\hat{y}_{t+1}=f(s_t^L)$. Now, we also pass the current token, $\\hat{y}_t$ and the context vector, $z$ to the linear layer.\n",
        "\n",
        "Note, the initial hidden state, $s_0$, is still the context vector, $z$, so when generating the first token we are actually inputting two identical context vectors into the GRU.\n",
        "\n",
        "How do these two changes reduce the information compression? Well, hypothetically the decoder hidden states, $s_t$, no longer need to contain information about the source sequence as it is always available as an input. Thus, it only needs to contain information about what tokens it has generated so far. The addition of $y_t$ to the linear layer also means this layer can directly see what the token is, without having to get this information from the hidden state.\n",
        "\n",
        "Within the implementation, we will pass $y_t$ and $z$ to the GRU by concatenating them together, so the input dimensions to the GRU are now emb_dim + hid_dim (as context vector will be of size hid_dim). The linear layer will take $y_t, s_t$ and $z$ also by concatenating them together, hence the input dimensions are now emb_dim + hid_dim*2.\n",
        "\n",
        "forward now takes a context argument. Inside of forward, we concatenate $y_t$ and $z$ as emb_con before feeding to the GRU, and we concatenate $y_t$, $s_t$ and $z$ together as output before feeding it through the linear layer to receive our predictions, $\\hat{y}_{t+1}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcBfiZuzqON0",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Phrase Decoder\n",
        "\n",
        "class DecoderRNN2(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.emb_dim = emb_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.dropout = dropout\n",
        "    \n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    \n",
        "    self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "    \n",
        "    self.out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "    \n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "  def forward(self, input, hidden, context):\n",
        "    #input = [batch size]\n",
        "    #hidden = [n layers * n directions, batch size, hid dim]\n",
        "    #context = [n layers * n directions, batch size, hid dim]\n",
        "    #n layers and n directions in the decoder will both always be 1, therefore:\n",
        "    #hidden = [1, batch size, hid dim]\n",
        "    #context = [1, batch size, hid dim]\n",
        "    input = input.unsqueeze(0)\n",
        "    #input = [1, batch size]\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    #embedded = [1, batch size, emb dim]\n",
        "    emb_con = torch.cat((embedded, context), dim=2)\n",
        "    #emb_con = [1, batch size, emb dim + hid dim]\n",
        "    output, hidden = self.rnn(emb_con, hidden)\n",
        "    #output = [sent len, batch size, hid dim * n directions]\n",
        "    #hidden = [n layers * n directions, batch size, hid dim]\n",
        "    #sent len, n layers and n directions will always be 1 in the decoder, therefore:\n",
        "    #output = [1, batch size, hid dim]\n",
        "    #hidden = [1, batch size, hid dim]\n",
        "    output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)),\n",
        "                      dim=1)\n",
        "    #output = [batch size, emb dim + hid dim * 2]\n",
        "    prediction = self.out(output)\n",
        "    #prediction = [batch size, output dim]\n",
        "    return prediction, hidden    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei5ujcBEs_0a",
        "colab_type": "text"
      },
      "source": [
        "Briefly going over all of the steps:\n",
        "\n",
        "- the outputs tensor is created to hold all predictions, $\\hat{Y}$\n",
        "- the source sequence, $X$, is fed into the encoder to receive a context vector\n",
        "- the initial decoder hidden state is set to be the context vector, $s_0 = z = h_T$\n",
        "- we use a batch of <sos> tokens as the first input, $y_1$\n",
        "- we then decode within a loop:\n",
        " - inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and the context vector, $z$, into the decoder\n",
        " - receiving a prediction, $\\hat{y}_{t+1}$, and a new hidden state, $s_t$\n",
        " - we then decide if we are going to teacher force or not, setting the next input as appropriate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UepqHVJGs9a4",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Phrase Seq2Seq\n",
        "class Seq2Seq2(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = DEVICE\n",
        "    \n",
        "    assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "      \"Hidden dimensions of enc and dec must be equal\"\n",
        "    \n",
        "  def forward(self, src, trg, tfc=0.5):\n",
        "    #src = [src sent len, batch size]\n",
        "    #trg = [trg sent len, batch size]\n",
        "    batch_size = trg.shape[1]\n",
        "    max_len = trg.shape[0]\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "    #tensor to store dec outputs\n",
        "    outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "    #last hidden state of encoder is context vector\n",
        "    context = self.encoder(src)\n",
        "    #context also inithidden state for decoder\n",
        "    hidden = context\n",
        "    #first input to decoder is <sos>\n",
        "    input = trg[0,:]\n",
        "    #loop over the decode to predict the next word\n",
        "    for t in range(1, max_len):\n",
        "      output, hidden = self.decoder(input, hidden, context)\n",
        "      outputs[t] = output\n",
        "      teach = random.random() < tfc\n",
        "      top1 = output.max(1)[1]\n",
        "      input = (trg[t] if teach else top1)\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU1gCCvDuwq1",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Phrase Params\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "OUTPUT_DIM = len(TEXT.vocab)\n",
        "ENC_EMB_DIM = 50\n",
        "DEC_EMB_DIM = 50\n",
        "HID_DIM = 50\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = EncoderRNN2(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = DecoderRNN2(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Seq2Seq2(enc, dec, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLerVwbIu5Jq",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "be3d17d1-9a52-4947-946b-440fc832a778",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "#@title \n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq2(\n",
              "  (encoder): EncoderRNN2(\n",
              "    (embedding): Embedding(25004, 50)\n",
              "    (rnn): GRU(50, 50)\n",
              "    (dropout): Dropout(p=0.5)\n",
              "  )\n",
              "  (decoder): DecoderRNN2(\n",
              "    (embedding): Embedding(25004, 50)\n",
              "    (rnn): GRU(100, 50)\n",
              "    (out): Linear(in_features=150, out_features=25004, bias=True)\n",
              "    (dropout): Dropout(p=0.5)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 437
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyVhWp-dvIwa",
        "colab_type": "code",
        "outputId": "e9afcb54-1b01-4b3b-dfea-2b98bcdd82f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 6,314,104 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVHpDyJfvLQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "PAD_IDX = TEXT.vocab.stoi['<pad>']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pckYswFMvTFH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.article\n",
        "        trg = batch.summary\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg sent len, batch size]\n",
        "        #output = [trg sent len, batch size, output dim]\n",
        "        \n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg sent len - 1) * batch size]\n",
        "        #output = [(trg sent len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxQHME0nvWv9",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.article\n",
        "            trg = batch.summary\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg sent len, batch size]\n",
        "            #output = [trg sent len, batch size, output dim]\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg sent len - 1) * batch size]\n",
        "            #output = [(trg sent len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZuZ0gZ_vX1c",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx_b0V5wvavj",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "import math\n",
        "import time\n",
        "\n",
        "N_EPOCHS = 2\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoWbN3seve88",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Neural Machine Translation by Jointly Learning to Align and Translate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuhoD1DUwIia",
        "colab_type": "text"
      },
      "source": [
        "In the previous model, our architecture was set-up in a way to reduce \"information compression\" by explicitly passing the context vector, $z$, to the decoder at every time-step and by passing both the context vector and input word, $y_t$, along with the hidden state, $s_t$, to the linear layer, $f$, to make a prediction.\n",
        "\n",
        "Even though we have reduced some of this compression, our context vector still needs to contain all of the information about the source sentence. The model implemented in this notebook avoids this compression by allowing the decoder to look at the entire source sentence (via its hidden states) at each decoding step! How does it do this? It uses attention.\n",
        "\n",
        "Attention works by first, calculating an attention vector, $a$, that is the length of the source sentence. The attention vector has the property that each element is between 0 and 1, and the entire vector sums to 1. We then calculate a weighted sum of our source sentence hidden states, $H$, to get a weighted source vector, $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fbjHSW_z1ke",
        "colab_type": "text"
      },
      "source": [
        "First, we'll build the encoder. Similar to the previous model, we only use a single layer GRU, however we now use a bidirectional RNN. \n",
        "\n",
        "The RNN returns outputs and hidden.\n",
        "\n",
        "outputs is of size [src sent len, batch size, hid dim * num directions] where the first hid_dim elements in the third axis are the hidden states from the top layer forward RNN, and the last hid_dim elements are hidden states from the top layer backward RNN. You can think of the third axis as being the forward and backward hidden states stacked on top of each other, i.e. $h_1 = [h_1^\\rightarrow; h_{T}^\\leftarrow]$, $h_2 = [h_2^\\rightarrow; h_{T-1}^\\leftarrow]$ and we can denote all stacked encoder hidden states as $H=\\{ h_1, h_2, ..., h_T\\}$.\n",
        "\n",
        "hidden is of size [n layers * num directions, batch size, hid dim], where [-2, :, :] gives the top layer forward RNN hidden state after the final time-step (i.e. after it has seen the last word in the sentence) and [-1, :, :] gives the top layer backward RNN hidden state after the final time-step (i.e. after it has seen the first word in the sentence).\n",
        "\n",
        "As the decoder is not bidirectional, it only needs a single context vector, $z$, to use as its initial hidden state, $s_0$, and we currently have two, a forward and a backward one ($z^\\rightarrow=h_T^\\rightarrow$ and $z^\\leftarrow=h_T^\\leftarrow$, respectively). We solve this by concatenating the two context vectors together, passing them through a linear layer, $g$, and applying the $\\tanh$ activation function.\n",
        "\n",
        "As we want our model to look back over the whole of the source sentence we return outputs, the stacked forward and backward hidden states for every token in the source sentence. We also return hidden, which acts as our initial hidden state in the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGc5AqzZwANk",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title NMT Encoder { form-width: \"5px\" }\n",
        "\n",
        "class EncoderNMT(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "      super().__init__()\n",
        "      self.input_dim = input_dim\n",
        "      self.emb_dim = emb_dim\n",
        "      self.enc_hid_dim = enc_hid_dim\n",
        "      self.dec_hid_dim = dec_hid_dim\n",
        "      self.dropout = dropout\n",
        "      \n",
        "      self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "      \n",
        "      self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n",
        "      \n",
        "      self.fc = nn.Linear(enc_hid_dim *2, dec_hid_dim)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "      \n",
        "  def forward(self, src):\n",
        "    #src = [src sent len, batch size]\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "    #embedded = [src sent len, batch size, emb dim]\n",
        "    outputs, hidden = self.rnn(embedded)\n",
        "    #outputs = [src sent len, batch size, hid dim * num directions]\n",
        "    #hidden = [n layers * num directions, batch size, hid dim]\n",
        "    \n",
        "    #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "    #outputs are always from the last layer\n",
        "      \n",
        "    #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "    #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "    hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))) \n",
        "    #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "    #  encoder RNNs fed through a linear layer\n",
        "      \n",
        "    #outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "      \n",
        "    return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJeuYK6A4LQM",
        "colab_type": "text"
      },
      "source": [
        "Next up is the attention layer. This will take in the previous hidden state of the decoder, $s_{t-1}$, and all of the stacked forward and backward hidden states from the encoder, $H$. The layer will output an attention vector, $a_t$, that is the length of the source sentence, each element is between 0 and 1 and the entire vector sums to 1 (since tanh).\n",
        "\n",
        "Intuitively, this layer takes what we have decoded so far, $s_{t-1}$, and all of what we have encoded, $H$, to produce a vector, $a_t$, that represents which words in the source sentence we should pay the most attention to in order to correctly predict the next word to decode, $\\hat{y}_{t+1}$.\n",
        "\n",
        "First, we calculate the energy between the previous decoder hidden state and the encoder hidden states. **As our encoder hidden states are a sequence of $T$ tensors, and our previous decoder hidden state is a single tensor, the first thing we do is repeat the previous decoder hidden state $T$ times.** We then calculate the energy, $E_t$, between them by concatenating them together and passing them through a linear layer (attn) and a $\\tanh$ activation function.\n",
        "\n",
        "$$E_t = \\tanh(\\text{attn}(s_{t-1}, H))$$\n",
        "This can be thought of as calculating how well each encoder hidden state \"matches\" the previous decoder hidden state.\n",
        "\n",
        "We currently have a [dec hid dim, src sent len] tensor for each example in the batch. We want this to be [src sent len] for each example in the batch as the attention should be over the length of the source sentence. This is achieved by multiplying the energy by a [1, dec hid dim] tensor, $v$.\n",
        "\n",
        "$$\\hat{a}_t = v E_t$$\n",
        "We can think of this as calculating a weighted sum of the \"match\" over all dec_hid_dem elements for each encoder hidden state, where the weights are learned (as we learn the parameters of $v$).\n",
        "\n",
        "Finally, we ensure the attention vector fits the constraints of having all elements between 0 and 1 and the vector summing to 1 by passing it through a $\\text{softmax}$ layer.\n",
        "\n",
        "$$a_t = \\text{softmax}(\\hat{a_t})$$\n",
        "This gives us the attention over the source sentence!\n",
        "\n",
        "Graphically, this looks something like below. This is for calculating the very first attention vector, where $s_{t-1} = s_0 = z$. The green/yellow blocks represent the hidden states from both the forward and backward RNNs, and the attention computation is all done within the pink block.\n",
        "\n",
        "![alt text](https://github.com/bentrevett/pytorch-seq2seq/raw/61157fe51246a68db40dbff69adcd839abcaee05/assets/seq2seq9.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on9Z2ZJoXfZA",
        "colab_type": "text"
      },
      "source": [
        "The attention vector is a linear layer of the concatentation of all the hidden states (encoder and previous decoder) that produces a vector -> attention, that is then normalized with tanh -> Energy. This is how well the encoder hidden state matches the previous decoder hidden state. To enable us to use this Energy tensor for each decoder example we need a vector which we multiply with the Energy - which is length of 1 by decoder hidden dimension. This is a weighted sum that matches over all decoder hidden dimensional elements for each encoder hidden state where the weights are learned hence v = nn.Parameters. We then use this $a_t$ distribution over the source sentence with the encoder hidden states $H$ using $a_t$ as weights: $$w_t=a_tH$$. The embedded input word $y_t$, the weighted source vector $w_t$ and previous decoder hidden state $s_{t-1}$ are passed to an RNN with $y_t$ and $w_t$ being concatenated. Then pass $y_t w_t s_t$ through a linear layer to predict the next word in the target sequence by concatenating them all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRxCnjILTj-o",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "862b4da3-225f-435b-bcb3-796e073fd4e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#@title diff between .permute and .view()\n",
        "\n",
        "a = torch.tensor([[1,2],[3,4]])\n",
        "a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdAIInOTT6iK",
        "colab_type": "text"
      },
      "source": [
        "View changes how the tensor is represented. For ex: a tensor with 4 elements can be represented as 4X1 or 2X2 or 1X4 but permute changes the axes. While permuting the data is moved but with view data is not moved but just reinterpreted.\n",
        "\n",
        "Below code examples may help you. a is 2x2 tensor/matrix. With the use of view you can read a as a column or row vector (tensor). But you can't transpose it. To transpose you need permute. Transpose is achieved by swapping/permuting axes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q9GbIQUTqkp",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "a644ecda-647e-4919-97df-95cccccaf2be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#@title\n",
        "a.permute(1,0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 3],\n",
              "        [2, 4]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEN0s6R4TupD",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "a.view(4,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeF3j1Tbg5qO",
        "colab_type": "code",
        "outputId": "e227fce8-67db-48d9-c0e3-26eef4bd8fcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "a.squeeze(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Z5sMx_3dTg",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title NMT Attention { form-width: \"5px\" }\n",
        "\n",
        "class NMTAttention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.enc_hid_dim = enc_hid_dim\n",
        "    self.dec_hid_dim = dec_hid_dim\n",
        "    \n",
        "    self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "    #A learnable parameter.\n",
        "    #This will be our 1 by decoder hidden dimension vector that is the weighted sum that looks at what\n",
        "    #to pay attention to over the src. \n",
        "    self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
        "    \n",
        "  def forward(self, hidden, encoder_outputs):\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "    batch_size = encoder_outputs.shape[1]\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "    #repeat encoder hidden state src_len times\n",
        "    hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "    #hidden = [batch size, src sent len, dec hid dim]\n",
        "    #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "    #energy = [batch size, src sent len, dec hid dim]\n",
        "    energy = energy.permute(0, 2, 1)\n",
        "    #energy = [batch size, dec hid dim, src sent len]\n",
        "    #v = [dec hid dim]\n",
        "    #repeat used to expand a vector in this case batch wise\n",
        "    v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
        "    #v = [batch size, 1, dec hid dim]\n",
        "    #Performs a batch matrix-matrix product of matrices\n",
        "    attention = torch.bmm(v, energy).squeeze(1)\n",
        "    #attention= [batch size, src len]\n",
        "    return F.softmax(attention, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dDlP-YUVJJI",
        "colab_type": "text"
      },
      "source": [
        "The decoder contains the attention layer, attention, which takes the previous hidden state, $s_{t-1}$, all of the encoder hidden states, $H$, and returns the attention vector, $a_t$.\n",
        "\n",
        "We then use this attention vector to create a weighted source vector, $w_t$, denoted by weighted, which is a weighted sum of the encoder hidden states, $H$, using $a_t$ as the weights.\n",
        "\n",
        "$$w_t = a_t H$$\n",
        "The input word (that has been embedded), $y_t$, the weighted source vector, $w_t$, and the previous decoder hidden state, $s_{t-1}$, are then all passed into the decoder RNN, with $y_t$ and $w_t$ being concatenated together.\n",
        "\n",
        "$$s_t = \\text{DecoderGRU}(y_t, w_t, s_{t-1})$$\n",
        "We then pass $y_t$, $w_t$ and $s_t$ through the linear layer, $f$, to make a prediction of the next word in the target sentence, $\\hat{y}_{t+1}$. This is done by concatenating them all together.\n",
        "\n",
        "$$\\hat{y}_{t+1} = f(y_t, w_t, s_t)$$\n",
        "The image below shows decoding the first word in an example translation.\n",
        "\n",
        "![alt text](https://github.com/bentrevett/pytorch-seq2seq/raw/61157fe51246a68db40dbff69adcd839abcaee05/assets/seq2seq10.png)\n",
        "\n",
        "So we use the encoder hidden states and previous hidden state (concat of bidir encoder hidden states -> context vector * 2) to produce an attention vector. We then use this attention vector for a weighted sum with all encoder hidden states to produce attention weights. \n",
        "\n",
        "The green/yellow blocks show the forward/backward encoder RNNs which output $H$, the red block shows the context vector, $z = h_T = \\tanh(g(h^\\rightarrow_T,h^\\leftarrow_T)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$, the blue block shows the decoder RNN which outputs $s_t$, the purple block shows the linear layer, $f$, which outputs $\\hat{y}_{t+1}$ and the orange block shows the calculation of the weighted sum over $H$ by $a_t$ and outputs $w_t$. Not shown is the calculation of $a_t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmbGQnsOU8AP",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title NMT Decoder { form-width: \"5px\" }\n",
        "\n",
        "class NMTDecoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.emb_dim = emb_dim\n",
        "    self.enc_hid_dim = enc_hid_dim\n",
        "    self.dec_hid_dim = dec_hid_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.dropout = dropout\n",
        "    self.attention = attention\n",
        "    \n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    \n",
        "    self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "    \n",
        "    self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + enc_hid_dim, output_dim)\n",
        "    \n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "  def forward(self, input, hidden, encoder_outputs):\n",
        "    #input = [batch size]\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "    input = input.unsqueeze(0)\n",
        "    #input = [1, batch size]\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    #embedded = [1, batch size, emb dim]\n",
        "    a = self.attention(hidden, encoder_outputs)\n",
        "    #a = [batch size, src len]\n",
        "    a = a.unsqueeze(1)\n",
        "    #unsqueeze adds another dimension \n",
        "    #a = [batch size, 1, src len]\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "    #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "    weighted = torch.bmm(a, encoder_outputs)\n",
        "    #weighted = [batch size, 1, enc hid dim * 2]\n",
        "    weighted = weighted.permute(1, 0, 2)\n",
        "    #weighted = [1, batch size, enc hid dim * 2]\n",
        "    rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "    #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "    output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "    #output = [sent len, batch size, dec hid dim * n directions]\n",
        "    #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "    #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "    #output = [1, batch size, dec hid dim]\n",
        "    #hidden = [1, batch size, dec hid dim]\n",
        "    #this also means that output == hidden\n",
        "    assert(output == hidden).all()\n",
        "    embedded = embedded.squeeze(0)\n",
        "    output = output.squeeze(0)\n",
        "    weighted = weighted.squeeze(0)\n",
        "    \n",
        "    output = self.out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "    #output = [bsz, output dim]\n",
        "    return output, hidden.squeeze(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knf0e-aVkf0Z",
        "colab_type": "text"
      },
      "source": [
        "This is the first model where we don't have to have the encoder RNN and decoder RNN have the same hidden dimensions, however the encoder has to be bidirectional. This requirement can be removed by changing all occurences of enc_dim * 2 to enc_dim * 2 if encoder_is_bidirectional else enc_dim.\n",
        "\n",
        "This seq2seq encapsulator is similar to the last two. The only difference is that the encoder returns both the final hidden state (which is the final hidden state from both the forward and backward encoder RNNs passed through a linear layer) to be used as the initial hidden state for the encoder, as well as every hidden state (which are the forward and backward hidden states stacked on top of each other). We also need to ensure that hidden and encoder_outputs are passed to the decoder.\n",
        "\n",
        "Briefly going over all of the decoding steps:\n",
        "\n",
        "Briefly going over all of the steps:\n",
        "\n",
        "- the outputs tensor is created to hold all predictions, $\\hat{Y}$\n",
        "- the source sequence, $X$, is fed into the encoder to receive $z$ and $H$\n",
        "- the initial decoder hidden state is set to be the context vector, $s_0 = z = h_T$\n",
        "- we use a batch of <sos> tokens as the first input, $y_1$\n",
        "- we then decode within a loop:\n",
        " - inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and all encoder outputs, $H$, into the decoder\n",
        "  - receiving a prediction, $\\hat{y}_{t+1}$, and a new hidden state, $s_t$\n",
        "  - we then decide if we are going to teacher force or not, setting the next input as appropriate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RByfnOlKcvy_",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title NMT Seq2Seq { form-width: \"5px\" }\n",
        "\n",
        "class NMTSeq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "    \n",
        "  def forward(self, src, trg, tfr=0.5):\n",
        "    #src = [src sent len, batch size]\n",
        "    #trg = [trg sent len, batch size]\n",
        "    #teacher_forcing_ratio is probability to use teacher forcing\n",
        "    #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "    \n",
        "    batch_size = src.shape[1]\n",
        "    max_len = trg.shape[0]\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "    \n",
        "    #tensor to store decoder outputs\n",
        "    outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "    #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "    #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "    encoder_outputs, hidden = self.encoder(src)\n",
        "    #first input to the decoder is the <sos> tokens\n",
        "    output = trg[0,:]\n",
        "    \n",
        "    for t in range(1, max_len):\n",
        "      output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
        "      outputs[t] = output\n",
        "      teacher_force = random.random() < tfr\n",
        "      top1 = output.max(1)[1]\n",
        "      output = (trg[t] if teacher_force else top1)\n",
        "      \n",
        "    return outputs\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zy6w4CVmQqD",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "import random\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "OUTPUT_DIM = len(TEXT.vocab)\n",
        "ENC_EMB_DIM = 50\n",
        "DEC_EMB_DIM = 50\n",
        "ENC_HID_DIM = 50\n",
        "DEC_HID_DIM = 50\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = NMTAttention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = EncoderNMT(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = NMTDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = NMTSeq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXdTzEF7mcKj",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "b7bb660a-06b8-4a5a-b502-533c554c3d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "#@title\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMTSeq2Seq(\n",
              "  (encoder): EncoderNMT(\n",
              "    (embedding): Embedding(8004, 50)\n",
              "    (rnn): GRU(50, 50, bidirectional=True)\n",
              "    (fc): Linear(in_features=100, out_features=50, bias=True)\n",
              "    (dropout): Dropout(p=0.5)\n",
              "  )\n",
              "  (decoder): NMTDecoder(\n",
              "    (attention): NMTAttention(\n",
              "      (attn): Linear(in_features=150, out_features=50, bias=True)\n",
              "    )\n",
              "    (embedding): Embedding(8004, 50)\n",
              "    (rnn): GRU(150, 50)\n",
              "    (out): Linear(in_features=200, out_features=8004, bias=True)\n",
              "    (dropout): Dropout(p=0.5)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmvJwrKPm2F4",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "6588c8f2-3b8b-4453-c0c1-13cb942ea4f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,482,754 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrMUvOYOnCCV",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "PAD_IDX = TEXT.vocab.stoi['<pad>']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbBfKwZTnxYt",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        if i < iterator.batch_size:\n",
        "          src = batch.article\n",
        "          trg = batch.summary\n",
        "          print(\"Iter...{}\".format(i))\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          output = model(src, trg)\n",
        "\n",
        "          #trg = [trg sent len, batch size]\n",
        "          #output = [trg sent len, batch size, output dim]\n",
        "\n",
        "          output = output[1:].view(-1, output.shape[-1])\n",
        "          trg = trg[1:].view(-1)\n",
        "\n",
        "          #trg = [(trg sent len - 1) * batch size]\n",
        "          #output = [(trg sent len - 1) * batch size, output dim]\n",
        "\n",
        "          loss = criterion(output, trg)\n",
        "\n",
        "          loss.backward()\n",
        "\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          epoch_loss += loss.item()\n",
        "          print(\"Iter...{} and loss {}\".format(i, loss.item()))\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl2Rgi5Yn6w-",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "          if i < iterator.batch_size:\n",
        "\n",
        "            src = batch.article\n",
        "            trg = batch.summary\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg sent len, batch size]\n",
        "            #output = [trg sent len, batch size, output dim]\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg sent len - 1) * batch size]\n",
        "            #output = [(trg sent len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            print(\"Validation: Iter...{} and loss {}\".format(i, loss.item()))\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbvgOaMKn8si",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsaxJg84n-sP",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "b8afd442-1a94-4a53-e837-07d15f7d62db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1012
        }
      },
      "source": [
        "#@title\n",
        "import math\n",
        "import time\n",
        "\n",
        "N_EPOCHS = 2\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    print(\"Epoch...{}\".format(epoch))\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch...0\n",
            "Iter...0\n",
            "Iter...0 and loss 6.697270393371582\n",
            "Iter...1\n",
            "Iter...1 and loss 6.594736576080322\n",
            "Iter...2\n",
            "Iter...2 and loss 6.386347770690918\n",
            "Iter...3\n",
            "Iter...3 and loss 6.596447467803955\n",
            "Iter...4\n",
            "Iter...4 and loss 6.547032356262207\n",
            "Iter...5\n",
            "Iter...5 and loss 6.75457763671875\n",
            "Iter...6\n",
            "Iter...6 and loss 6.582798480987549\n",
            "Iter...7\n",
            "Iter...7 and loss 6.5457000732421875\n",
            "Validation: Iter...0 and loss 6.3806939125061035\n",
            "Validation: Iter...1 and loss 6.297412872314453\n",
            "Validation: Iter...2 and loss 6.585784435272217\n",
            "Validation: Iter...3 and loss 6.369948387145996\n",
            "Validation: Iter...4 and loss 6.566341876983643\n",
            "Validation: Iter...5 and loss 6.342964172363281\n",
            "Validation: Iter...6 and loss 6.522686004638672\n",
            "Validation: Iter...7 and loss 6.137399196624756\n",
            "Epoch: 01 | Time: 0m 35s\n",
            "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
            "\t Val. Loss: 0.914 |  Val. PPL:   2.495\n",
            "Epoch...1\n",
            "Iter...0\n",
            "Iter...0 and loss 6.474645137786865\n",
            "Iter...1\n",
            "Iter...1 and loss 6.434267044067383\n",
            "Iter...2\n",
            "Iter...2 and loss 6.703467845916748\n",
            "Iter...3\n",
            "Iter...3 and loss 6.449825286865234\n",
            "Iter...4\n",
            "Iter...4 and loss 6.453035354614258\n",
            "Iter...5\n",
            "Iter...5 and loss 6.642396450042725\n",
            "Iter...6\n",
            "Iter...6 and loss 6.486896991729736\n",
            "Iter...7\n",
            "Iter...7 and loss 6.6140336990356445\n",
            "Validation: Iter...0 and loss 6.37753963470459\n",
            "Validation: Iter...1 and loss 6.285377025604248\n",
            "Validation: Iter...2 and loss 6.576964378356934\n",
            "Validation: Iter...3 and loss 6.361161231994629\n",
            "Validation: Iter...4 and loss 6.5604143142700195\n",
            "Validation: Iter...5 and loss 6.336118698120117\n",
            "Validation: Iter...6 and loss 6.517699718475342\n",
            "Validation: Iter...7 and loss 6.124942779541016\n",
            "Epoch: 02 | Time: 0m 45s\n",
            "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
            "\t Val. Loss: 0.913 |  Val. PPL:   2.492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-ju2gN8p7lq",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "6e95bd81-6c6f-4f9f-b153-f70aeea7c387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "#@title\n",
        "model.load_state_dict(torch.load('tut3-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation: Iter...0 and loss 6.299138069152832\n",
            "Validation: Iter...1 and loss 6.552175998687744\n",
            "Validation: Iter...2 and loss 6.621049880981445\n",
            "Validation: Iter...3 and loss 6.262699127197266\n",
            "Validation: Iter...4 and loss 6.4683990478515625\n",
            "Validation: Iter...5 and loss 6.356363773345947\n",
            "Validation: Iter...6 and loss 6.285589218139648\n",
            "Validation: Iter...7 and loss 6.335705757141113\n",
            "| Test Loss: 1.137 | Test PPL:   3.119 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AtleoqDrEkH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Packed Padded Sequences, Masking and Inference { form-width: \"5px\" }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8OXSAdzrkRg",
        "colab_type": "text"
      },
      "source": [
        "we will be adding a few improvements - packed padded sequences and masking - to the model from the previous.\n",
        "Packed padded sequences are used to tell our RNN to skip over padding tokens in our encoder. Masking explicitly forces the model to ignore certain values, such as attention over padded elements. Both of these techniques are commonly used in NLP.\n",
        "\n",
        "We will also look at how to use our model for inference, by giving it a sentence, seeing what it translates it as and seeing where exactly it pays attention to when translating each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTFmjkC8rdj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.datasets import TranslationDataset, Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9isjILH0ryxO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "When using packed padded sequences, we need to tell PyTorch how long the actual (non-padded) sequences are. Luckily for us, TorchText's Field objects allow us to use the include_lengths argument, this will cause our batch.src to be a tuple. The first element of the tuple is the same as before, a batch of numericalized source sentence as a tensor, and the second element is the non-padded lengths of each source sentence within the batch.\n",
        "\n",
        "One quirk about packed padded sequences is that all elements in the batch need to be sorted by their non-padded lengths in descending order, i.e. the first sentence in the batch needs to be the longest. We use two arguments of the iterator to handle this, sort_within_batch which tells the iterator that the contents of the batch need to be sorted, and sort_key a function which tells the iterator how to sort the elements in the batch. Here, we sort by the length of the src sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZofc7cwrsDe",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Torchtext Padded { form-width: \"5px\" }\n",
        "\n",
        "#get data\n",
        "SRC = Field(tokenize = 'spacy', init_token='<sos>',\n",
        "                  eos_token='<eos>', lower=True, include_lengths=True)\n",
        "\n",
        "TRG = Field(tokenize = 'spacy', init_token='<sos>',\n",
        "                  eos_token='<eos>', lower=True)\n",
        "\n",
        "MAX_VOCAB_SIZE = 8000\n",
        "\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(path='./',\n",
        "                                                              train='train.csv',\n",
        "                                                              validation='valid.csv',\n",
        "                                                              test='test.csv',\n",
        "                                                              format='csv',\n",
        "                                                              fields=[('src', SRC), \n",
        "                                                                      ('trg', TRG)])\n",
        "\n",
        "SRC.build_vocab(train_data, vectors='glove.6B.50d', max_size = MAX_VOCAB_SIZE, min_freq=2)\n",
        "TRG.build_vocab(train_data, vectors='glove.6B.50d', max_size = MAX_VOCAB_SIZE, min_freq=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgHDlBxSMOm8",
        "colab_type": "code",
        "outputId": "ff65e1ec-454d-45d8-b439-4232eddbda34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(TRG.vocab), len(SRC.vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8004, 8004)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH1hCi2h5Yjl",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title  { form-width: \"5px\" }\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.src),\n",
        "     device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwwVfuFB7TZl",
        "colab_type": "text"
      },
      "source": [
        "The changes here all within the forward method. It now accepts the lengths of the source sentences as well as the sentences themselves.\n",
        "\n",
        "After the source sentence (padded automatically within the iterator) has been embedded, we can then use pack_padded_sequence on it with the lengths of the sentences. packed_embedded will then be our packed padded sequence. This can be then fed to our RNN as normal which will return packed_outputs, a packed tensor containing all of the hidden states from the sequence, and hidden which is simply the final hidden state from our sequence. hidden is a standard tensor and not packed in any way, the only difference is that as the input was a packed sequence, this tensor is from the final non-padded element in the sequence.\n",
        "\n",
        "We then unpack our packed_outputs using pad_packed_sequence which returns the outputs and the lengths of each, which we don't need.\n",
        "\n",
        "The first dimension of outputs is the padded sequence lengths however due to using a packed padded sequence the values of tensors when a padding token was the input will be all zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToLBKY925ssT",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Padded Encoder { form-width: \"5px\" }\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class PaddedEncoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.input_dim = input_dim\n",
        "    self.emb_dim = emb_dim\n",
        "    self.enc_hid_dim = enc_hid_dim\n",
        "    self.dec_hid_dim = dec_hid_dim\n",
        "    self.dropout = dropout\n",
        "    \n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    \n",
        "    self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n",
        "    \n",
        "    self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "    \n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "  def forward(self, src, src_len):\n",
        "    #src = [src sent len, batch size]\n",
        "    #src_len = [src sent len]\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "    #embedded = [src sent len, batch size, emb dim]\n",
        "    packed_embedded = pack_padded_sequence(embedded, src_len)\n",
        "    packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "    #packed_outputs is a packed sequence containing all hidden states\n",
        "    #hidden is now from the final non-padded element in the batch\n",
        "    outputs, _ = pad_packed_sequence(packed_outputs)\n",
        "    #outputs is now a non-packed sequence, all hidden states obtained\n",
        "    #  when the input is a pad token are all zeros\n",
        "            \n",
        "    #outputs = [sent len, batch size, hid dim * num directions]\n",
        "    #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "    #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "    #outputs are always from the last layer\n",
        "        \n",
        "    #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "    #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "    #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "    #  encoder RNNs fed through a linear layer\n",
        "    hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
        "    #outputs = [sent len, batch size, enc hid dim * 2]\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rThYpqMq8-cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Padded Attention { form-width: \"5px\" }\n",
        "class PaddedAttention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.enc_hid_dim = enc_hid_dim\n",
        "    self.dec_hid_dim = dec_hid_dim\n",
        "    \n",
        "    self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "    self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
        "    \n",
        "  def forward(self, hidden, encoder_outputs, mask):\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "    #mask = [batch size, src sent len]\n",
        "    batch_size = encoder_outputs.shape[1]\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "    #repeat encoder hidden state src_len times\n",
        "    hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "    \n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "    #hidden = [batch size, src sent len, dec hid dim]\n",
        "    #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "    \n",
        "    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))\n",
        "    #energy = [batch size, src sent len, dec hid dim]\n",
        "    energy = energy.permute(0, 2, 1)\n",
        "    #energy = [batch size, dec hid dim, src sent len]\n",
        "    #v = [dec hid dim]\n",
        "    v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
        "    #v = [batch size, 1, dec hid dim]\n",
        "    #squeeze gets rid of that dimension\n",
        "    attention = torch.bmm(v, energy).squeeze(1)\n",
        "    #attention = [batch size, src sent len]\n",
        "    attention = attention.masked_fill(mask == 0, -1e10)\n",
        "    return F.softmax(attention, dim = 1)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGSx8dmwEKvx",
        "colab_type": "text"
      },
      "source": [
        "The decoder only needs a few small changes. It needs to accept a mask over the source sentence and pass this to the attention module. As we want to view the values of attention during inference, we also return the attention tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg8t9i5cB9bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Padded Decoder { form-width: \"5px\" }\n",
        "\n",
        "class PaddedDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        #mask = [batch size, src sent len]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "        \n",
        "        #a = [batch size, src sent len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a = [batch size, 1, src sent len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "        \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [sent len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert(output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        output = self.out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        #output = [bsz, output dim]\n",
        "        return output, hidden.squeeze(0), a.squeeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ge-5eMYHEzr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The overarching seq2seq model also needs a few changes for packed padded sequences, masking and inference.\n",
        "\n",
        "We need to tell it what the indexes are for the pad token, sos token and the eos token and also pass the source sentence lengths as input to the forward method.\n",
        "\n",
        "We use the pad token index to create the masks, by creating a mask tensor that is 1 wherever the source sentence is not equal to the pad token. This is all done within the create_mask function.\n",
        "\n",
        "To **use this model for inference, we simply pass a target sentence, trg, of None**. This will set inference to true and **create a fake trg tensor filled with <sos> tokens**. We need to fill it with <sos> tokens as **one needs to be passed to the decoder to start the decoding**, the rest are never used as we assert the teacher forcing ratio is 0 and** thus the model only ever uses its own predictions.** We set the dummy target tensor to have a max length of 100, meaning that is the maximum number of target tokens we will attempt to output.\n",
        "\n",
        "We also create an attentions tensor to store the values of attention for inference.\n",
        "\n",
        "Within the decoder loop, while doing inference, we check if the decoded token is the <eos> token, and if so we immediately stop decoding and return the translation and attentions generated so far."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6f9yNAkF-dt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Padded Seq2Seq { form-width: \"5px\" }\n",
        "\n",
        "class PaddedSeq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, pad_idx, sos_idx, eos_idx, device):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.pad_idx = pad_idx\n",
        "    self.sos_idx = sos_idx\n",
        "    self.eos_idx = eos_idx\n",
        "    self.device = device\n",
        "    \n",
        "  def create_mask(self, src):\n",
        "    mask = (src != self.pad_idx).permute(1, 0)\n",
        "    return mask\n",
        "  \n",
        "  def forward(self, src, src_len, trg, tfr=0.5):\n",
        "    #src = [src sent len, batch size]\n",
        "    #src_len = [batch size]\n",
        "    #trg = [trg sent len, batch size]\n",
        "    #teacher_forcing_ratio is probability to use teacher forcing\n",
        "    #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "    if trg is None:\n",
        "      assert tfr == 0, \"Must be zero during inference\"\n",
        "      inference = True\n",
        "      #set 100 to whatever max len you want\n",
        "      trg = torch.zeros((100, src.shape[1])).long().fill_(self.sos_idx).to(src.device)\n",
        "    else:\n",
        "      inference = False\n",
        "      \n",
        "    batch_size = src.shape[1]\n",
        "    max_len = trg.shape[0]\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "    #tensor to store decoder outputs\n",
        "    outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "    #tensor to store attention\n",
        "    attentions = torch.zeros(max_len, batch_size, src.shape[0]).to(self.device)\n",
        "    #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "    #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "    encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "    #first input to the decoder is <sos> token\n",
        "    output = trg[0,:]\n",
        "    mask = self.create_mask(src)\n",
        "    #mask = [batch size, src sent len]\n",
        "    for t in range(1, max_len):\n",
        "      output, hidden, attention = self.decoder(output, hidden, encoder_outputs, mask)\n",
        "      outputs[t] = output\n",
        "      attentions[t] = attention\n",
        "      teacher_force = random.random() < tfr\n",
        "      top1 = output.max(1)[1]\n",
        "      output = (trg[t] if teacher_force else top1)\n",
        "      if inference and output.item() == self.eos_idx:\n",
        "        return outputs[:t], attentions[:t]\n",
        "    return outputs, attentions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2T5k1PJLszs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 50\n",
        "DEC_EMB_DIM = 50\n",
        "ENC_HID_DIM = 50\n",
        "DEC_HID_DIM = 50\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "PAD_IDX = SRC.vocab.stoi['<pad>']\n",
        "SOS_IDX = TRG.vocab.stoi['<sos>']\n",
        "EOS_IDX = TRG.vocab.stoi['<eos>']\n",
        "\n",
        "\n",
        "attn = PaddedAttention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = PaddedEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = PaddedDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = PaddedSeq2Seq(enc, dec, PAD_IDX, SOS_IDX, EOS_IDX, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23XSiRd-L9JI",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "7cfd39ab-961b-4e06-af98-977a888ab34c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "#@title\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PaddedSeq2Seq(\n",
              "  (encoder): PaddedEncoder(\n",
              "    (embedding): Embedding(8004, 50)\n",
              "    (rnn): GRU(50, 50, bidirectional=True)\n",
              "    (fc): Linear(in_features=100, out_features=50, bias=True)\n",
              "    (dropout): Dropout(p=0.5)\n",
              "  )\n",
              "  (decoder): PaddedDecoder(\n",
              "    (attention): PaddedAttention(\n",
              "      (attn): Linear(in_features=150, out_features=50, bias=True)\n",
              "    )\n",
              "    (embedding): Embedding(8004, 50)\n",
              "    (rnn): GRU(150, 50)\n",
              "    (out): Linear(in_features=200, out_features=8004, bias=True)\n",
              "    (dropout): Dropout(p=0.5)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otYzidABL-vo",
        "colab_type": "code",
        "outputId": "68b18dc0-2488-4fac-cc17-bb5453f5c9e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,482,754 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpJEVYH6MAwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEY_qzptMZEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "      if i < iterator.batch_size:\n",
        "        \n",
        "        src, src_len = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, attetion = model(src, src_len, trg)\n",
        "        \n",
        "        #trg = [trg sent len, batch size]\n",
        "        #output = [trg sent len, batch size, output dim]\n",
        "        \n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg sent len - 1) * batch size]\n",
        "        #output = [(trg sent len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        print(\"Iter...{} and loss {}\".format(i, loss.item()))\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXng_OAXMgmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "          if i < iterator.batch_size:\n",
        "\n",
        "            src, src_len = batch.src\n",
        "            trg = batch.trg\n",
        "            \n",
        "\n",
        "            output, attention = model(src, src_len, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg sent len, batch size]\n",
        "            #output = [trg sent len, batch size, output dim]\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg sent len - 1) * batch size]\n",
        "            #output = [(trg sent len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            print(\"Iter...{} and loss {}\".format(i, loss.item()))\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDqMBrnnMke2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHVATHGHMmDF",
        "colab_type": "code",
        "outputId": "1a0eadfb-b4c9-4d3a-c6cd-4fdd3d91f14d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        }
      },
      "source": [
        "N_EPOCHS = 2\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter...0 and loss 9.064964294433594\n",
            "Iter...1 and loss 9.044617652893066\n",
            "Iter...2 and loss 9.031811714172363\n",
            "Iter...3 and loss 9.023863792419434\n",
            "Iter...4 and loss 8.971464157104492\n",
            "Iter...5 and loss 8.918035507202148\n",
            "Iter...6 and loss 8.858675003051758\n",
            "Iter...7 and loss 8.800673484802246\n",
            "Iter...0 and loss 8.56302261352539\n",
            "Iter...1 and loss 8.548050880432129\n",
            "Iter...2 and loss 8.599295616149902\n",
            "Iter...3 and loss 8.563963890075684\n",
            "Iter...4 and loss 8.533535957336426\n",
            "Iter...5 and loss 8.540865898132324\n",
            "Iter...6 and loss 8.527188301086426\n",
            "Iter...7 and loss 8.532539367675781\n",
            "Epoch: 01 | Time: 0m 13s\n",
            "\tTrain Loss: 0.401 | Train PPL:   1.493\n",
            "\t Val. Loss: 1.222 |  Val. PPL:   3.393\n",
            "Iter...0 and loss 8.700028419494629\n",
            "Iter...1 and loss 8.610822677612305\n",
            "Iter...2 and loss 8.541361808776855\n",
            "Iter...3 and loss 8.473971366882324\n",
            "Iter...4 and loss 8.31816577911377\n",
            "Iter...5 and loss 8.154869079589844\n",
            "Iter...6 and loss 8.053915977478027\n",
            "Iter...7 and loss 7.921507358551025\n",
            "Iter...0 and loss 7.492670059204102\n",
            "Iter...1 and loss 7.436717510223389\n",
            "Iter...2 and loss 7.5244269371032715\n",
            "Iter...3 and loss 7.49055290222168\n",
            "Iter...4 and loss 7.412436485290527\n",
            "Iter...5 and loss 7.460485935211182\n",
            "Iter...6 and loss 7.376376152038574\n",
            "Iter...7 and loss 7.447419166564941\n",
            "Epoch: 02 | Time: 0m 22s\n",
            "\tTrain Loss: 0.373 | Train PPL:   1.452\n",
            "\t Val. Loss: 1.065 |  Val. PPL:   2.901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq_WShR2Mxqz",
        "colab_type": "code",
        "outputId": "49ad8c06-6132-475b-9dd5-d13fa32736cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "model.load_state_dict(torch.load('tut4-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter...0 and loss 7.6259870529174805\n",
            "Iter...1 and loss 7.492910861968994\n",
            "Iter...2 and loss 7.420287609100342\n",
            "Iter...3 and loss 7.512353897094727\n",
            "Iter...4 and loss 7.548835277557373\n",
            "Iter...5 and loss 7.530538558959961\n",
            "Iter...6 and loss 7.495456218719482\n",
            "Iter...7 and loss 7.568209171295166\n",
            "| Test Loss: 1.338 | Test PPL:   3.810 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaEcsIIWaefA",
        "colab_type": "text"
      },
      "source": [
        "- ensure our model is in evaluation mode, which it should always be for inference\n",
        "\n",
        "- tokenize our input/src sentence\n",
        "\n",
        "- lowercase our tokens and append the start and end of sequence tokens\n",
        "\n",
        "- use our vocabulary to numericalize our tokens by converting them into their indexes\n",
        "\n",
        "- get the sentence length and convert into a tensor\n",
        "\n",
        "- convert the numericalized sentence into a tensor, add a batch dimension and place on GPU\n",
        "\n",
        "- pass inputs into the model, making sure trg is set to None for inference and the teacher forcing ratio is zero\n",
        "\n",
        " - this gives us the raw (unnormalized) predictions for each token in our target sequence\n",
        "\n",
        "- get the highest predicted token index for each element in the target sequence using argmax\n",
        "\n",
        "- convert these indexes into strings\n",
        "\n",
        "- as the first element in our output and attention tensors from our models are all zeros, we trim these before returning them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdJFIfFgbv08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, batch in enumerate(test_iterator):\n",
        "  if i < 1:\n",
        "    for j in batch.trg:\n",
        "      print(j)\n",
        "      #TRG.vocab.itos[j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExMWqCRwcrMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_gen = BatchGenerator(test_iterator, 'src', 'trg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBQ5zWmgcw3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_gen = next(iter(test_gen))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL_Wf898c4VJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_gen[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOpEuYJtaTpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title inference { form-width: \"5px\" }\n",
        "\n",
        "def translate_sentence(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = nltk_tokenizer(sentence)\n",
        "    tokenized = ['<sos>'] + [t.lower() for t in tokenized] + ['<eos>']\n",
        "    numericalized = [SRC.vocab.stoi[t] for t in tokenized] \n",
        "    sentence_length = torch.LongTensor([len(numericalized)]).to(device) \n",
        "    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(device) \n",
        "    translation_tensor_logits, attention = model(tensor, sentence_length, None, 0) \n",
        "    translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1)\n",
        "    translation = [TRG.vocab.itos[t] for t in translation_tensor]\n",
        "    translation, attention = translation[1:], attention[1:]\n",
        "    return translation, attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXxHbI2vdz61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_attention(candidate, translation, attention):\n",
        "    \n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "    \n",
        "    cax = ax.matshow(attention, cmap='bone')\n",
        "   \n",
        "    ax.tick_params(labelsize=15)\n",
        "    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in nltk_tokenizer(candidate)] + ['<eos>'], \n",
        "                       rotation=45)\n",
        "    ax.set_yticklabels([''] + translation)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecrYy2MUd5YX",
        "colab_type": "code",
        "outputId": "77cea210-2992-48dc-af6b-a4ea7ccb49b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "example_idx = 10\n",
        "\n",
        "src = ' '.join(vars(train_data.examples[example_idx])['src'])\n",
        "trg = ' '.join(vars(train_data.examples[example_idx])['trg'])\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = cole faces lengthy injury lay - off , , \" aston villas carlton cole could be out for six weeks with a knee injury . \" , , the striker , who is on a season - long loan from chelsea , picked up the knock in an england under-21 match against holland earlier this month . \" carlton will be out of action for four to six weeks after a bad challenge , \" said villa boss david o\\leary . \" i won\\t be able to tell you whether he will need an operation until maybe next week . whether he has an operation has got to be left to chelsea . \" cole , who also struggled with an ankle problem earlier in the season , was unable to rest because o\\leary had a shortage of strikers . the return to fitness of darius vassell after four months out with a broken ankle and the emergence of luke moore has alleviated some of the villa\\s manager\\s problems in that department .\n",
            "trg = aston villa\\s carlton cole could be out for six weeks with a knee injury . the return to fitness of darius vassell after four months out with a broken ankle and the emergence of luke moore has alleviated some of the villa\\s manager\\s problems in that department.\"carlton will be out of action for four to six weeks after a bad challenge , \" said villa boss david o\\leary .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "487azJDmd7cw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translation, attention = translate_sentence(model, src)\n",
        "\n",
        "print(f'predicted trg = {translation}')\n",
        "\n",
        "display_attention(src, translation, attention)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWRUhaZkexK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokeniz = nltk_tokenizer(src)\n",
        "tokeniz = ['<sos>'] + [t.lower() for t in tokeniz] + ['<eos>']\n",
        "numericali = [SRC.vocab.stoi[t] for t in tokeniz]\n",
        "sentence_length = torch.LongTensor([len(numericali)]).to(device) \n",
        "tensor = torch.LongTensor(numericali).unsqueeze(1).to(device) \n",
        "tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaLRiMehfO3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translation_tensor_logits, attention = model(tensor, sentence_length, None, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGsoiVVYfgdk",
        "colab_type": "code",
        "outputId": "b49628b6-be19-46f5-cad2-f2b008077d7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "translation_tensor_logits.shape, attention.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([100, 1, 8004]), torch.Size([100, 1, 323]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 271
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqugdH7pfaHa",
        "colab_type": "code",
        "outputId": "44ad0451-9251-4b17-d6d5-d09fb9a654ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1)\n",
        "translation_tensor_logits"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "        [[ 0.9199, -0.7790, -0.3452,  ..., -0.9069, -0.8112, -0.9310]],\n",
              "\n",
              "        [[ 1.8919, -1.0777, -0.9501,  ..., -1.1786, -0.7049, -1.5094]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 2.6082, -1.0976, -1.0294,  ..., -1.6108, -1.4187, -1.6307]],\n",
              "\n",
              "        [[ 2.6082, -1.0976, -1.0294,  ..., -1.6108, -1.4187, -1.6307]],\n",
              "\n",
              "        [[ 2.6082, -1.0976, -1.0294,  ..., -1.6108, -1.4187, -1.6307]]],\n",
              "       device='cuda:0', grad_fn=<CopySlices>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFhR20mCfUhE",
        "colab_type": "code",
        "outputId": "6cba25c5-dd75-40a2-a4fe-613ad7b1eb7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "translation_tensor_logits.shape, attention.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([100, 1, 8004]), torch.Size([100, 1, 323]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1alnQWuvf7ng",
        "colab_type": "code",
        "outputId": "94c8e876-3ff6-4e9b-a34c-4b73be313a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "translation_tensor.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 278
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAJpHkfUfpKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for t in translation_tensor:\n",
        "  print(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwNJI0FPfksr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translation = [TRG.vocab.itos[t] for t in translation]\n",
        "translation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_lAKoBZd_CQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_infer_art(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = nltk_tokenizer(sentence)\n",
        "    tokenized = ['<sos>'] + [t.lower() for t in tokenized] + ['<eos>']\n",
        "    numericalized = [SRC.vocab.stoi[t] for t in tokenized] \n",
        "    sentence_length = torch.LongTensor([len(numericalized)]).to(device) \n",
        "    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(device) \n",
        "    translation_tensor_logits, attention = model(tensor, sentence_length, None, 0) \n",
        "    translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1)\n",
        "    translation = [TRG.vocab.itos[t] for t in translation_tensor]\n",
        "    translation, attention = translation[1:], attention[1:]\n",
        "    return translation, attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDn7AHjZgGt5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Conv seq to seq\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g9DeplfkTr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchtext.data as data\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import TranslationDataset, Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weY6-jgoix54",
        "colab_type": "code",
        "outputId": "2215c0b0-cf90-4703-a956-a68e726855d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(torch.__version__)\n",
        "print(torchtext.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.0\n",
            "0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbD2w-YfjqA6",
        "colab_type": "code",
        "outputId": "de39a3d9-82b1-4bef-db40-4705707d3ea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "!pip install torchtext==0.3.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.3.0\n",
            "\u001b[31m  Could not find a version that satisfies the requirement torchtext==0.3.0 (from versions: 0.1.1, 0.2.0, 0.2.1, 0.2.3, 0.3.1)\u001b[0m\n",
            "\u001b[31mNo matching distribution found for torchtext==0.3.0\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JODVxevgi5Pe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WT15pE7gNP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC = Field(tokenize='spacy', \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)\n",
        "\n",
        "TRG = Field(tokenize='spacy', \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW1hu0k6gPt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 15000\n",
        "\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(path='./',\n",
        "                                                              train='train.csv',\n",
        "                                                              validation='valid.csv',\n",
        "                                                              test='test.csv',\n",
        "                                                              format='csv',\n",
        "                                                              fields=[('src', SRC), \n",
        "                                                                      ('trg', TRG)])\n",
        "\n",
        "SRC.build_vocab(train_data, vectors='glove.6B.50d', max_size = MAX_VOCAB_SIZE, min_freq=2)\n",
        "TRG.build_vocab(train_data, vectors='glove.6B.50d', max_size = MAX_VOCAB_SIZE, min_freq=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bto2rIEJglzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title  { form-width: \"5px\" }\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwi9jibOgv_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title CNN Encoder { form-width: \"5px\" }\n",
        "\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, kernel_size, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.pos_embedding = nn.Embedding(100, emb_dim)\n",
        "        \n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
        "                                              out_channels = 2 * hid_dim, \n",
        "                                              kernel_size = kernel_size, \n",
        "                                              padding = (kernel_size - 1) // 2)\n",
        "                                    for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [batch size, src sent len]\n",
        "        \n",
        "        #create position tensor\n",
        "        pos = torch.arange(0, src.shape[1]).unsqueeze(0).repeat(src.shape[0], 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src sent len]\n",
        "        \n",
        "        #embed tokens and positions\n",
        "        tok_embedded = self.tok_embedding(src)\n",
        "        pos_embedded = self.pos_embedding(pos)\n",
        "        \n",
        "        #tok_embedded = pos_embedded = [batch size, src sent len, emb dim]\n",
        "        \n",
        "        #combine embeddings by elementwise summing\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
        "        \n",
        "        #embedded = [batch size, src sent len, emb dim]\n",
        "        \n",
        "        #pass embedded through linear layer to go through emb dim -> hid dim\n",
        "        conv_input = self.emb2hid(embedded)\n",
        "        \n",
        "        #conv_input = [batch size, src sent len, hid dim]\n",
        "        \n",
        "        #permute for convolutional layer\n",
        "        conv_input = conv_input.permute(0, 2, 1) \n",
        "        \n",
        "        #conv_input = [batch size, hid dim, src sent len]\n",
        "        \n",
        "        for i, conv in enumerate(self.convs):\n",
        "        \n",
        "            #pass through convolutional layer\n",
        "            conved = conv(self.dropout(conv_input))\n",
        "\n",
        "            #conved = [batch size, 2*hid dim, src sent len]\n",
        "\n",
        "            #pass through GLU activation function\n",
        "            conved = F.glu(conved, dim = 1)\n",
        "\n",
        "            #conved = [batch size, hid dim, src sent len]\n",
        "            \n",
        "            #apply residual connection\n",
        "            conved = (conved + conv_input) * self.scale\n",
        "\n",
        "            #conved = [batch size, hid dim, src sent len]\n",
        "            \n",
        "            #set conv_input to conved for next loop iteration\n",
        "            conv_input = conved\n",
        "        \n",
        "        #permute and convert back to emb dim\n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
        "        \n",
        "        #conved = [batch size, src sent len, emb dim]\n",
        "        \n",
        "        #elementwise sum output (conved) and input (embedded) to be used for attention\n",
        "        combined = (conved + embedded) * self.scale\n",
        "        \n",
        "        #combined = [batch size, src sent len, emb dim]\n",
        "        \n",
        "        return conved, combined\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBZKtNSnhbcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title CNN Decoder { form-width: \"5px\" }\n",
        "\n",
        "class CNNDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, kernel_size, dropout, pad_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dropout = dropout\n",
        "        self.pad_idx = pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.pos_embedding = nn.Embedding(100, emb_dim)\n",
        "        \n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        \n",
        "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        \n",
        "        self.out = nn.Linear(emb_dim, output_dim)\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv1d(hid_dim, 2*hid_dim, kernel_size)\n",
        "                                    for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "      \n",
        "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
        "        \n",
        "        #embedded = [batch size, trg sent len, emb dim]\n",
        "        #conved = [batch size, hid dim, trg sent len]\n",
        "        #encoder_conved = encoder_combined = [batch size, src sent len, emb dim]\n",
        "        \n",
        "        #permute and convert back to emb dim\n",
        "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
        "        \n",
        "        #conved_emb = [batch size, trg sent len, emb dim]\n",
        "        \n",
        "        combined = (embedded + conved_emb) * self.scale\n",
        "        \n",
        "        #combined = [batch size, trg sent len, emb dim]\n",
        "                \n",
        "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
        "        \n",
        "        #energy = [batch size, trg sent len, src sent len]\n",
        "        \n",
        "        attention = F.softmax(energy, dim=2)\n",
        "        \n",
        "        #attention = [batch size, trg sent len, src sent len]\n",
        "            \n",
        "        attended_encoding = torch.matmul(attention, (encoder_conved + encoder_combined))\n",
        "        \n",
        "        #attended_encoding = [batch size, trg sent len, emd dim]\n",
        "        \n",
        "        #convert from emb dim -> hid dim\n",
        "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
        "        \n",
        "        #attended_encoding = [batch size, trg sent len, hid dim]\n",
        "        \n",
        "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
        "        \n",
        "        #attended_combined = [batch size, hid dim, trg sent len]\n",
        "        \n",
        "        return attention, attended_combined\n",
        "        \n",
        "    def forward(self, trg, encoder_conved, encoder_combined):\n",
        "        \n",
        "        #trg = [batch size, trg sent len]\n",
        "        #encoder_conved = encoder_combined = [batch size, src sent len, emb dim]\n",
        "                \n",
        "        #create position tensor\n",
        "        pos = torch.arange(0, trg.shape[1]).unsqueeze(0).repeat(trg.shape[0], 1).to(device)\n",
        "        \n",
        "        #pos = [batch size, trg sent len]\n",
        "        \n",
        "        #embed tokens and positions\n",
        "        tok_embedded = self.tok_embedding(trg)\n",
        "        pos_embedded = self.pos_embedding(pos)\n",
        "        \n",
        "        #tok_embedded = [batch size, trg sent len, emb dim]\n",
        "        #pos_embedded = [batch size, trg sent len, emb dim]\n",
        "        \n",
        "        #combine embeddings by elementwise summing\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
        "        \n",
        "        #embedded = [batch size, trg sent len, emb dim]\n",
        "        \n",
        "        #pass embedded through linear layer to go through emb dim -> hid dim\n",
        "        conv_input = self.emb2hid(embedded)\n",
        "        \n",
        "        #conv_input = [batch size, trg sent len, hid dim]\n",
        "        \n",
        "        #permute for convolutional layer\n",
        "        conv_input = conv_input.permute(0, 2, 1) \n",
        "        \n",
        "        #conv_input = [batch size, hid dim, trg sent len]\n",
        "        \n",
        "        for i, conv in enumerate(self.convs):\n",
        "        \n",
        "            #apply dropout\n",
        "            conv_input = self.dropout(conv_input)\n",
        "        \n",
        "            #need to pad so decoder can't \"cheat\"\n",
        "            padding = torch.zeros(conv_input.shape[0], conv_input.shape[1], self.kernel_size-1).fill_(self.pad_idx).to(device)\n",
        "            padded_conv_input = torch.cat((padding, conv_input), dim=2)\n",
        "        \n",
        "            #padded_conv_input = [batch size, hid dim, trg sent len + kernel size - 1]\n",
        "        \n",
        "            #pass through convolutional layer\n",
        "            conved = conv(padded_conv_input)\n",
        "\n",
        "            #conved = [batch size, 2*hid dim, trg sent len]\n",
        "            \n",
        "            #pass through GLU activation function\n",
        "            conved = F.glu(conved, dim=1)\n",
        "\n",
        "            #conved = [batch size, hid dim, trg sent len]\n",
        "            \n",
        "            attention, conved = self.calculate_attention(embedded, conved, encoder_conved, encoder_combined)\n",
        "            \n",
        "            #attention = [batch size, trg sent len, src sent len]\n",
        "            #conved = [batch size, hid dim, trg sent len]\n",
        "            \n",
        "            #apply residual connection\n",
        "            conved = (conved + conv_input) * self.scale\n",
        "            \n",
        "            #conved = [batch size, hid dim, trg sent len]\n",
        "            \n",
        "            #set conv_input to conved for next loop iteration\n",
        "            conv_input = conved\n",
        "            \n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
        "         \n",
        "        #conved = [batch size, trg sent len, hid dim]\n",
        "            \n",
        "        output = self.out(self.dropout(conved))\n",
        "        \n",
        "        #output = [batch size, trg sent len, output dim]\n",
        "            \n",
        "        return output, attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGn9sAQUhg78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title CNN Seq2Seq { form-width: \"5px\" }\n",
        "class CNNSeq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src sent len]\n",
        "        #trg = [batch size, trg sent len]\n",
        "           \n",
        "        #calculate z^u (encoder_conved) and e (encoder_combined)\n",
        "        #encoder_conved is output from final encoder conv. block\n",
        "        #encoder_combined is encoder_conved plus (elementwise) src embedding plus positional embeddings \n",
        "        encoder_conved, encoder_combined = self.encoder(src)\n",
        "            \n",
        "        #encoder_conved = [batch size, src sent len, emb dim]\n",
        "        #encoder_combined = [batch size, src sent len, emb dim]\n",
        "        \n",
        "        #calculate predictions of next words\n",
        "        #output is a batch of predictions for each word in the trg sentence\n",
        "        #attention a batch of attention scores across the src sentence for each word in the trg sentence\n",
        "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
        "        \n",
        "        #output = [batch size, trg sent len, output dim]\n",
        "        #attention = [batch size, trg sent len, src sent len]\n",
        "        \n",
        "        return output, attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVUJ6XWyhmUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "EMB_DIM = 50\n",
        "HID_DIM = 100\n",
        "ENC_LAYERS = 10\n",
        "DEC_LAYERS = 10\n",
        "ENC_KERNEL_SIZE = 3\n",
        "DEC_KERNEL_SIZE = 3\n",
        "ENC_DROPOUT = 0.25\n",
        "DEC_DROPOUT = 0.25\n",
        "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
        "    \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "enc = CNNEncoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\n",
        "dec = CNNDecoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, PAD_IDX, device)\n",
        "\n",
        "model = CNNSeq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPOx-bakhuYy",
        "colab_type": "code",
        "outputId": "dc830b15-2719-40c3-8092-d278c85b8b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 3,058,887 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYLh0C1Bhziu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kme3ghorhyUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "      if i < iterator.batch_size:\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "        \n",
        "        #output = [batch size, trg sent len - 1, output dim]\n",
        "        #trg = [batch size, trg sent len]\n",
        "        \n",
        "        output = output.contiguous().view(-1, output.shape[-1])\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "        \n",
        "        #output = [batch size * trg sent len - 1, output dim]\n",
        "        #trg = [batch size * trg sent len - 1]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        print(\"Iter...{} and loss {}\".format(i, loss.item()))\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prTerZewh1x4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "          if i < iterator.batch_size:\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "        \n",
        "            #output = [batch size, trg sent len - 1, output dim]\n",
        "            #trg = [batch size, trg sent len]\n",
        "\n",
        "            output = output.contiguous().view(-1, output.shape[-1])\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "            #output = [batch size * trg sent len - 1, output dim]\n",
        "            #trg = [batch size * trg sent len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            print(\"Iter...{} and loss {}\".format(i, loss.item()))\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBsT9s_sh3Rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykaAjJK6h5nM",
        "colab_type": "code",
        "outputId": "a37b47f1-4a3d-43ad-8207-647dc224f6f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1254
        }
      },
      "source": [
        "N_EPOCHS = 3\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-9ebc4af5a311>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-cc342e3631ab>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#output = [batch size, trg sent len - 1, output dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-4b57009a0eac>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#encoder_conved is output from final encoder conv. block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#encoder_combined is encoder_conved plus (elementwise) src embedding plus positional embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mencoder_conved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#encoder_conved = [batch size, src sent len, emb dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-7d1878f8dd2b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#combine embeddings by elementwise summing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_embedded\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_embedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#embedded = [batch size, src sent len, emb dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    747\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[1;32m    748\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Creating MTGP constants failed. at /pytorch/aten/src/THC/THCTensorRandom.cu:35"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8K7dJwsiJmH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Attention is all you need"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOpHgHZcjH4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, encoder_layer, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.pf_dim = pf_dim\n",
        "        self.encoder_layer = encoder_layer\n",
        "        self.self_attention = self_attention\n",
        "        self.positionwise_feedforward = positionwise_feedforward\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(1000, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([encoder_layer(hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src sent len]\n",
        "        #src_mask = [batch size, src sent len]\n",
        "        \n",
        "        pos = torch.arange(0, src.shape[1]).unsqueeze(0).repeat(src.shape[0], 1).to(self.device)\n",
        "        \n",
        "        src = self.do((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src sent len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        return src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp7pk-emjSLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.ln = nn.LayerNorm(hid_dim)\n",
        "        self.sa = self_attention(hid_dim, n_heads, dropout, device)\n",
        "        self.pf = positionwise_feedforward(hid_dim, pf_dim, dropout)\n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src sent len, hid dim]\n",
        "        #src_mask = [batch size, src sent len]\n",
        "        \n",
        "        src = self.ln(src + self.do(self.sa(src, src, src, src_mask)))\n",
        "        \n",
        "        src = self.ln(src + self.do(self.pf(src)))\n",
        "        \n",
        "        return src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZCgbSNmjTU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.w_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.w_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.w_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads]))#.to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \n",
        "        bsz = query.shape[0]\n",
        "        \n",
        "        #query = key = value [batch size, sent len, hid dim]\n",
        "                \n",
        "        Q = self.w_q(query)\n",
        "        K = self.w_k(key)\n",
        "        V = self.w_v(value)\n",
        "        \n",
        "        #Q, K, V = [batch size, sent len, hid dim]\n",
        "        \n",
        "        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        K = K.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        V = V.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q, K, V = [batch size, n heads, sent len, hid dim // n heads]\n",
        "        \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, sent len, sent len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = self.do(F.softmax(energy, dim=-1))\n",
        "        \n",
        "        #attention = [batch size, n heads, sent len, sent len]\n",
        "        \n",
        "        x = torch.matmul(attention, V)\n",
        "        \n",
        "        #x = [batch size, n heads, sent len, hid dim // n heads]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, sent len, n heads, hid dim // n heads]\n",
        "        \n",
        "        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))\n",
        "        \n",
        "        #x = [batch size, src sent len, hid dim]\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        \n",
        "        #x = [batch size, sent len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qFL27sojVQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedforward(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.pf_dim = pf_dim\n",
        "        \n",
        "        self.fc_1 = nn.Conv1d(hid_dim, pf_dim, 1)\n",
        "        self.fc_2 = nn.Conv1d(pf_dim, hid_dim, 1)\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, sent len, hid dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1)\n",
        "        \n",
        "        #x = [batch size, hid dim, sent len]\n",
        "        \n",
        "        x = self.do(F.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, ff dim, sent len]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, hid dim, sent len]\n",
        "        \n",
        "        x = x.permute(0, 2, 1)\n",
        "        \n",
        "        #x = [batch size, sent len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQusX9zqjXd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, pf_dim, decoder_layer, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.pf_dim = pf_dim\n",
        "        self.decoder_layer = decoder_layer\n",
        "        self.self_attention = self_attention\n",
        "        self.positionwise_feedforward = positionwise_feedforward\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(1000, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([decoder_layer(hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch_size, trg sent len]\n",
        "        #src = [batch_size, src sent len]\n",
        "        #trg_mask = [batch size, trg sent len]\n",
        "        #src_mask = [batch size, src sent len]\n",
        "        \n",
        "        pos = torch.arange(0, trg.shape[1]).unsqueeze(0).repeat(trg.shape[0], 1).to(self.device)\n",
        "                \n",
        "        trg = self.do((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #trg = [batch size, trg sent len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, src, trg_mask, src_mask)\n",
        "            \n",
        "        return self.fc(trg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOLg9ZKNjZWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.ln = nn.LayerNorm(hid_dim)\n",
        "        self.sa = self_attention(hid_dim, n_heads, dropout, device)\n",
        "        self.ea = self_attention(hid_dim, n_heads, dropout, device)\n",
        "        self.pf = positionwise_feedforward(hid_dim, pf_dim, dropout)\n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg sent len, hid dim]\n",
        "        #src = [batch size, src sent len, hid dim]\n",
        "        #trg_mask = [batch size, trg sent len]\n",
        "        #src_mask = [batch size, src sent len]\n",
        "                \n",
        "        trg = self.ln(trg + self.do(self.sa(trg, trg, trg, trg_mask)))\n",
        "                \n",
        "        trg = self.ln(trg + self.do(self.ea(trg, src, src, src_mask)))\n",
        "        \n",
        "        trg = self.ln(trg + self.do(self.pf(trg)))\n",
        "        \n",
        "        return trg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8a6s9Mfja9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, pad_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.pad_idx = pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_masks(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src sent len]\n",
        "        #trg = [batch size, trg sent len]\n",
        "        \n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        trg_pad_mask = (trg != self.pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), dtype=torch.uint8, device=self.device))\n",
        "        \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        return src_mask, trg_mask\n",
        "    \n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src sent len]\n",
        "        #trg = [batch size, trg sent len]\n",
        "                \n",
        "        src_mask, trg_mask = self.make_masks(src, trg)\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src sent len, hid dim]\n",
        "                \n",
        "        out = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #out = [batch size, trg sent len, output dim]\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOo0QoJclPfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CUDA_LAUNCH_BLOCKING=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQXVtV16jcPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = len(SRC.vocab)\n",
        "hid_dim = 50\n",
        "n_layers = 0\n",
        "n_heads = 4\n",
        "pf_dim = 2048\n",
        "dropout = 0.1\n",
        "device = \"cpu\"\n",
        "enc = Encoder(input_dim, hid_dim, n_layers, n_heads, pf_dim, EncoderLayer, \n",
        "              SelfAttention, PositionwiseFeedforward, dropout, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MteTVpcOmFU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_dim = len(TRG.vocab)\n",
        "hid_dim = 50\n",
        "n_layers = 0\n",
        "n_heads = 4\n",
        "pf_dim = 2048\n",
        "dropout = 0.1\n",
        "\n",
        "dec = Decoder(output_dim, hid_dim, n_layers, n_heads, pf_dim, \n",
        "              DecoderLayer, SelfAttention, PositionwiseFeedforward, dropout, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOSLVszHmqA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_idx = SRC.vocab.stoi['<pad>']\n",
        "\n",
        "model = Seq2Seq(enc, dec, pad_idx, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gpVSDPRmaBD",
        "colab_type": "code",
        "outputId": "18828f77-a2a6-4bc7-9eef-aa82d9248a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 1,914,437 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bkCjldsmdxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD2yWTFSmkqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = NoamOpt(hid_dim, 1, 2000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1QABXNOml5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5ip6fttmm2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "      if i < iterator.batch_size:\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg sent len - 1, output dim]\n",
        "        #trg = [batch size, trg sent len]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output.shape[-1])\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg sent len - 1, output dim]\n",
        "        #trg = [batch size * trg sent len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        print(\"Iteration ...{} and loss ... {}\".format(i, loss.item()))\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9-qFa3qm9VO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "          if i < iterator.batch_size:\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg sent len - 1, output dim]\n",
        "            #trg = [batch size, trg sent len]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output.shape[-1])\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg sent len - 1, output dim]\n",
        "            #trg = [batch size * trg sent len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            print(\"Iteration ...{} and loss ... {}\".format(i, loss.item()))\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xClLWg_3nEFK",
        "colab_type": "code",
        "outputId": "337b618a-556e-4859-e367-5283e6db6dc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        }
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "SAVE_DIR = 'models'\n",
        "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'transformer-seq2seq.pt')\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "if not os.path.isdir(f'{SAVE_DIR}'):\n",
        "    os.makedirs(f'{SAVE_DIR}')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "    \n",
        "    print(f'| Epoch: {epoch+1:03} | Time: {epoch_mins}m {epoch_secs}s| Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f} |')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-9465d9ad0c95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-f00be610f606>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[1;32m    200\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0Plne5znJHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}