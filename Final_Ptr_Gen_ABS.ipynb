{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Ptr_Gen_ABS.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paruliansaragi/Abstractive-Text-Summarization/blob/master/Final_Ptr_Gen_ABS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1_jxVPKZoDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip './bbc-news-summary.zip'\n",
        "!unrar x 'BBC News Summary.rar'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVcMDBQa1NRQ",
        "colab_type": "code",
        "outputId": "32ed3b29-e1f4-406a-c381-6d3971bc8bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-08 06:18:03--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-05-08 06:18:03--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.51MB/s    in 3m 8s   \n",
            "\n",
            "2019-05-08 06:21:12 (4.37 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvU_ES_p1Azb",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Imports \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from typing import NamedTuple, List, Callable, Dict, Tuple, Optional, Collection\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext.datasets import TranslationDataset, Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "from random import shuffle\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import gzip\n",
        "\n",
        "import spacy\n",
        "\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlgOGrB31B-l",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Params.py { form-width: \"35px\" }\n",
        "from typing import Optional, Union, List\n",
        "\n",
        "\n",
        "class Params:\n",
        "  # Model architecture\n",
        "  vocab_size: int = 30000\n",
        "  hidden_size: int = 150  # of the encoder; default decoder size is doubled if encoder is bidi\n",
        "  dec_hidden_size: Optional[int] = 200  # if set, a matrix will transform enc state into dec state\n",
        "  embed_size: int = 100\n",
        "  enc_bidi: bool = True\n",
        "  enc_attn: bool = True  # decoder has attention over encoder states?\n",
        "  dec_attn: bool = False  # decoder has attention over previous decoder states?\n",
        "  pointer: bool = True  # use pointer network (copy mechanism) in addition to word generator?\n",
        "  out_embed_size: Optional[int] = None  # if set, use an additional layer before decoder output\n",
        "  tie_embed: bool = True  # tie the decoder output layer to the input embedding layer?\n",
        "\n",
        "  # Coverage (to turn on/off, change both `enc_attn_cover` and `cover_loss`)\n",
        "  enc_attn_cover: bool = True  # provide coverage as input when computing enc attn?\n",
        "  cover_func: str = 'max'  # how to aggregate previous attention distributions? sum or max\n",
        "  cover_loss: float = 1  # add coverage loss if > 0; weight of coverage loss as compared to NLLLoss\n",
        "  show_cover_loss: bool = False  # include coverage loss in the loss shown in the progress bar?\n",
        "\n",
        "  # Regularization\n",
        "  enc_rnn_dropout: float = 0\n",
        "  dec_in_dropout: float = 0\n",
        "  dec_rnn_dropout: float = 0\n",
        "  dec_out_dropout: float = 0\n",
        "\n",
        "  # Training\n",
        "  optimizer: str = 'adam'  # adam or adagrad\n",
        "  lr: float = 0.001  # learning rate\n",
        "  adagrad_accumulator: float = 0.1\n",
        "  lr_decay_step: int = 5  # decay lr every how many epochs?\n",
        "  lr_decay: Optional[float] = None  # decay lr by multiplying this factor\n",
        "  #batch_size: int = 32\n",
        "  batch_size: int = 8\n",
        "  #n_batches: int = 1000  # how many batches per epoch\n",
        "  n_batches: int = 250\n",
        "  #val_batch_size: int = 32\n",
        "  val_batch_size: int = 8\n",
        "  n_val_batches: int = 100  # how many validation batches per epoch\n",
        "  #n_epochs: int = 75\n",
        "  n_epochs: int = 5\n",
        "  pack_seq: bool = True  # use packed sequence to skip PAD inputs?\n",
        "  forcing_ratio: float = 0.75  # initial percentage of using teacher forcing\n",
        "  partial_forcing: bool = True  # in a seq, can some steps be teacher forced and some not?\n",
        "  forcing_decay_type: Optional[str] = 'exp'  # linear, exp, sigmoid, or None\n",
        "  forcing_decay: float = 0.9999\n",
        "  sample: bool = True  # are non-teacher forced inputs based on sampling or greedy selection?\n",
        "  grad_norm: float = 1  # use gradient clipping if > 0; max gradient norm\n",
        "  # note: enabling reinforcement learning can significantly slow down training\n",
        "  rl_ratio: float = 0  # use mixed objective if > 0; ratio of RL in the loss function\n",
        "  rl_ratio_power: float = 1  # increase rl_ratio by **= rl_ratio_power after each epoch; (0, 1]\n",
        "  rl_start_epoch: int = 1  # start RL at which epoch (later start can ensure a strong baseline)?\n",
        "\n",
        "  # Data\n",
        "  embed_file: Optional[str] = './glove.6B.50d.txt'  # use pre-trained embeddings\n",
        "  data_path: str = './sentences_aa.txt'\n",
        "  val_data_path: Optional[str] = './sentences_ab.txt'\n",
        "  max_src_len: int = 400  # exclusive of special tokens such as EOS\n",
        "  max_tgt_len: int = 100  # exclusive of special tokens such as EOS\n",
        "  truncate_src: bool = True  # truncate to max_src_len? if false, drop example if too long\n",
        "  truncate_tgt: bool = True  # truncate to max_tgt_len? if false, drop example if too long\n",
        "\n",
        "  # Saving model automatically during training\n",
        "  model_path_prefix: Optional[str] = './checkpoints/m05'\n",
        "  keep_every_epoch: bool = False  # save all epochs, or only the best and the latest one?\n",
        "\n",
        "  # Testing\n",
        "  beam_size: int = 4\n",
        "  min_out_len: int = 60\n",
        "  max_out_len: Optional[int] = 100\n",
        "  out_len_in_words: bool = False\n",
        "  #test_data_path: str = 'data/cnndm.test.gz'\n",
        "  test_sample_ratio: float = 1  # what portion of the test data is used? (1 for all data)\n",
        "  test_save_results: bool = False\n",
        "\n",
        "  def update(self, cmd_args: List[str]):\n",
        "    \"\"\"Update configuration by a list of command line arguments\"\"\"\n",
        "    arg_name = None\n",
        "    for arg_text in cmd_args:\n",
        "      if arg_name is None:\n",
        "        assert arg_text.startswith('--')  # the arg name has to start with \"--\"\n",
        "        arg_name = arg_text[2:]\n",
        "      else:\n",
        "        arg_curr_value = getattr(self, arg_name)\n",
        "        if arg_text.lower() == 'none':\n",
        "          arg_new_value = None\n",
        "        elif arg_text.lower() == 'true':\n",
        "          arg_new_value = True\n",
        "        elif arg_text.lower() == 'false':\n",
        "          arg_new_value = False\n",
        "        else:\n",
        "          arg_type = self.__annotations__[arg_name]\n",
        "          if type(arg_type) is not type:  # support only Optional[T], where T is a basic type\n",
        "            assert arg_type.__origin__ is Union\n",
        "            arg_types = [t for t in arg_type.__args__ if t is not type(None)]\n",
        "            assert len(arg_types) == 1\n",
        "            arg_type = arg_types[0]\n",
        "            assert type(arg_type) is type\n",
        "          arg_new_value = arg_type(arg_text)\n",
        "        setattr(self, arg_name, arg_new_value)\n",
        "        print(\"Hyper-parameter %s = %s (was %s)\" % (arg_name, arg_new_value, arg_curr_value))\n",
        "        arg_name = None\n",
        "    if arg_name is not None:\n",
        "      print(\"Warning: Argument %s lacks a value and is ignored.\" % arg_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foBCqlUK1DiI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Vocab Class { form-width: \"40px\" }\n",
        "class Vocab(object):\n",
        "\n",
        "  PAD = 0\n",
        "  SOS = 1\n",
        "  EOS = 2\n",
        "  UNK = 3\n",
        "\n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.word2count = Counter()\n",
        "    self.reserved = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "    self.index2word = self.reserved[:]\n",
        "    self.embeddings = None\n",
        "\n",
        "  def add_words(self, words: List[str]):\n",
        "    for word in words:\n",
        "      if word not in self.word2index:\n",
        "        self.word2index[word] = len(self.index2word)\n",
        "        self.index2word.append(word)\n",
        "    self.word2count.update(words)\n",
        "\n",
        "  def trim(self, *, vocab_size: int=None, min_freq: int=1):\n",
        "    if min_freq <= 1 and (vocab_size is None or vocab_size >= len(self.word2index)):\n",
        "      return\n",
        "    ordered_words = sorted(((c, w) for (w, c) in self.word2count.items()), reverse=True)\n",
        "    if vocab_size:\n",
        "      ordered_words = ordered_words[:vocab_size]\n",
        "    self.word2index = {}\n",
        "    self.word2count = Counter()\n",
        "    self.index2word = self.reserved[:]\n",
        "    for count, word in ordered_words:\n",
        "      if count < min_freq: break\n",
        "      self.word2index[word] = len(self.index2word)\n",
        "      self.word2count[word] = count\n",
        "      self.index2word.append(word)\n",
        "\n",
        "  def load_embeddings(self, file_path: str, dtype=np.float32) -> int:\n",
        "    num_embeddings = 0\n",
        "    vocab_size = len(self)\n",
        "    with open(file_path, 'rb') as f:\n",
        "      for line in f:\n",
        "        line = line.split()\n",
        "        word = line[0].decode('utf-8')\n",
        "        idx = self.word2index.get(word)\n",
        "        if idx is not None:\n",
        "          vec = np.array(line[1:], dtype=dtype)\n",
        "          if self.embeddings is None:\n",
        "            n_dims = len(vec)\n",
        "            self.embeddings = np.random.normal(np.zeros((vocab_size, n_dims))).astype(dtype)\n",
        "            self.embeddings[self.PAD] = np.zeros(n_dims)\n",
        "          self.embeddings[idx] = vec\n",
        "          num_embeddings += 1\n",
        "    return num_embeddings\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    if type(item) is int:\n",
        "      return self.index2word[item]\n",
        "    return self.word2index.get(item, self.UNK)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.index2word)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZCDOoFq1GcS",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "1ac9d80b-80c8-40b0-9e29-90a084f14094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#@title nltk_tokenizer(doc) { form-width: \"40px\" }\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def nltk_tokenizer(doc):\n",
        "  return nltk.word_tokenize(doc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8V5MAFn1ILi",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Create Vocab \n",
        "\n",
        "Voc = Vocab()\n",
        "\n",
        "trn = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "val = pd.read_csv('valid.csv')\n",
        "\n",
        "for i in trn.values:\n",
        "  Voc.add_words(nltk_tokenizer(i[0])), Voc.add_words(nltk_tokenizer(i[1]))\n",
        "\n",
        "for i in test.values:\n",
        "  Voc.add_words(nltk_tokenizer(i[0])), Voc.add_words(nltk_tokenizer(i[1]))\n",
        "\n",
        "for i in val.values:\n",
        "  Voc.add_words(nltk_tokenizer(i[0])), Voc.add_words(nltk_tokenizer(i[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ews848t1JTb",
        "colab_type": "code",
        "outputId": "f87ad571-2296-4f69-cfc0-9e9f1b6f4ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Voc.load_embeddings('./glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20688"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfvkmrXk1Qkf",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Example Class { form-width: \"40px\" }\n",
        "class Example(NamedTuple):\n",
        "  src: List[str]\n",
        "  tgt: List[str]\n",
        "  src_len: int  # inclusive of EOS, so that it corresponds to tensor shape\n",
        "  tgt_len: int  # inclusive of EOS, so that it corresponds to tensor shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z08W9Tyy1UB-",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title OOV Dictionary Class { form-width: \"40px\" }\n",
        "\n",
        "class OOVDict(object):\n",
        "\n",
        "  def __init__(self, base_oov_idx):\n",
        "    self.word2index = {}  # type: Dict[Tuple[int, str], int]\n",
        "    self.index2word = {}  # type: Dict[Tuple[int, int], str]\n",
        "    self.next_index = {}  # type: Dict[int, int]\n",
        "    self.base_oov_idx = base_oov_idx\n",
        "    self.ext_vocab_size = base_oov_idx\n",
        "\n",
        "  def add_word(self, idx_in_batch, word) -> int:\n",
        "    key = (idx_in_batch, word)\n",
        "    index = self.word2index.get(key)\n",
        "    if index is not None: return index\n",
        "    index = self.next_index.get(idx_in_batch, self.base_oov_idx)\n",
        "    self.next_index[idx_in_batch] = index + 1\n",
        "    self.word2index[key] = index\n",
        "    self.index2word[(idx_in_batch, index)] = word\n",
        "    self.ext_vocab_size = max(self.ext_vocab_size, index + 1)\n",
        "    return index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-BGRyP91Zbe",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Batch { form-width: \"40px\" }\n",
        "\n",
        "class Batch(NamedTuple):\n",
        "  examples: List[Example]\n",
        "  input_tensor: Optional[torch.Tensor]\n",
        "  target_tensor: Optional[torch.Tensor]\n",
        "  input_lengths: Optional[List[int]]\n",
        "  oov_dict: Optional[OOVDict]\n",
        "\n",
        "  @property\n",
        "  def ext_vocab_size(self):\n",
        "    if self.oov_dict is not None:\n",
        "      return self.oov_dict.ext_vocab_size\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76vCeE3I1fIX",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Dataset Class { form-width: \"40px\" }\n",
        "\n",
        "def simple_tokenizer(text: str, lower: bool=False, newline: str=None) -> List[str]:\n",
        "  \"\"\"Split an already tokenized input `text`.\"\"\"\n",
        "  if lower:\n",
        "    text = text.lower()\n",
        "  if newline is not None:  # replace newline by a token\n",
        "    text = text.replace('\\n', ' ' + newline + ' ')\n",
        "  return text.split()\n",
        "\n",
        "class Dataset(object):\n",
        "\n",
        "  def __init__(self, filename: Optional[str], dataframe: Optional[pd.core.frame.DataFrame], tokenize: Callable=simple_tokenizer, max_src_len: int=None,\n",
        "               max_tgt_len: int=None, truncate_src: bool=False, truncate_tgt: bool=False):\n",
        "    \n",
        "    if isinstance(dataframe, pd.DataFrame):\n",
        "      print(\"Reading dataframe ...\")\n",
        "    else:\n",
        "      print(\"Reading dataset %s...\" % filename, end=' ', flush=True)\n",
        "    self.filename = filename\n",
        "    self.pairs = []\n",
        "    self.src_len = 0\n",
        "    self.tgt_len = 0\n",
        "      #requires a file that splits src and tgt by a tab \\t \n",
        "    if filename:\n",
        "      with open(filename, 'rt', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "          pair = line.strip().split('\\t') # pair = (src, tgt)\n",
        "          if len(pair) != 2:\n",
        "            print(\"Line %d of %s is malformed.\" % (i, filename))\n",
        "            continue\n",
        "          src = tokenize(pair[0])\n",
        "          if max_src_len and len(src) > max_src_len:\n",
        "            if truncate_src:\n",
        "              src = src[:max_src_len]\n",
        "            else:\n",
        "              continue\n",
        "          tgt = tokenize(pair[1])\n",
        "          if max_tgt_len and len(tgt) > max_tgt_len:\n",
        "            if truncate_tgt:\n",
        "              tgt = tgt[:max_tgt_len]\n",
        "            else:\n",
        "              continue\n",
        "          src_len = len(src) + 1  # EOS\n",
        "          tgt_len = len(tgt) + 1  # EOS\n",
        "          self.src_len = max(self.src_len, src_len)\n",
        "          self.tgt_len = max(self.tgt_len, tgt_len)\n",
        "          self.pairs.append(Example(src, tgt, src_len, tgt_len))\n",
        "      print(\"%d pairs.\" % len(self.pairs))\n",
        "    else:#from list of strings in separate src, tgt instead of file\n",
        "      for i in dataframe.values:\n",
        "        src = nltk_tokenizer(i[0])\n",
        "        if max_src_len and len(src) > max_src_len:\n",
        "            if truncate_src:\n",
        "              src = src[:max_src_len]\n",
        "            else:\n",
        "              continue\n",
        "              \n",
        "        tgt = nltk_tokenizer(i[1])\n",
        "        if max_tgt_len and len(tgt) > max_tgt_len:\n",
        "            if truncate_tgt:\n",
        "              tgt = tgt[:max_tgt_len]\n",
        "            else:\n",
        "              continue\n",
        "        src_len = len(src) + 1\n",
        "        tgt_len = len(tgt) + 1\n",
        "        self.src_len = max(self.src_len, src_len)\n",
        "        self.tgt_len = max(self.tgt_len, tgt_len)\n",
        "        self.pairs.append(Example(src, tgt, src_len, tgt_len))\n",
        "      print(\"%d pairs.\" % len(self.pairs))\n",
        "\n",
        "  def build_vocab(self, ttv: str, vocab_size: int=None, src: bool=True, tgt: bool=True,\n",
        "                  embed_file: str=None) -> Vocab:\n",
        "    if self.filename:  \n",
        "      filename, _ = os.path.splitext(self.filename)\n",
        "      if vocab_size:\n",
        "        filename += \".%d\" % vocab_size\n",
        "      filename += '.vocab'\n",
        "      if os.path.isfile(filename):\n",
        "        vocab = torch.load(filename)\n",
        "        print(\"Vocabulary loaded, %d words.\" % len(vocab))\n",
        "    else:\n",
        "      print(\"Building vocabulary...\", end=' ', flush=True)\n",
        "      vocab = Vocab()\n",
        "      filename = \"{}\".format(ttv)\n",
        "      filename += \".%d\" % vocab_size\n",
        "      for example in self.pairs:\n",
        "        if src:\n",
        "          vocab.add_words(example.src)\n",
        "        if tgt:\n",
        "          vocab.add_words(example.tgt)\n",
        "      vocab.trim(vocab_size=vocab_size)\n",
        "      print(\"%d words.\" % len(vocab))\n",
        "      torch.save(vocab, filename)\n",
        "    if embed_file:\n",
        "      count = vocab.load_embeddings(embed_file)\n",
        "      print(\"%d pre-trained embeddings loaded.\" % count)\n",
        "    return vocab\n",
        "\n",
        "  def generator(self, batch_size: int, src_vocab: Vocab=None, tgt_vocab: Vocab=None,\n",
        "                ext_vocab: bool=False):\n",
        "    ptr = len(self.pairs)  # make sure to shuffle at first run\n",
        "    if ext_vocab:\n",
        "      assert src_vocab is not None\n",
        "      base_oov_idx = len(src_vocab)\n",
        "    while True:\n",
        "      if ptr + batch_size > len(self.pairs):\n",
        "        shuffle(self.pairs)  # shuffle inplace to save memory\n",
        "        ptr = 0\n",
        "      examples = self.pairs[ptr:ptr + batch_size]\n",
        "      ptr += batch_size\n",
        "      src_tensor, tgt_tensor = None, None\n",
        "      lengths, oov_dict = None, None\n",
        "      if src_vocab or tgt_vocab:\n",
        "        # initialize tensors\n",
        "        if src_vocab:\n",
        "          examples.sort(key=lambda x: -x.src_len)\n",
        "          lengths = [x.src_len for x in examples]\n",
        "          max_src_len = lengths[0]\n",
        "          src_tensor = torch.zeros(max_src_len, batch_size, dtype=torch.long)\n",
        "          if ext_vocab:\n",
        "            oov_dict = OOVDict(base_oov_idx)\n",
        "        if tgt_vocab:\n",
        "          max_tgt_len = max(x.tgt_len for x in examples)\n",
        "          tgt_tensor = torch.zeros(max_tgt_len, batch_size, dtype=torch.long)\n",
        "        # fill up tensors by word indices\n",
        "        for i, example in enumerate(examples):\n",
        "          if src_vocab:\n",
        "            for j, word in enumerate(example.src):\n",
        "              idx = src_vocab[word]\n",
        "              if ext_vocab and idx == src_vocab.UNK:\n",
        "                idx = oov_dict.add_word(i, word)\n",
        "              src_tensor[j, i] = idx\n",
        "            src_tensor[example.src_len - 1, i] = src_vocab.EOS\n",
        "          if tgt_vocab:\n",
        "            for j, word in enumerate(example.tgt):\n",
        "              idx = tgt_vocab[word]\n",
        "              if ext_vocab and idx == src_vocab.UNK:\n",
        "                idx = oov_dict.word2index.get((i, word), idx)\n",
        "              tgt_tensor[j, i] = idx\n",
        "            tgt_tensor[example.tgt_len - 1, i] = tgt_vocab.EOS\n",
        "      yield Batch(examples, src_tensor, tgt_tensor, lengths, oov_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Son14LN1h98",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Hypothesis Class \n",
        "\n",
        "class Hypothesis(object):\n",
        "\n",
        "  def __init__(self, tokens, log_probs, dec_hidden, dec_states, enc_attn_weights, num_non_words):\n",
        "    self.tokens = tokens  # type: List[int]\n",
        "    self.log_probs = log_probs  # type: List[float]\n",
        "    self.dec_hidden = dec_hidden  # shape: (1, 1, hidden_size)\n",
        "    self.dec_states = dec_states  # list of dec_hidden\n",
        "    self.enc_attn_weights = enc_attn_weights  # list of shape: (1, 1, src_len)\n",
        "    self.num_non_words = num_non_words  # type: int\n",
        "\n",
        "  def __repr__(self):\n",
        "    return repr(self.tokens)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tokens) - self.num_non_words\n",
        "\n",
        "  @property\n",
        "  def avg_log_prob(self):\n",
        "    return sum(self.log_probs) / len(self.log_probs)\n",
        "\n",
        "  def create_next(self, token, log_prob, dec_hidden, add_dec_states, enc_attn, non_word):\n",
        "    return Hypothesis(tokens=self.tokens + [token], log_probs=self.log_probs + [log_prob],\n",
        "                      dec_hidden=dec_hidden, dec_states=\n",
        "                      self.dec_states + [dec_hidden] if add_dec_states else self.dec_states,\n",
        "                      enc_attn_weights=self.enc_attn_weights + [enc_attn]\n",
        "                      if enc_attn is not None else self.enc_attn_weights,\n",
        "                      num_non_words=self.num_non_words + 1 if non_word else self.num_non_words)\n",
        "\n",
        "\n",
        "def show_plot(loss, step=1, val_loss=None, val_metric=None, val_step=1, file_prefix=None):\n",
        "  plt.figure()\n",
        "  fig, ax = plt.subplots(figsize=(12, 8))\n",
        "  # this locator puts ticks at regular intervals\n",
        "  loc = ticker.MultipleLocator(base=0.2)\n",
        "  ax.yaxis.set_major_locator(loc)\n",
        "  ax.set_ylabel('Loss', color='b')\n",
        "  ax.set_xlabel('Batch')\n",
        "  plt.plot(range(step, len(loss) * step + 1, step), loss, 'b')\n",
        "  if val_loss:\n",
        "    plt.plot(range(val_step, len(val_loss) * val_step + 1, val_step), val_loss, 'g')\n",
        "  if val_metric:\n",
        "    ax2 = ax.twinx()\n",
        "    ax2.plot(range(val_step, len(val_metric) * val_step + 1, val_step), val_metric, 'r')\n",
        "    ax2.set_ylabel('ROUGE', color='r')\n",
        "  if file_prefix:\n",
        "    plt.savefig(file_prefix + '.png')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def show_attention_map(src_words, pred_words, attention, pointer_ratio=None):\n",
        "  fig, ax = plt.subplots(figsize=(16, 4))\n",
        "  im = plt.pcolormesh(np.flipud(attention), cmap=\"GnBu\")\n",
        "  # set ticks and labels\n",
        "  ax.set_xticks(np.arange(len(src_words)) + 0.5)\n",
        "  ax.set_xticklabels(src_words, fontsize=14)\n",
        "  ax.set_yticks(np.arange(len(pred_words)) + 0.5)\n",
        "  ax.set_yticklabels(reversed(pred_words), fontsize=14)\n",
        "  if pointer_ratio is not None:\n",
        "    ax1 = ax.twinx()\n",
        "    ax1.set_yticks(np.concatenate([np.arange(0.5, len(pred_words)), [len(pred_words)]]))\n",
        "    ax1.set_yticklabels('%.3f' % v for v in np.flipud(pointer_ratio))\n",
        "    ax1.set_ylabel('Copy probability', rotation=-90, va=\"bottom\")\n",
        "  # let the horizontal axes labelling appear on top\n",
        "  ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
        "  # rotate the tick labels and set their alignment\n",
        "  plt.setp(ax.get_xticklabels(), rotation=-45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "\n",
        "non_word_char_in_word = re.compile(r\"(?<=\\w)\\W(?=\\w)\")\n",
        "not_for_output = {'<PAD>', '<SOS>', '<EOS>', '<UNK>'}\n",
        "\n",
        "def format_tokens(tokens: List[str], newline: str= '<P>', for_rouge: bool=False) -> str:\n",
        "  \"\"\"Join output `tokens` for ROUGE evaluation.\"\"\"\n",
        "  tokens = filter(lambda t: t not in not_for_output, tokens)\n",
        "  if for_rouge:\n",
        "    tokens = [non_word_char_in_word.sub(\"\", t) for t in tokens]  # \"n't\" => \"nt\"\n",
        "  if newline is None:\n",
        "    s = ' '.join(tokens)\n",
        "  else:  # replace newline tokens by newlines\n",
        "    lines, line = [], []\n",
        "    for tok in tokens:\n",
        "      if tok == newline:\n",
        "        if line: lines.append(\" \".join(line))\n",
        "        line = []\n",
        "      else:\n",
        "        line.append(tok)\n",
        "    if line: lines.append(\" \".join(line))\n",
        "    s = '\\n'.join(lines)\n",
        "  return s\n",
        "\n",
        "def format_rouge_scores(rouge_result: Dict[str, float]) -> str:\n",
        "  lines = []\n",
        "  line, prev_metric = [], None\n",
        "  for key in sorted(rouge_result.keys()):\n",
        "    metric = key.rsplit(\"_\", maxsplit=1)[0]\n",
        "    if metric != prev_metric and prev_metric is not None:\n",
        "      lines.append(\"\\t\".join(line))\n",
        "      line = []\n",
        "    line.append(\"%s %s\" % (key, rouge_result[key]))\n",
        "    prev_metric = metric\n",
        "  lines.append(\"\\t\".join(line))\n",
        "  return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "rouge_pattern = re.compile(rb\"(\\d+) ROUGE-(.+) Average_([RPF]): ([\\d.]+) \"\n",
        "                           rb\"\\(95%-conf\\.int\\. ([\\d.]+) - ([\\d.]+)\\)\")\n",
        "\n",
        "def rouge(target: List[List[str]], *predictions: List[List[str]]) -> List[Dict[str, float]]:\n",
        "  \"\"\"Perform single-reference ROUGE evaluation of one or more systems' predictions.\"\"\"\n",
        "  results = [dict() for _ in range(len(predictions))]  # e.g. 0 => 'su4_f' => 0.35\n",
        "  print('Why are we skipping this??')\n",
        "  with TemporaryDirectory() as folder:  # on my server, /tmp is a RAM disk\n",
        "    # write SPL files\n",
        "    eval_entries = []\n",
        "    for i, tgt_tokens in enumerate(target):\n",
        "      sys_entries = []\n",
        "      for j, pred_docs in enumerate(predictions):\n",
        "        sys_file = 'sys%d_%d.spl' % (j, i)\n",
        "        sys_entries.append('\\n    <P ID=\"%d\">%s</P>' % (j, sys_file))\n",
        "        with open(os.path.join(folder, sys_file), 'wt') as f:\n",
        "          f.write(format_tokens(pred_docs[i], for_rouge=True))\n",
        "      ref_file = 'ref_%d.spl' % i\n",
        "      with open(os.path.join(folder, ref_file), 'wt') as f:\n",
        "        f.write(format_tokens(tgt_tokens, for_rouge=True))\n",
        "      eval_entry = \"\"\"\n",
        "<EVAL ID=\"{1}\">\n",
        "  <PEER-ROOT>{0}</PEER-ROOT>\n",
        "  <MODEL-ROOT>{0}</MODEL-ROOT>\n",
        "  <INPUT-FORMAT TYPE=\"SPL\"></INPUT-FORMAT>\n",
        "  <PEERS>{2}\n",
        "  </PEERS>\n",
        "  <MODELS>\n",
        "    <M ID=\"A\">{3}</M>\n",
        "  </MODELS>\n",
        "</EVAL>\"\"\".format(folder, i, ''.join(sys_entries), ref_file)\n",
        "      eval_entries.append(eval_entry)\n",
        "    # write config file\n",
        "    xml = '<ROUGE-EVAL version=\"1.0\">{0}\\n</ROUGE-EVAL>'.format(\"\".join(eval_entries))\n",
        "    config_path = os.path.join(folder, 'task.xml')\n",
        "    #ROUGE-eval-config-file: Specify the evaluation setup. Three files come with the ROUGE \n",
        "            #evaluation package, i.e. ROUGE-test.xml, verify.xml, and verify-spl.xml are \n",
        "            #good examples.\n",
        "    with open(config_path, 'wt') as f:\n",
        "      f.write(xml)\n",
        "      print('Written config for rouge...{}'.format(config_path))\n",
        "    # run ROUGE\n",
        "    out = subprocess.check_output('./ROUGE-1.5.5.pl -e data -a -n 2 -2 4 -u ' + config_path,\n",
        "                                  shell=True, cwd=os.path.join(this_dir, 'data'))\n",
        "  # parse ROUGE output\n",
        "  for line in out.split(b'\\n'):\n",
        "    match = rouge_pattern.match(line)\n",
        "    if match:\n",
        "      sys_id, metric, rpf, value, low, high = match.groups()\n",
        "      results[int(sys_id)][(metric + b'_' + rpf).decode('utf-8').lower()] = float(value)\n",
        "  return results\n",
        "\n",
        "\n",
        "def rouge_single(example: List[List[str]]) -> List[Dict[str, float]]:\n",
        "  \"\"\"Helper for `rouge_parallel()`.\"\"\"\n",
        "  return rouge(*example)\n",
        "\n",
        "\n",
        "def rouge_parallel(target: List[List[str]], *predictions: List[List[str]]) \\\n",
        "        -> List[List[Dict[str, float]]]:\n",
        "  \"\"\"\n",
        "  Run ROUGE tests in parallel (by Python multi-threading, i.e. multiprocessing.dummy) to obtain\n",
        "  per-document scores. Depending on batch size and hardware, this may be slower or faster than\n",
        "  `rouge()`.\n",
        "  \"\"\"\n",
        "  with Pool() as p:\n",
        "    return p.map(rouge_single, zip(target, *predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UjBWk071zBx",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Model.py { form-width: \"15px\" }\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import random\n",
        "#from params import Params\n",
        "#from utils import Vocab, Hypothesis, word_detector\n",
        "from typing import Union, List\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "eps = 1e-31\n",
        "\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_size, hidden_size, bidi=True, *, rnn_drop: float=0):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_directions = 2 if bidi else 1\n",
        "    self.gru = nn.GRU(embed_size, hidden_size, bidirectional=bidi, dropout=rnn_drop)\n",
        "\n",
        "  def forward(self, embedded, hidden, input_lengths=None):\n",
        "    \"\"\"\n",
        "    :param embedded: (src seq len, batch size, embed size)\n",
        "    :param hidden: (num directions, batch size, encoder hidden size)\n",
        "    :param input_lengths: list containing the non-padded length of each sequence in this batch;\n",
        "                          if set, we use `PackedSequence` to skip the PAD inputs and leave the\n",
        "                          corresponding encoder states as zeros\n",
        "    :return: (src seq len, batch size, hidden size * num directions = decoder hidden size)\n",
        "    Perform multi-step encoding.\n",
        "    \"\"\"\n",
        "    if input_lengths is not None:\n",
        "      embedded = pack_padded_sequence(embedded, input_lengths)\n",
        "\n",
        "    output, hidden = self.gru(embedded, hidden)\n",
        "\n",
        "    if input_lengths is not None:\n",
        "      output, _ = pad_packed_sequence(output)\n",
        "\n",
        "    if self.num_directions > 1:\n",
        "      # hidden: (num directions, batch, hidden) => (1, batch, hidden * 2)\n",
        "      batch_size = hidden.size(1)\n",
        "      hidden = hidden.transpose(0, 1).contiguous().view(1, batch_size,\n",
        "                                                        self.hidden_size * self.num_directions)\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    return torch.zeros(self.num_directions, batch_size, self.hidden_size, device=DEVICE)\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embed_size, hidden_size, *, enc_attn=True, dec_attn=True,\n",
        "               enc_attn_cover=True, pointer=True, tied_embedding=None, out_embed_size=None,\n",
        "               in_drop: float=0, rnn_drop: float=0, out_drop: float=0, enc_hidden_size=None):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.combined_size = self.hidden_size\n",
        "    self.enc_attn = enc_attn\n",
        "    self.dec_attn = dec_attn\n",
        "    self.enc_attn_cover = enc_attn_cover\n",
        "    self.pointer = pointer\n",
        "    self.out_embed_size = out_embed_size\n",
        "    if tied_embedding is not None and self.out_embed_size and embed_size != self.out_embed_size:\n",
        "      print(\"Warning: Output embedding size %d is overriden by its tied embedding size %d.\"\n",
        "            % (self.out_embed_size, embed_size))\n",
        "      self.out_embed_size = embed_size\n",
        "\n",
        "    self.in_drop = nn.Dropout(in_drop) if in_drop > 0 else None\n",
        "    self.gru = nn.GRU(embed_size, self.hidden_size, dropout=rnn_drop)\n",
        "\n",
        "    if enc_attn:\n",
        "      if not enc_hidden_size: enc_hidden_size = self.hidden_size\n",
        "      self.enc_bilinear = nn.Bilinear(self.hidden_size, enc_hidden_size, 1)\n",
        "      self.combined_size += enc_hidden_size\n",
        "      if enc_attn_cover:\n",
        "        self.cover_weight = nn.Parameter(torch.rand(1))\n",
        "\n",
        "    if dec_attn:\n",
        "      self.dec_bilinear = nn.Bilinear(self.hidden_size, self.hidden_size, 1)\n",
        "      self.combined_size += self.hidden_size\n",
        "\n",
        "    self.out_drop = nn.Dropout(out_drop) if out_drop > 0 else None\n",
        "    if pointer:\n",
        "      self.ptr = nn.Linear(self.combined_size, 1)\n",
        "\n",
        "    if tied_embedding is not None and embed_size != self.combined_size:\n",
        "      # use pre_out layer if combined size is different from embedding size\n",
        "      self.out_embed_size = embed_size\n",
        "\n",
        "    if self.out_embed_size:  # use pre_out layer\n",
        "      self.pre_out = nn.Linear(self.combined_size, self.out_embed_size)\n",
        "      size_before_output = self.out_embed_size\n",
        "    else:  # don't use pre_out layer\n",
        "      size_before_output = self.combined_size\n",
        "\n",
        "    self.out = nn.Linear(size_before_output, vocab_size)\n",
        "    if tied_embedding is not None:\n",
        "      self.out.weight = tied_embedding.weight\n",
        "\n",
        "  def forward(self, embedded, hidden, encoder_states=None, decoder_states=None, coverage_vector=None, *,\n",
        "              encoder_word_idx=None, ext_vocab_size: int=None, log_prob: bool=True):\n",
        "    \"\"\"\n",
        "    :param embedded: (batch size, embed size)\n",
        "    :param hidden: (1, batch size, decoder hidden size)\n",
        "    :param encoder_states: (src seq len, batch size, hidden size), for attention mechanism\n",
        "    :param decoder_states: (past dec steps, batch size, hidden size), for attention mechanism\n",
        "    :param encoder_word_idx: (src seq len, batch size), for pointer network\n",
        "    :param ext_vocab_size: the dynamic vocab size, determined by the max num of OOV words contained\n",
        "                           in any src seq in this batch, for pointer network\n",
        "    :param log_prob: return log probability instead of probability\n",
        "    :return: tuple of four things:\n",
        "             1. word prob or log word prob, (batch size, dynamic vocab size);\n",
        "             2. RNN hidden state after this step, (1, batch size, decoder hidden size);\n",
        "             3. attention weights over encoder states, (batch size, src seq len);\n",
        "             4. prob of copying by pointing as opposed to generating, (batch size, 1)\n",
        "    Perform single-step decoding.\n",
        "    \"\"\"\n",
        "    batch_size = embedded.size(0)\n",
        "    combined = torch.zeros(batch_size, self.combined_size, device=DEVICE)\n",
        "\n",
        "    if self.in_drop: embedded = self.in_drop(embedded)\n",
        "\n",
        "    output, hidden = self.gru(embedded.unsqueeze(0), hidden)  # unsqueeze and squeeze are necessary\n",
        "    combined[:, :self.hidden_size] = output.squeeze(0)        # as RNN expects a 3D tensor (step=1)\n",
        "    offset = self.hidden_size\n",
        "    enc_attn, prob_ptr = None, None  # for visualization\n",
        "\n",
        "    if self.enc_attn or self.pointer:\n",
        "      # energy and attention: (num encoder states, batch size, 1)\n",
        "      num_enc_steps = encoder_states.size(0)\n",
        "      enc_total_size = encoder_states.size(2)\n",
        "      enc_energy = self.enc_bilinear(hidden.expand(num_enc_steps, batch_size, -1).contiguous(),\n",
        "                                     encoder_states)\n",
        "      if self.enc_attn_cover and coverage_vector is not None:\n",
        "        enc_energy += self.cover_weight * torch.log(coverage_vector.transpose(0, 1).unsqueeze(2) + eps)\n",
        "      # transpose => (batch size, num encoder states, 1)\n",
        "      enc_attn = F.softmax(enc_energy, dim=0).transpose(0, 1)\n",
        "      if self.enc_attn:\n",
        "        # context: (batch size, encoder hidden size, 1)\n",
        "        enc_context = torch.bmm(encoder_states.permute(1, 2, 0), enc_attn)\n",
        "        combined[:, offset:offset+enc_total_size] = enc_context.squeeze(2)\n",
        "        offset += enc_total_size\n",
        "      enc_attn = enc_attn.squeeze(2)\n",
        "\n",
        "    if self.dec_attn:\n",
        "      if decoder_states is not None and len(decoder_states) > 0:\n",
        "        dec_energy = self.dec_bilinear(hidden.expand_as(decoder_states).contiguous(),\n",
        "                                       decoder_states)\n",
        "        dec_attn = F.softmax(dec_energy, dim=0).transpose(0, 1)\n",
        "        dec_context = torch.bmm(decoder_states.permute(1, 2, 0), dec_attn)\n",
        "        combined[:, offset:offset + self.hidden_size] = dec_context.squeeze(2)\n",
        "      offset += self.hidden_size\n",
        "\n",
        "    if self.out_drop: combined = self.out_drop(combined)\n",
        "\n",
        "    # generator\n",
        "    if self.out_embed_size:\n",
        "      out_embed = self.pre_out(combined)\n",
        "    else:\n",
        "      out_embed = combined\n",
        "    logits = self.out(out_embed)  # (batch size, vocab size)\n",
        "\n",
        "    # pointer\n",
        "    if self.pointer:\n",
        "      output = torch.zeros(batch_size, ext_vocab_size, device=DEVICE)\n",
        "      # distribute probabilities between generator and pointer\n",
        "      prob_ptr = F.sigmoid(self.ptr(combined))  # (batch size, 1)\n",
        "      #prob_ptr = torch.sigmoid(self.ptr(combined))\n",
        "      prob_gen = 1 - prob_ptr\n",
        "      # add generator probabilities to output\n",
        "      gen_output = F.softmax(logits, dim=1)  # can't use log_softmax due to adding probabilities\n",
        "      output[:, :self.vocab_size] = prob_gen * gen_output\n",
        "      # add pointer probabilities to output\n",
        "      ptr_output = enc_attn\n",
        "      output.scatter_add_(1, encoder_word_idx.transpose(0, 1), prob_ptr * ptr_output)\n",
        "      if log_prob: output = torch.log(output + eps)\n",
        "    else:\n",
        "      if log_prob: output = F.log_softmax(logits, dim=1)\n",
        "      else: output = F.softmax(logits, dim=1)\n",
        "\n",
        "    return output, hidden, enc_attn, prob_ptr\n",
        "\n",
        "\n",
        "class Seq2SeqOutput(object):\n",
        "\n",
        "  def __init__(self, encoder_outputs: torch.Tensor, encoder_hidden: torch.Tensor,\n",
        "               decoded_tokens: torch.Tensor, loss: Union[torch.Tensor, float]=0,\n",
        "               loss_value: float=0, enc_attn_weights: torch.Tensor=None,\n",
        "               ptr_probs: torch.Tensor=None):\n",
        "    self.encoder_outputs = encoder_outputs\n",
        "    self.encoder_hidden = encoder_hidden\n",
        "    self.decoded_tokens = decoded_tokens  # (out seq len, batch size)\n",
        "    self.loss = loss  # scalar\n",
        "    self.loss_value = loss_value  # float value, excluding coverage loss\n",
        "    self.enc_attn_weights = enc_attn_weights  # (out seq len, batch size, src seq len)\n",
        "    self.ptr_probs = ptr_probs  # (out seq len, batch size)\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab: Vocab, params: Params, max_dec_steps=None):\n",
        "    \"\"\"\n",
        "    :param vocab: mainly for info about special tokens and vocab size\n",
        "    :param params: model hyper-parameters\n",
        "    :param max_dec_steps: max num of decoding steps (only effective at test time, as during\n",
        "                          training the num of steps is determined by the `target_tensor`); it is\n",
        "                          safe to change `self.max_dec_steps` as the network architecture is\n",
        "                          independent of src/tgt seq lengths\n",
        "    Create the seq2seq model; its encoder and decoder will be created automatically.\n",
        "    \"\"\"\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.vocab = vocab\n",
        "    self.vocab_size = len(vocab)\n",
        "    if vocab.embeddings is not None:\n",
        "      self.embed_size = vocab.embeddings.shape[1]\n",
        "      if params.embed_size is not None and self.embed_size != params.embed_size:\n",
        "        print(\"Warning: Model embedding size %d is overriden by pre-trained embedding size %d.\"\n",
        "              % (params.embed_size, self.embed_size))\n",
        "      embedding_weights = torch.from_numpy(vocab.embeddings)\n",
        "    else:\n",
        "      self.embed_size = params.embed_size\n",
        "      embedding_weights = None\n",
        "    self.max_dec_steps = params.max_tgt_len + 1 if max_dec_steps is None else max_dec_steps\n",
        "    self.enc_attn = params.enc_attn\n",
        "    self.enc_attn_cover = params.enc_attn_cover\n",
        "    self.dec_attn = params.dec_attn\n",
        "    self.pointer = params.pointer\n",
        "    self.cover_loss = params.cover_loss\n",
        "    self.cover_func = params.cover_func\n",
        "    enc_total_size = params.hidden_size * 2 if params.enc_bidi else params.hidden_size\n",
        "    if params.dec_hidden_size:\n",
        "      dec_hidden_size = params.dec_hidden_size\n",
        "      self.enc_dec_adapter = nn.Linear(enc_total_size, dec_hidden_size)\n",
        "    else:\n",
        "      dec_hidden_size = enc_total_size\n",
        "      self.enc_dec_adapter = None\n",
        "\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.embed_size, padding_idx=vocab.PAD,\n",
        "                                  _weight=embedding_weights)\n",
        "    self.encoder = EncoderRNN(self.embed_size, params.hidden_size, params.enc_bidi,\n",
        "                              rnn_drop=params.enc_rnn_dropout)\n",
        "    self.decoder = DecoderRNN(self.vocab_size, self.embed_size, dec_hidden_size,\n",
        "                              enc_attn=params.enc_attn, dec_attn=params.dec_attn,\n",
        "                              pointer=params.pointer, out_embed_size=params.out_embed_size,\n",
        "                              tied_embedding=self.embedding if params.tie_embed else None,\n",
        "                              in_drop=params.dec_in_dropout, rnn_drop=params.dec_rnn_dropout,\n",
        "                              out_drop=params.dec_out_dropout, enc_hidden_size=enc_total_size)\n",
        "\n",
        "  def filter_oov(self, tensor, ext_vocab_size):\n",
        "    \"\"\"Replace any OOV index in `tensor` with UNK\"\"\"\n",
        "    if ext_vocab_size and ext_vocab_size > self.vocab_size:\n",
        "      result = tensor.clone()\n",
        "      result[tensor >= self.vocab_size] = self.vocab.UNK\n",
        "      return result\n",
        "    return tensor\n",
        "\n",
        "  def get_coverage_vector(self, enc_attn_weights):\n",
        "    \"\"\"Combine the past attention weights into one vector\"\"\"\n",
        "    if self.cover_func == 'max':\n",
        "      coverage_vector, _ = torch.max(torch.cat(enc_attn_weights), dim=0)\n",
        "    elif self.cover_func == 'sum':\n",
        "      coverage_vector = torch.sum(torch.cat(enc_attn_weights), dim=0)\n",
        "    else:\n",
        "      raise ValueError('Unrecognized cover_func: ' + self.cover_func)\n",
        "    return coverage_vector\n",
        "\n",
        "  def forward(self, input_tensor, target_tensor=None, input_lengths=None, criterion=None, *,\n",
        "              forcing_ratio=0, partial_forcing=True, ext_vocab_size=None, sample=False,\n",
        "              saved_out: Seq2SeqOutput=None, visualize: bool=None, include_cover_loss: bool=False)\\\n",
        "          -> Seq2SeqOutput:\n",
        "    \"\"\"\n",
        "    :param input_tensor: tensor of word indices, (src seq len, batch size)\n",
        "    :param target_tensor: tensor of word indices, (tgt seq len, batch size)\n",
        "    :param input_lengths: see explanation in `EncoderRNN`\n",
        "    :param criterion: the loss function; if set, loss will be returned\n",
        "    :param forcing_ratio: see explanation in `Params` (requires `target_tensor`, training only)\n",
        "    :param partial_forcing: see explanation in `Params` (training only)\n",
        "    :param ext_vocab_size: see explanation in `DecoderRNN`\n",
        "    :param sample: if True, the returned `decoded_tokens` will be based on random sampling instead\n",
        "                   of greedily selecting the token of the highest probability at each step\n",
        "    :param saved_out: the output of this function in a previous run; if set, the encoding step will\n",
        "                      be skipped and we reuse the encoder states saved in this object\n",
        "    :param visualize: whether to return data for attention and pointer visualization; if None,\n",
        "                      return if no `criterion` is provided\n",
        "    :param include_cover_loss: whether to include coverage loss in the returned `loss_value`\n",
        "    Run the seq2seq model for training or testing.\n",
        "    \"\"\"\n",
        "    input_length = input_tensor.size(0)\n",
        "    batch_size = input_tensor.size(1)\n",
        "    log_prob = not (sample or self.decoder.pointer)  # don't apply log too soon in these cases\n",
        "    if visualize is None:\n",
        "      visualize = criterion is None\n",
        "    if visualize and not (self.enc_attn or self.pointer):\n",
        "      visualize = False  # nothing to visualize\n",
        "\n",
        "    if target_tensor is None:\n",
        "      target_length = self.max_dec_steps\n",
        "    else:\n",
        "      target_length = target_tensor.size(0)\n",
        "\n",
        "    if forcing_ratio == 1:\n",
        "      # if fully teacher-forced, it may be possible to eliminate the for-loop over decoder steps\n",
        "      # for generality, this optimization is not investigated\n",
        "      use_teacher_forcing = True\n",
        "    elif forcing_ratio > 0:\n",
        "      if partial_forcing:\n",
        "        use_teacher_forcing = None  # decide later individually in each step\n",
        "      else:\n",
        "        use_teacher_forcing = random.random() < forcing_ratio\n",
        "    else:\n",
        "      use_teacher_forcing = False\n",
        "\n",
        "    if saved_out:  # reuse encoder states of a previous run\n",
        "      encoder_outputs = saved_out.encoder_outputs\n",
        "      encoder_hidden = saved_out.encoder_hidden\n",
        "      assert input_length == encoder_outputs.size(0)\n",
        "      assert batch_size == encoder_outputs.size(1)\n",
        "    else:  # run the encoder\n",
        "      encoder_hidden = self.encoder.init_hidden(batch_size)\n",
        "      # encoder_embedded: (input len, batch size, embed size)\n",
        "      encoder_embedded = self.embedding(self.filter_oov(input_tensor, ext_vocab_size))\n",
        "      encoder_outputs, encoder_hidden = \\\n",
        "        self.encoder(encoder_embedded, encoder_hidden, input_lengths)\n",
        "\n",
        "    # initialize return values\n",
        "    r = Seq2SeqOutput(encoder_outputs, encoder_hidden,\n",
        "                      torch.zeros(target_length, batch_size, dtype=torch.long))\n",
        "    if visualize:#Visualize attention\n",
        "      r.enc_attn_weights = torch.zeros(target_length, batch_size, input_length)\n",
        "      if self.pointer:\n",
        "        r.ptr_probs = torch.zeros(target_length, batch_size)\n",
        "\n",
        "    decoder_input = torch.tensor([self.vocab.SOS] * batch_size, device=DEVICE)\n",
        "    if self.enc_dec_adapter is None:\n",
        "      decoder_hidden = encoder_hidden\n",
        "    else:\n",
        "      decoder_hidden = self.enc_dec_adapter(encoder_hidden)\n",
        "    decoder_states = []\n",
        "    enc_attn_weights = []\n",
        "\n",
        "    for di in range(target_length):\n",
        "      decoder_embedded = self.embedding(self.filter_oov(decoder_input, ext_vocab_size))\n",
        "      if enc_attn_weights:\n",
        "        coverage_vector = self.get_coverage_vector(enc_attn_weights)\n",
        "      else:\n",
        "        coverage_vector = None\n",
        "      decoder_output, decoder_hidden, dec_enc_attn, dec_prob_ptr = \\\n",
        "        self.decoder(decoder_embedded, decoder_hidden, encoder_outputs,\n",
        "                     torch.cat(decoder_states) if decoder_states else None, coverage_vector,\n",
        "                     encoder_word_idx=input_tensor, ext_vocab_size=ext_vocab_size,\n",
        "                     log_prob=log_prob)\n",
        "      if self.dec_attn:\n",
        "        decoder_states.append(decoder_hidden)\n",
        "      # save the decoded tokens\n",
        "      if not sample:\n",
        "        _, top_idx = decoder_output.data.topk(1)  # top_idx shape: (batch size, k=1)\n",
        "      else:\n",
        "        prob_distribution = torch.exp(decoder_output) if log_prob else decoder_output\n",
        "        top_idx = torch.multinomial(prob_distribution, 1)\n",
        "      top_idx = top_idx.squeeze(1).detach()  # detach from history as input\n",
        "      r.decoded_tokens[di] = top_idx\n",
        "      # compute loss\n",
        "      if criterion:\n",
        "        if target_tensor is None:\n",
        "          gold_standard = top_idx  # for sampling\n",
        "        else:\n",
        "          gold_standard = target_tensor[di]\n",
        "        if not log_prob:\n",
        "          decoder_output = torch.log(decoder_output + eps)  # necessary for NLLLoss\n",
        "        nll_loss = criterion(decoder_output, gold_standard)\n",
        "        r.loss += nll_loss\n",
        "        r.loss_value += nll_loss.item()\n",
        "      # update attention history and compute coverage loss\n",
        "      if self.enc_attn_cover or (criterion and self.cover_loss > 0):\n",
        "        if coverage_vector is not None and criterion and self.cover_loss > 0:\n",
        "          coverage_loss = torch.sum(torch.min(coverage_vector, dec_enc_attn)) / batch_size \\\n",
        "                          * self.cover_loss\n",
        "          r.loss += coverage_loss\n",
        "          if include_cover_loss: r.loss_value += coverage_loss.item()\n",
        "        enc_attn_weights.append(dec_enc_attn.unsqueeze(0))\n",
        "      # save data for visualization\n",
        "      if visualize:\n",
        "        r.enc_attn_weights[di] = dec_enc_attn.data\n",
        "        if self.pointer:\n",
        "          r.ptr_probs[di] = dec_prob_ptr.squeeze(1).data\n",
        "      # decide the next input\n",
        "      if use_teacher_forcing or (use_teacher_forcing is None and random.random() < forcing_ratio):\n",
        "        decoder_input = target_tensor[di]  # teacher forcing\n",
        "      else:\n",
        "        decoder_input = top_idx\n",
        "    \n",
        "    return r\n",
        "\n",
        "  def beam_search(self, input_tensor, input_lengths=None, ext_vocab_size=None, beam_size=4, *,\n",
        "                  min_out_len=1, max_out_len=None, len_in_words=True) -> List[Hypothesis]:\n",
        "    \"\"\"\n",
        "    :param input_tensor: tensor of word indices, (src seq len, batch size); for now, batch size has\n",
        "                         to be 1\n",
        "    :param input_lengths: see explanation in `EncoderRNN`\n",
        "    :param ext_vocab_size: see explanation in `DecoderRNN`\n",
        "    :param beam_size: the beam size\n",
        "    :param min_out_len: required minimum output length\n",
        "    :param max_out_len: required maximum output length (if None, use the model's own value)\n",
        "    :param len_in_words: if True, count output length in words instead of tokens (i.e. do not count\n",
        "                         punctuations)\n",
        "    :return: list of the best decoded sequences, in descending order of probability\n",
        "    Use beam search to generate summaries.\n",
        "    \"\"\"\n",
        "    batch_size = input_tensor.size(1)\n",
        "    assert batch_size == 1\n",
        "    if max_out_len is None:\n",
        "      max_out_len = self.max_dec_steps - 1  # max_out_len doesn't count EOS\n",
        "\n",
        "    # encode\n",
        "    encoder_hidden = self.encoder.init_hidden(batch_size)\n",
        "    # encoder_embedded: (input len, batch size, embed size)\n",
        "    encoder_embedded = self.embedding(self.filter_oov(input_tensor, ext_vocab_size))\n",
        "    encoder_outputs, encoder_hidden = \\\n",
        "      self.encoder(encoder_embedded, encoder_hidden, input_lengths)\n",
        "    if self.enc_dec_adapter is None:\n",
        "      decoder_hidden = encoder_hidden\n",
        "    else:\n",
        "      decoder_hidden = self.enc_dec_adapter(encoder_hidden)\n",
        "    # turn batch size from 1 to beam size (by repeating)\n",
        "    # if we want dynamic batch size, the following must be created for all possible batch sizes\n",
        "    encoder_outputs = encoder_outputs.expand(-1, beam_size, -1).contiguous()\n",
        "    input_tensor = input_tensor.expand(-1, beam_size).contiguous()\n",
        "\n",
        "    # decode\n",
        "    hypos = [Hypothesis([self.vocab.SOS], [], decoder_hidden, [], [], 1)]\n",
        "    results, backup_results = [], []\n",
        "    step = 0\n",
        "    while hypos and step < 2 * max_out_len:  # prevent infinitely generating punctuations\n",
        "      # make batch size equal to beam size (n_hypos <= beam size)\n",
        "      n_hypos = len(hypos)\n",
        "      if n_hypos < beam_size:\n",
        "        hypos.extend(hypos[-1] for _ in range(beam_size - n_hypos))\n",
        "      # assemble existing hypotheses into a batch\n",
        "      decoder_input = torch.tensor([h.tokens[-1] for h in hypos], device=DEVICE)\n",
        "      decoder_hidden = torch.cat([h.dec_hidden for h in hypos], 1)\n",
        "      if self.dec_attn and step > 0:  # dim 0 is decoding step, dim 1 is beam batch\n",
        "        decoder_states = torch.cat([torch.cat(h.dec_states, 0) for h in hypos], 1)\n",
        "      else:\n",
        "        decoder_states = None\n",
        "      if self.enc_attn_cover:\n",
        "        enc_attn_weights = [torch.cat([h.enc_attn_weights[i] for h in hypos], 1)\n",
        "                            for i in range(step)]\n",
        "      else:\n",
        "        enc_attn_weights = []\n",
        "      if enc_attn_weights:\n",
        "        coverage_vector = self.get_coverage_vector(enc_attn_weights)  # shape: (beam size, src len)\n",
        "      else:\n",
        "        coverage_vector = None\n",
        "      # run the decoder over the assembled batch\n",
        "      decoder_embedded = self.embedding(self.filter_oov(decoder_input, ext_vocab_size))\n",
        "      decoder_output, decoder_hidden, dec_enc_attn, dec_prob_ptr = \\\n",
        "        self.decoder(decoder_embedded, decoder_hidden, encoder_outputs,\n",
        "                     decoder_states, coverage_vector,\n",
        "                     encoder_word_idx=input_tensor, ext_vocab_size=ext_vocab_size)\n",
        "      top_v, top_i = decoder_output.data.topk(beam_size)  # shape of both: (beam size, beam size)\n",
        "      # create new hypotheses\n",
        "      new_hypos = []\n",
        "      for in_idx in range(n_hypos):\n",
        "        for out_idx in range(beam_size):\n",
        "          new_tok = top_i[in_idx][out_idx].item()\n",
        "          new_prob = top_v[in_idx][out_idx].item()\n",
        "          if len_in_words:\n",
        "            non_word = not self.vocab.is_word(new_tok)\n",
        "          else:\n",
        "            non_word = new_tok == self.vocab.EOS  # only SOS & EOS don't count\n",
        "          new_hypo = hypos[in_idx].create_next(new_tok, new_prob,\n",
        "                                               decoder_hidden[0][in_idx].unsqueeze(0).unsqueeze(0),\n",
        "                                               self.dec_attn,\n",
        "                                               dec_enc_attn[in_idx].unsqueeze(0).unsqueeze(0)\n",
        "                                               if dec_enc_attn is not None else None, non_word)\n",
        "          new_hypos.append(new_hypo)\n",
        "      # process the new hypotheses\n",
        "      new_hypos = sorted(new_hypos, key=lambda h: -h.avg_log_prob)\n",
        "      hypos = []\n",
        "      new_complete_results, new_incomplete_results = [], []\n",
        "      for nh in new_hypos:\n",
        "        length = len(nh)\n",
        "        if nh.tokens[-1] == self.vocab.EOS:  # a complete hypothesis\n",
        "          if len(new_complete_results) < beam_size and min_out_len <= length <= max_out_len:\n",
        "            new_complete_results.append(nh)\n",
        "        elif len(hypos) < beam_size and length < max_out_len:  # an incomplete hypothesis\n",
        "          hypos.append(nh)\n",
        "        elif length == max_out_len and len(new_incomplete_results) < beam_size:\n",
        "          new_incomplete_results.append(nh)\n",
        "      if new_complete_results:\n",
        "        results.extend(new_complete_results)\n",
        "      elif new_incomplete_results:\n",
        "        backup_results.extend(new_incomplete_results)\n",
        "      step += 1\n",
        "    if not results:  # if no sequence ends with EOS within desired length, fallback to sequences\n",
        "      results = backup_results  # that are \"truncated\" at the end to max_out_len\n",
        "    return sorted(results, key=lambda h: -h.avg_log_prob)[:beam_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taBwwnbf1lWU",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Test.py { form-width: \"5px\" }\n",
        "\n",
        "import torch\n",
        "import tarfile\n",
        "from io import BytesIO\n",
        "from typing import Dict, Tuple, List, Union, Optional\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def decode_batch_output(decoded_tokens, vocab: Vocab, oov_dict: OOVDict) -> List[List[str]]:\n",
        "  \"\"\"Convert word indices to strings.\"\"\"\n",
        "  decoded_batch = []\n",
        "  if not isinstance(decoded_tokens, list):\n",
        "    decoded_tokens = decoded_tokens.transpose(0, 1).tolist()\n",
        "  for i, doc in enumerate(decoded_tokens):\n",
        "    decoded_doc = []\n",
        "    for word_idx in doc:\n",
        "      if word_idx >= len(vocab):\n",
        "        word = oov_dict.index2word.get((i, word_idx), '<UNK>')\n",
        "      else:\n",
        "        word = vocab[word_idx]\n",
        "      decoded_doc.append(word)\n",
        "      if word_idx == vocab.EOS:\n",
        "        break\n",
        "    decoded_batch.append(decoded_doc)\n",
        "  return decoded_batch\n",
        "\n",
        "\n",
        "def decode_batch(batch: Batch, model: Seq2Seq, vocab: Vocab, criterion=None, *, pack_seq=True,\n",
        "                 show_cover_loss=False) -> Tuple[List[List[str]], Seq2SeqOutput]:\n",
        "  \"\"\"Test the `model` on the `batch`, return the decoded textual tokens and the Seq2SeqOutput.\"\"\"\n",
        "  if not pack_seq:\n",
        "    input_lengths = None\n",
        "  else:\n",
        "    input_lengths = batch.input_lengths\n",
        "  with torch.no_grad():\n",
        "    input_tensor = batch.input_tensor.to(DEVICE)\n",
        "    if batch.target_tensor is None or criterion is None:\n",
        "      target_tensor = None\n",
        "    else:\n",
        "      target_tensor = batch.target_tensor.to(DEVICE)\n",
        "    out = model(input_tensor, target_tensor, input_lengths, criterion,\n",
        "                ext_vocab_size=batch.ext_vocab_size, include_cover_loss=show_cover_loss)\n",
        "    decoded_batch = decode_batch_output(out.decoded_tokens, vocab, batch.oov_dict)\n",
        "  target_length = batch.target_tensor.size(0)\n",
        "  out.loss_value /= target_length\n",
        "  return decoded_batch, out\n",
        "\n",
        "\n",
        "def decode_one(*args, **kwargs):\n",
        "  \"\"\"\n",
        "  Same as `decode_batch()` but because batch size is 1, the batch dim in visualization data is\n",
        "  eliminated.\n",
        "  \"\"\"\n",
        "  decoded_batch, out = decode_batch(*args, **kwargs)\n",
        "  decoded_doc = decoded_batch[0]\n",
        "  if out.enc_attn_weights is not None:\n",
        "    out.enc_attn_weights = out.enc_attn_weights[:len(decoded_doc), 0, :]\n",
        "  if out.ptr_probs is not None:\n",
        "    out.ptr_probs = out.ptr_probs[:len(decoded_doc), 0]\n",
        "  return decoded_doc, out\n",
        "\n",
        "\n",
        "def eval_batch(batch: Batch, model: Seq2Seq, vocab: Vocab, criterion=None, *, pack_seq=True,\n",
        "               show_cover_loss=False) -> Tuple[float, float]:\n",
        "  \"\"\"Test the `model` on the `batch`, return the ROUGE score and the loss.\"\"\"\n",
        "  decoded_batch, out = decode_batch(batch, model, vocab, criterion=criterion, pack_seq=pack_seq,\n",
        "                                    show_cover_loss=show_cover_loss)\n",
        "  examples = batch[0]\n",
        "  gold_summaries = [ex.tgt for ex in examples]\n",
        "  scores = rouge(gold_summaries, decoded_batch)\n",
        "  return out.loss_value, scores[0]['l_f']\n",
        "\n",
        "\n",
        "def eval_batch_output(tgt_tensor_or_tokens: Union[torch.Tensor, List[List[str]]], vocab: Vocab,\n",
        "                      oov_dict: OOVDict, *pred_tensors: torch.Tensor) -> List[Dict[str, float]]:\n",
        "  \"\"\"\n",
        "  :param tgt_tensor_or_tokens: the gold standard, either as indices or textual tokens\n",
        "  :param vocab: the fixed-size vocab\n",
        "  :param oov_dict: out-of-vocab dict\n",
        "  :param pred_tensors: one or more systems' prediction (output tensors)\n",
        "  :return: two-level score lookup (system index => ROUGE metric => value)\n",
        "  Evaluate one or more systems' output.\n",
        "  \"\"\"\n",
        "  decoded_batch = [decode_batch_output(pred_tensor, vocab, oov_dict)\n",
        "                   for pred_tensor in pred_tensors]\n",
        "  if isinstance(tgt_tensor_or_tokens, torch.Tensor):\n",
        "    gold_summaries = decode_batch_output(tgt_tensor_or_tokens, vocab, oov_dict)\n",
        "  else:\n",
        "    gold_summaries = tgt_tensor_or_tokens\n",
        "  scores = rouge(gold_summaries, *decoded_batch)\n",
        "  return scores\n",
        "\n",
        "\n",
        "def eval_bs_batch(batch: Batch, model: Seq2Seq, vocab: Vocab, *, pack_seq=True, beam_size=4,\n",
        "                  min_out_len=1, max_out_len=None, len_in_words=True, best_only=True,\n",
        "                  details: bool=True) -> Tuple[Optional[List[Dict[str, float]]], Optional[str]]:\n",
        "  \"\"\"\n",
        "  :param batch: a test batch of a single example\n",
        "  :param model: a trained summarizer\n",
        "  :param vocab: vocabulary of the trained summarizer\n",
        "  :param pack_seq: currently has no effect as batch size is 1\n",
        "  :param beam_size: the beam size\n",
        "  :param min_out_len: required minimum output length\n",
        "  :param max_out_len: required maximum output length (if None, use the model's own value)\n",
        "  :param len_in_words: if True, count output length in words instead of tokens (i.e. do not count\n",
        "                       punctuations)\n",
        "  :param best_only: if True, run ROUGE only on the best hypothesis instead of all `beam size` many\n",
        "  :param details: if True, also return a string containing the result of this document\n",
        "  :return: two-level score lookup (hypothesis index => ROUGE metric => value)\n",
        "  Test a trained summarizer on a document using the beam search decoder.\n",
        "  \"\"\"\n",
        "  assert len(batch.examples) == 1\n",
        "  with torch.no_grad():\n",
        "    input_tensor = batch.input_tensor.to(DEVICE)\n",
        "    hypotheses = model.beam_search(input_tensor, batch.input_lengths if pack_seq else None,\n",
        "                                   batch.ext_vocab_size, beam_size, min_out_len=min_out_len,\n",
        "                                   max_out_len=max_out_len, len_in_words=len_in_words)\n",
        "  if best_only:\n",
        "    to_decode = [hypotheses[0].tokens]\n",
        "  else:\n",
        "    to_decode = [h.tokens for h in hypotheses]\n",
        "  decoded_batch = decode_batch_output(to_decode, vocab, batch.oov_dict)\n",
        "  if details:\n",
        "    file_content = \"[System Summary]\\n\" + format_tokens(decoded_batch[0])\n",
        "  else:\n",
        "    file_content = None\n",
        "  if batch.examples[0].tgt is not None:  # run ROUGE if gold standard summary exists\n",
        "  #  gold_summaries = [batch.examples[0].tgt for _ in range(len(decoded_batch))]\n",
        "  #  scores = rouge(gold_summaries, decoded_batch)\n",
        "    if details:\n",
        "      file_content += \"\\n\\n\\n[Reference Summary]\\n\" + format_tokens(batch.examples[0].tgt)\n",
        "  #    file_content += \"\\n\\n\\n[ROUGE Scores]\\n\" + format_rouge_scores(scores[0]) + \"\\n\"\n",
        "  else:\n",
        "    scores = None\n",
        "  if details:\n",
        "    file_content += \"\\n\\n\\n[Source Text]\\n\" + format_tokens(batch.examples[0].src)\n",
        "  return scores, file_content\n",
        "\n",
        "\n",
        "def eval_bs(test_set: Dataset, vocab: Vocab, model: Seq2Seq, params: Params):\n",
        "  test_gen = test_set.generator(1, vocab, None, True if params.pointer else False)\n",
        "  n_samples = int(params.test_sample_ratio * len(test_set.pairs))\n",
        "\n",
        "  if params.test_save_results and params.model_path_prefix:\n",
        "    result_file = tarfile.open(params.model_path_prefix + \".results.tgz\", 'w:gz')\n",
        "  else:\n",
        "    result_file = None\n",
        "\n",
        "  model.eval()\n",
        "  r1, r2, rl, rsu4 = 0, 0, 0, 0\n",
        "  prog_bar = tqdm(range(1, n_samples + 1))\n",
        "  for i in prog_bar:\n",
        "    batch = next(test_gen)\n",
        "    scores, file_content = eval_bs_batch(batch, model, vocab, pack_seq=params.pack_seq,\n",
        "                                         beam_size=params.beam_size,\n",
        "                                         min_out_len=params.min_out_len,\n",
        "                                         max_out_len=params.max_out_len,\n",
        "                                         len_in_words=params.out_len_in_words,\n",
        "                                         details=result_file is not None)\n",
        "    if file_content:\n",
        "      file_content = file_content.encode('utf-8')\n",
        "      file_info = tarfile.TarInfo(name='%06d.txt' % i)\n",
        "      file_info.size = len(file_content)\n",
        "      result_file.addfile(file_info, fileobj=BytesIO(file_content))\n",
        "    if scores:\n",
        "      r1 += scores[0]['1_f']\n",
        "      r2 += scores[0]['2_f']\n",
        "      rl += scores[0]['l_f']\n",
        "      rsu4 += scores[0]['su4_f']\n",
        "      prog_bar.set_postfix(R1='%.4g' % (r1 / i * 100), R2='%.4g' % (r2 / i * 100),\n",
        "                           RL='%.4g' % (rl / i * 100), RSU4='%.4g' % (rsu4 / i * 100))\n",
        "\n",
        "test_flag=\"nogo\"\n",
        "if test_flag == \"go\":\n",
        "  import argparse\n",
        "  import os.path\n",
        "\n",
        "  #parser = argparse.ArgumentParser(description='Evaluate a summarization model.')\n",
        "  #parser.add_argument('--model', type=str, metavar='M', help='path to the model to be evaluated')\n",
        "  #args, unknown_args = parser.parse_known_args()\n",
        "  \n",
        "  p = Params()\n",
        "  #if unknown_args:  # allow command line args to override params.py\n",
        "  #  p.update(unknown_args)\n",
        "\n",
        "  if args.model:  # evaluate a specific model\n",
        "    filename = args.model\n",
        "  else:  # evaluate the best model\n",
        "    train_status = torch.load(p.model_path_prefix + \".train.pt\")\n",
        "    filename = '%s.%02d.pt' % (p.model_path_prefix, train_status['best_epoch_so_far'])\n",
        "\n",
        "  print(\"Evaluating %s...\" % filename)\n",
        "  m = torch.load(filename)  # use map_location='cpu' if you are testing a CUDA model using CPU\n",
        "\n",
        "  m.encoder.gru.flatten_parameters()\n",
        "  m.decoder.gru.flatten_parameters()\n",
        "\n",
        "  if hasattr(m, 'vocab'):\n",
        "    v = m.vocab\n",
        "  else:  # fixes for models trained by a previous version of the summarizer\n",
        "    filename, _ = os.path.splitext(p.data_path)\n",
        "    if p.vocab_size:\n",
        "      filename += \".%d\" % p.vocab_size\n",
        "    filename += '.vocab'\n",
        "    v = torch.load(filename)\n",
        "    m.vocab = v\n",
        "    m.max_dec_steps = m.max_output_length\n",
        "\n",
        "  d = Dataset(p.test_data_path)\n",
        "  eval_bs(d, v, m, p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7qYKykg1rVo",
        "colab_type": "code",
        "outputId": "6effce8c-2354-4570-f7d0-da2f502cbd1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "trn_ds = Dataset(filename=None, dataframe=trn, max_src_len=400, \n",
        "                 max_tgt_len=100, truncate_src=True, truncate_tgt=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading dataframe ...\n",
            "1424 pairs.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baMnggzD15wS",
        "colab_type": "code",
        "outputId": "59a93dfe-c241-43cf-eeaf-9910198165bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Par = Params()\n",
        "m = Seq2Seq(Voc, Par)\n",
        "\n",
        "!mkdir checkpoints"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Model embedding size 100 is overriden by pre-trained embedding size 50.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5Wi4Fnq1sh2",
        "colab_type": "code",
        "outputId": "aaee68f3-fe44-46e2-f8d0-09745a0ec240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "trn_ds.build_vocab(ttv='train',vocab_size=Par.vocab_size, embed_file='./glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building vocabulary... 30004 words.\n",
            "15052 pre-trained embeddings loaded.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Vocab at 0x7fdd1d69ac88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7VdDscV1t_t",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "examples = trn_ds.pairs[0:0 + 1]\n",
        "examples.sort(key=lambda x: -x.src_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWwOIQ7T1v5O",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "\n",
        "src_tensor = torch.zeros(400, 1, dtype=torch.long)\n",
        "max_tgt_len = max(x.tgt_len for x in examples)\n",
        "tgt_tensor = torch.zeros(max_tgt_len, 1, dtype=torch.long)\n",
        "lengths = [x.src_len for x in examples]\n",
        "for i, example in enumerate(examples):\n",
        "  for j, word in enumerate(example.src):\n",
        "    \n",
        "    idx = Voc[word]\n",
        "    src_tensor[j, i] = idx\n",
        "    tgt_tensor[j, i] = idx\n",
        "  src_tensor[example.src_len - 1, i] = src_vocab.EOS\n",
        "  tgt_tensor[example.tgt_len - 1, i] = tgt_vocab.EOS\n",
        "yield Batch(examples, src_tensor, tgt_tensor, lengths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Elvsr23q1xUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_gen = trn_ds.generator(batch_size=8, src_vocab=Voc, tgt_vocab=Voc, ext_vocab=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNYKFnFV10ot",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Train.py { form-width: \"5px\" }\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import os\n",
        "from torch import optim\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_batch(batch: Batch, model: Seq2Seq, criterion, optimizer, *,\n",
        "                pack_seq=True, forcing_ratio=0.5, partial_forcing=True, sample=False,\n",
        "                rl_ratio: float=0.1, vocab=None, grad_norm: float=0, show_cover_loss=False):\n",
        "  if not pack_seq:\n",
        "    input_lengths = None\n",
        "  else:\n",
        "    input_lengths = batch.input_lengths\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  input_tensor = batch.input_tensor.to(DEVICE)\n",
        "  target_tensor = batch.target_tensor.to(DEVICE)\n",
        "  ext_vocab_size = batch.ext_vocab_size\n",
        "\n",
        "  out = model(input_tensor, target_tensor, input_lengths, criterion,\n",
        "              forcing_ratio=forcing_ratio, partial_forcing=partial_forcing, sample=sample,\n",
        "              ext_vocab_size=ext_vocab_size, include_cover_loss=show_cover_loss)\n",
        "\n",
        "  if rl_ratio > 0:\n",
        "    assert vocab is not None\n",
        "    sample_out = model(input_tensor, saved_out=out, criterion=criterion, sample=True,\n",
        "                       ext_vocab_size=ext_vocab_size)\n",
        "    baseline_out = model(input_tensor, saved_out=out, visualize=False,\n",
        "                         ext_vocab_size=ext_vocab_size)\n",
        "    scores = eval_batch_output([ex.tgt for ex in batch.examples], \n",
        "                               vocab, batch.oov_dict,\n",
        "                               sample_out.decoded_tokens, \n",
        "                               baseline_out.decoded_tokens)\n",
        "    greedy_rouge = scores[1]['l_f']\n",
        "    neg_reward = greedy_rouge - scores[0]['l_f']\n",
        "    # if sample > baseline, the reward is positive (i.e. good exploration), \n",
        "    # rl_loss is negative\n",
        "    rl_loss = neg_reward * sample_out.loss\n",
        "    rl_loss_value = neg_reward * sample_out.loss_value\n",
        "    loss = (1 - rl_ratio) * out.loss + rl_ratio * rl_loss\n",
        "    loss_value = (1 - rl_ratio) * out.loss_value + rl_ratio * rl_loss_value\n",
        "  else:\n",
        "    loss = out.loss\n",
        "    loss_value = out.loss_value\n",
        "    greedy_rouge = None\n",
        "\n",
        "  loss.backward()\n",
        "  if grad_norm > 0:\n",
        "    clip_grad_norm_(model.parameters(), grad_norm)\n",
        "  optimizer.step()\n",
        "\n",
        "  target_length = target_tensor.size(0)\n",
        "  return loss_value / target_length, greedy_rouge\n",
        "\n",
        "\n",
        "def train(train_generator, vocab: Vocab, model: Seq2Seq, params: Params, valid_generator=None,\n",
        "          saved_state: dict=None):\n",
        "  # variables for plotting\n",
        "  plot_points_per_epoch = max(math.log(params.n_batches, 1.6), 1.)\n",
        "  plot_every = round(params.n_batches / plot_points_per_epoch)\n",
        "  plot_losses, cached_losses = [], []\n",
        "  plot_val_losses, plot_val_metrics = [], []\n",
        "  #count number of parameters of the model\n",
        "  total_parameters = sum(parameter.numel() for parameter in model.parameters()\n",
        "                         if parameter.requires_grad)\n",
        "  print(\"Training %d trainable parameters...\" % total_parameters)\n",
        "  model.to(DEVICE)\n",
        "  \n",
        "  if saved_state is None:\n",
        "    if params.optimizer == 'adagrad':\n",
        "      optimizer = optim.Adagrad(model.parameters(), lr=params.lr,\n",
        "                                initial_accumulator_value=params.adagrad_accumulator)\n",
        "    else:\n",
        "      optimizer = optim.Adam(model.parameters(), lr=params.lr)\n",
        "    past_epochs = 0\n",
        "    total_batch_count = 0\n",
        "  else:\n",
        "    optimizer = saved_state['optimizer']\n",
        "    past_epochs = saved_state['epoch']\n",
        "    total_batch_count = saved_state['total_batch_count']\n",
        "  if params.lr_decay:\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, params.lr_decay_step, params.lr_decay,\n",
        "                                             past_epochs - 1)\n",
        "  criterion = nn.NLLLoss(ignore_index=vocab.PAD)\n",
        "  best_avg_loss, best_epoch_id = float(\"inf\"), None\n",
        "\n",
        "  for epoch_count in range(1 + past_epochs, params.n_epochs + 1):\n",
        "    if params.lr_decay:\n",
        "      lr_scheduler.step()\n",
        "    rl_ratio = params.rl_ratio if epoch_count >= params.rl_start_epoch else 0\n",
        "    epoch_loss, epoch_metric = 0, 0\n",
        "    epoch_avg_loss, valid_avg_loss, valid_avg_metric = None, None, None\n",
        "    prog_bar = tqdm(range(1, params.n_batches + 1), desc='Epoch %d' % epoch_count)\n",
        "    model.train()\n",
        "\n",
        "    for batch_count in prog_bar:  # training batches\n",
        "      if params.forcing_decay_type:\n",
        "        if params.forcing_decay_type == 'linear':\n",
        "          forcing_ratio = max(0, params.forcing_ratio - params.forcing_decay * total_batch_count)\n",
        "        elif params.forcing_decay_type == 'exp':\n",
        "          forcing_ratio = params.forcing_ratio * (params.forcing_decay ** total_batch_count)\n",
        "        elif params.forcing_decay_type == 'sigmoid':\n",
        "          forcing_ratio = params.forcing_ratio * params.forcing_decay / (\n",
        "                  params.forcing_decay + math.exp(total_batch_count / params.forcing_decay))\n",
        "        else:\n",
        "          raise ValueError('Unrecognized forcing_decay_type: ' + params.forcing_decay_type)\n",
        "      else:\n",
        "        forcing_ratio = params.forcing_ratio\n",
        "\n",
        "      batch = next(train_generator)\n",
        "      loss, metric = train_batch(batch, model, criterion, optimizer, pack_seq=params.pack_seq,\n",
        "                                 forcing_ratio=forcing_ratio,\n",
        "                                 partial_forcing=params.partial_forcing, sample=params.sample,\n",
        "                                 rl_ratio=rl_ratio, vocab=vocab, grad_norm=params.grad_norm,\n",
        "                                 show_cover_loss=params.show_cover_loss)\n",
        "\n",
        "      epoch_loss += float(loss)\n",
        "      epoch_avg_loss = epoch_loss / batch_count\n",
        "      if metric is not None:  # print ROUGE as well if reinforcement learning is enabled\n",
        "        epoch_metric += metric\n",
        "        epoch_avg_metric = epoch_metric / batch_count\n",
        "        prog_bar.set_postfix(loss='%g' % epoch_avg_loss, rouge='%.4g' % (epoch_avg_metric * 100))\n",
        "      else:\n",
        "        prog_bar.set_postfix(loss='%g' % epoch_avg_loss)\n",
        "\n",
        "      cached_losses.append(loss)\n",
        "      total_batch_count += 1\n",
        "      if total_batch_count % plot_every == 0:\n",
        "        period_avg_loss = sum(cached_losses) / len(cached_losses)\n",
        "        plot_losses.append(period_avg_loss)\n",
        "        cached_losses = []\n",
        "\n",
        "    if valid_generator is not None:  # validation batches\n",
        "      valid_loss, valid_metric = 0, 0\n",
        "      prog_bar = tqdm(range(1, params.n_val_batches + 1), desc='Valid %d' % epoch_count)\n",
        "      model.eval()\n",
        "\n",
        "      for batch_count in prog_bar:\n",
        "        batch = next(valid_generator)\n",
        "        loss, metric = eval_batch(batch, model, vocab, criterion, pack_seq=params.pack_seq,\n",
        "                                  show_cover_loss=params.show_cover_loss)\n",
        "        valid_loss += loss\n",
        "        valid_metric += metric\n",
        "        valid_avg_loss = valid_loss / batch_count\n",
        "        valid_avg_metric = valid_metric / batch_count\n",
        "        prog_bar.set_postfix(loss='%g' % valid_avg_loss, rouge='%.4g' % (valid_avg_metric * 100))\n",
        "\n",
        "      plot_val_losses.append(valid_avg_loss)\n",
        "      plot_val_metrics.append(valid_avg_metric)\n",
        "\n",
        "      metric_loss = -valid_avg_metric  # choose the best model by ROUGE instead of loss\n",
        "      if metric_loss < best_avg_loss:\n",
        "        best_epoch_id = epoch_count\n",
        "        best_avg_loss = metric_loss\n",
        "\n",
        "    else:  # no validation, \"best\" is defined by training loss\n",
        "      if epoch_avg_loss < best_avg_loss:\n",
        "        best_epoch_id = epoch_count\n",
        "        best_avg_loss = epoch_avg_loss\n",
        "\n",
        "    if params.model_path_prefix:\n",
        "      # save model\n",
        "      filename = '%s.%02d.pt' % (params.model_path_prefix, epoch_count)\n",
        "      torch.save(model, filename)\n",
        "      if not params.keep_every_epoch:  # clear previously saved models\n",
        "        for epoch_id in range(1 + past_epochs, epoch_count):\n",
        "          if epoch_id != best_epoch_id:\n",
        "            try:\n",
        "              prev_filename = '%s.%02d.pt' % (params.model_path_prefix, epoch_id)\n",
        "              os.remove(prev_filename)\n",
        "            except FileNotFoundError:\n",
        "              pass\n",
        "      # save training status\n",
        "      torch.save({\n",
        "        'epoch': epoch_count,\n",
        "        'total_batch_count': total_batch_count,\n",
        "        'train_avg_loss': epoch_avg_loss,\n",
        "        'valid_avg_loss': valid_avg_loss,\n",
        "        'valid_avg_metric': valid_avg_metric,\n",
        "        'best_epoch_so_far': best_epoch_id,\n",
        "        'params': params,\n",
        "        'optimizer': optimizer\n",
        "      }, '%s.train.pt' % params.model_path_prefix)\n",
        "\n",
        "    if rl_ratio > 0:\n",
        "      params.rl_ratio **= params.rl_ratio_power\n",
        "\n",
        "    show_plot(plot_losses, plot_every, plot_val_losses, plot_val_metrics, params.n_batches,\n",
        "              params.model_path_prefix)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O5386hreA3F",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "isayso ='no'\n",
        "flaggy='no'\n",
        "resume_from = ''\n",
        "if flaggy == \"go\":\n",
        "  import argparse\n",
        "\n",
        "  #parser = argparse.ArgumentParser(description='Train the seq2seq abstractive summarizer.')\n",
        "  #parser.add_argument('--resume_from', type=str, metavar='R',\n",
        "  #                    help='path to a saved training status (*.train.pt)')\n",
        "  #args, unknown_args = parser.parse_known_args()\n",
        "\n",
        "  if resume_from:\n",
        "    print(\"Resuming from %s...\" % resume_from)\n",
        "    train_status = torch.load(resume_from)\n",
        "    m = torch.load('%s.%02d.pt' % (resume_from[:-9], train_status['epoch']))\n",
        "    p = train_status['params']\n",
        "  else:\n",
        "    p = Params()\n",
        "    m = None\n",
        "    train_status = None\n",
        "\n",
        "  #if unknown_args:  # allow command line args to override params.py\n",
        "  #  p.update(unknown_args)\n",
        "\n",
        "  dataset = Dataset(p.data_path, max_src_len=p.max_src_len, max_tgt_len=p.max_tgt_len,\n",
        "                    truncate_src=p.truncate_src, truncate_tgt=p.truncate_tgt)\n",
        "  if m is None:\n",
        "    v = dataset.build_vocab(p.vocab_size, embed_file=p.embed_file)\n",
        "    m = Seq2Seq(v, p)\n",
        "  else:\n",
        "    v = dataset.build_vocab(p.vocab_size)\n",
        "\n",
        "  train_gen = dataset.generator(p.batch_size, v, v, True if p.pointer else False)\n",
        "  #if p.val_data_path and isayso=='go':\n",
        "  #  val_dataset = Dataset(p.val_data_path, max_src_len=p.max_src_len, max_tgt_len=p.max_tgt_len,\n",
        "  #                        truncate_src=p.truncate_src, truncate_tgt=p.truncate_tgt)\n",
        "  #  val_gen = val_dataset.generator(p.val_batch_size, v, v, True if p.pointer else False)\n",
        "  #  print('Validation data path exists... {}'.format(p.val_data_path))\n",
        "  #else:\n",
        "  #  val_gen = None\n",
        "  \n",
        "\n",
        "  train(train_gen, v, m, p, val_gen, train_status)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKHhYucy18Cp",
        "colab_type": "code",
        "outputId": "176fbd5c-cda2-4678-b214-2eadce8890f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "train_status = None\n",
        "train(trn_gen, Voc, m, Par, train_status)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 3199144 trainable parameters...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch 1:   0%|          | 0/250 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "Epoch 1: 100%|██████████| 250/250 [05:26<00:00,  1.31s/it, loss=4.68957]\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type EncoderRNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DecoderRNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "Epoch 2: 100%|██████████| 250/250 [05:30<00:00,  1.35s/it, loss=2.85948]\n",
            "Epoch 3: 100%|██████████| 250/250 [05:31<00:00,  1.33s/it, loss=2.26377]\n",
            "Epoch 4: 100%|██████████| 250/250 [05:31<00:00,  1.32s/it, loss=2.01686]\n",
            "Epoch 5: 100%|██████████| 250/250 [05:31<00:00,  1.32s/it, loss=1.83073]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpBRD4u21-c_",
        "colab_type": "code",
        "outputId": "b2c14308-318a-4a9d-b673-a5db1691dc63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "torch.save(m, 'abs.3.train.pt')\n",
        "sm = torch.load('abs.3.train.pt')\n",
        "sm.state_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type EncoderRNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DecoderRNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.state_dict of Seq2Seq(\n",
              "  (enc_dec_adapter): Linear(in_features=300, out_features=200, bias=True)\n",
              "  (embedding): Embedding(53341, 50, padding_idx=0)\n",
              "  (encoder): EncoderRNN(\n",
              "    (gru): GRU(50, 150, bidirectional=True)\n",
              "  )\n",
              "  (decoder): DecoderRNN(\n",
              "    (gru): GRU(50, 200)\n",
              "    (enc_bilinear): Bilinear(in1_features=200, in2_features=300, out_features=1, bias=True)\n",
              "    (ptr): Linear(in_features=500, out_features=1, bias=True)\n",
              "    (pre_out): Linear(in_features=500, out_features=50, bias=True)\n",
              "    (out): Linear(in_features=50, out_features=53341, bias=True)\n",
              "  )\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M6m95As1_no",
        "colab_type": "code",
        "outputId": "7d2449f2-50db-45f5-e1b4-274948ff95c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_ds = Dataset(filename=None, dataframe=test, max_src_len=400, \n",
        "                 max_tgt_len=100, truncate_src=True, truncate_tgt=True)\n",
        "\n",
        "test_gen = test_ds.generator(batch_size=8, src_vocab=Voc, tgt_vocab=Voc, ext_vocab=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading dataframe ...\n",
            "356 pairs.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fQ3yBsS2Al5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next_gen = next(test_gen)\n",
        "examp, src_tens, targ_tens, lens, oovs = next_gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDEgWEjd2BeG",
        "colab_type": "code",
        "outputId": "d7d0eb8a-91da-4dad-d6ab-0b2d6a66125d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "pred_src = examp[0][0]\n",
        "pred_src = \" \".join(pred_src)\n",
        "pred_tgt = examp[0][1]\n",
        "pred_tgt = \" \".join(pred_tgt)\n",
        "\n",
        "dec_batch, out = decode_batch(next_gen, sm, Voc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxiHLwyx2CXx",
        "colab_type": "code",
        "outputId": "4cee39e4-8dac-42eb-aedd-ab12421c26cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "decoded = \" \".join(dec_batch[0])\n",
        "decoded, pred_tgt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"`` The law lords are simply wrong to imply that this is a decision to detain these people on the whim or the certificate of the home secretary , '' he told BBC Radio 4\\\\s Today programme . `` The law lords are simply wrong to imply that this is a decision to detain these people on the whim or the certificate of the home secretary , '' he told BBC Radio 4\\\\s Today programme . `` The law lords are simply wrong to imply that this is a decision to detain these people on the whim or the certificate of\",\n",
              " \"He said the Law Lords\\\\ ruling was an `` embarrassment '' for the government and major changes were needed to the law.The foreign secretary said the right to life was the `` most important liberty '' and the government had a duty to protect people from terrorism.New Home Secretary Charles Clarke vowed the nine men would remain in prison while the law was being reviewed.On Thursday , Lord Bingham - a senior law lord - said the rules were incompatible with the European Convention on Human Rights as they allowed detentions `` in a way that discriminates on the ground\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}