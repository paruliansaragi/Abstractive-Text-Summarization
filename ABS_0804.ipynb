{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ABS-0804.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paruliansaragi/Abstractive-Text-Summarization/blob/master/ABS_0804.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf4CRhalWDNu",
        "colab_type": "code",
        "outputId": "2afcf170-078c-4e33-d0e7-ee09e437c2c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!unzip ./bbc-news-summary.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./bbc-news-summary.zip\n",
            "  inflating: BBC News Summary.rar    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP31HXR5WHnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unrar x \"./BBC News Summary.rar\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ak41D56fDC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK5H67tYWizi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir \"./data/train\"\n",
        "!mkdir \"./data/test\"\n",
        "!mkdir \"./data/val\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziZeHuOHdXyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir \"./data/train/article\"\n",
        "!mkdir \"./data/train/summary\"\n",
        "!mkdir \"./data/test/article\"\n",
        "!mkdir \"./data/test/summary\"\n",
        "!mkdir \"./data/val/article\"\n",
        "!mkdir \"./data/val/summary\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF0Ev1X_CxKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir bert_pretrained_models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yJaQMUN2HL3",
        "colab_type": "code",
        "outputId": "d173c173-a7bf-4651-a2a1-5714c4033457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip -P bert_pretrained_models/;\n",
        "!unzip bert_pretrained_models/uncased_L-12_H-768_A-12.zip -d bert_pretrained_models/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-09 07:04:31--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘bert_pretrained_models/uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   128MB/s    in 3.0s    \n",
            "\n",
            "2019-04-09 07:04:35 (128 MB/s) - ‘bert_pretrained_models/uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  bert_pretrained_models/uncased_L-12_H-768_A-12.zip\n",
            "   creating: bert_pretrained_models/uncased_L-12_H-768_A-12/\n",
            "  inflating: bert_pretrained_models/uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: bert_pretrained_models/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: bert_pretrained_models/uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: bert_pretrained_models/uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: bert_pretrained_models/uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA91HgH1Bwi_",
        "colab_type": "code",
        "outputId": "443a5e2e-4a0b-415b-9fdc-1570224152cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "!test -d texar_repo || git clone https://github.com/asyml/texar.git texar_repo\n",
        "if not 'texar_repo' in sys.path:\n",
        "  sys.path += ['texar_repo']\n",
        "!pip install funcsigs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'texar_repo'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)   \u001b[K\rremote: Counting objects:  28% (2/7)   \u001b[K\rremote: Counting objects:  42% (3/7)   \u001b[K\rremote: Counting objects:  57% (4/7)   \u001b[K\rremote: Counting objects:  71% (5/7)   \u001b[K\rremote: Counting objects:  85% (6/7)   \u001b[K\rremote: Counting objects: 100% (7/7)   \u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 10767 (delta 1), reused 2 (delta 1), pack-reused 10760\u001b[K\n",
            "Receiving objects: 100% (10767/10767), 2.44 MiB | 16.44 MiB/s, done.\n",
            "Resolving deltas: 100% (8190/8190), done.\n",
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Installing collected packages: funcsigs\n",
            "Successfully installed funcsigs-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYShvtI_DYm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import collections\n",
        "import sys\n",
        "from texar_repo.examples.bert.utils import data_utils, model_utils, tokenization\n",
        "import importlib\n",
        "import tensorflow as tf\n",
        "import texar as tx \n",
        "from texar_repo.examples.bert import config_classifier as config_downstream\n",
        "from texar_repo.texar.utils import transformer_utils\n",
        "from texar_repo.examples.transformer.utils import data_utils, utils\n",
        "from texar_repo.examples.transformer.bleu_tool import bleu_wrapper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBbDLqVIK6t-",
        "colab_type": "text"
      },
      "source": [
        "Create TF.record files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjwU60CzgOGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#config\n",
        "\n",
        "dcoder_config = {\n",
        "    'dim': 768,\n",
        "    'num_blocks': 6,\n",
        "    'multihead_attention': {\n",
        "        'num_heads': 8,\n",
        "        'output_dim': 768\n",
        "        # See documentation for more optional hyperparameters\n",
        "    },\n",
        "    'position_embedder_hparams': {\n",
        "        'dim': 768\n",
        "    },\n",
        "    'initializer': {\n",
        "        'type': 'variance_scaling_initializer',\n",
        "        'kwargs': {\n",
        "            'scale': 1.0,\n",
        "            'mode': 'fan_avg',\n",
        "            'distribution': 'uniform',\n",
        "        },\n",
        "    },\n",
        "    'poswise_feedforward': tx.modules.default_transformer_poswise_net_hparams(\n",
        "        output_dim=768)\n",
        "}\n",
        "\n",
        "loss_label_confidence = 0.9\n",
        "\n",
        "random_seed = 1234\n",
        "beam_width = 5\n",
        "alpha = 0.6\n",
        "hidden_dim = 768\n",
        "\n",
        "\n",
        "opt = {\n",
        "    'optimizer': {\n",
        "        'type': 'AdamOptimizer',\n",
        "        'kwargs': {\n",
        "            'beta1': 0.9,\n",
        "            'beta2': 0.997,\n",
        "            'epsilon': 1e-9\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "lr = {\n",
        "    'learning_rate_schedule': 'constant.linear_warmup.rsqrt_decay.rsqrt_depth',\n",
        "    'lr_constant': 2 * (hidden_dim ** -0.5),\n",
        "    'static_lr': 1e-3,\n",
        "    'warmup_steps': 2000,\n",
        "}\n",
        "\n",
        "bos_token_id =101\n",
        "eos_token_id = 102\n",
        "\n",
        "model_dir= \"./models\"\n",
        "run_mode= \"train_and_evaluate\"\n",
        "batch_size = 32\n",
        "test_batch_size = 32\n",
        "\n",
        "max_train_epoch = 20\n",
        "display_steps = 100\n",
        "eval_steps = 100000\n",
        "\n",
        "max_decoding_length = 400\n",
        "\n",
        "max_seq_length_src = 512\n",
        "max_seq_length_tgt = 400\n",
        "\n",
        "bert_pretrain_dir = 'bert_pretrained_models/uncased_L-12_H-768_A-12'\n",
        "#config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4buRKjiQUPmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample():\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence.\n",
        "                For single sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second\n",
        "                sequence. Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "                specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.src_txt = text_a\n",
        "        self.tgt_txt = text_b\n",
        "        \n",
        "class InputFeatures():\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, src_input_ids,src_input_mask,src_segment_ids,tgt_input_ids,tgt_input_mask,tgt_labels):\n",
        "        self.src_input_ids = src_input_ids\n",
        "        self.src_input_mask = src_input_mask\n",
        "        self.src_segment_ids = src_segment_ids\n",
        "        self.tgt_input_ids = tgt_input_ids\n",
        "        self.tgt_input_mask = tgt_input_mask \n",
        "        self.tgt_labels = tgt_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftOepLyeUTv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with tf.gfile.Open(input_file, \"r\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            i = 0\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "        return lines\n",
        "\n",
        "    #Amended Texar here by adding a read file option to read text files\n",
        "    @classmethod\n",
        "    def _read_file(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a text file.\"\"\"\n",
        "        with tf.gfile.Open(input_file, \"r\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\n\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            i = 0\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "        return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6ftzwaNVRyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_single_example(ex_index, example, max_seq_length_src,max_seq_length_tgt,\n",
        "                           tokenizer):\n",
        "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "    \"\"\"\n",
        "    label_map = {}\n",
        "    for (i, label) in enumerate(label_list):\n",
        "        label_map[label] = i\n",
        "    \"\"\"\n",
        "    tokens_a = tokenizer.tokenize(example.src_txt)\n",
        "    tokens_b = tokenizer.tokenize(example.tgt_txt)\n",
        "\n",
        "\n",
        "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "    # length is less than the specified length.\n",
        "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "    if len(tokens_a) > max_seq_length_src - 2:\n",
        "            tokens_a = tokens_a[0:(max_seq_length_src - 2)]\n",
        "    \n",
        "    if len(tokens_b) > max_seq_length_tgt - 2:\n",
        "            tokens_b = tokens_b[0:(max_seq_length_tgt - 2)]\n",
        "\n",
        "    \n",
        "    tokens_src = []\n",
        "    segment_ids_src = []\n",
        "    tokens_src.append(\"[CLS]\")\n",
        "    segment_ids_src.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens_src.append(token)\n",
        "        segment_ids_src.append(0)\n",
        "    tokens_src.append(\"[SEP]\")\n",
        "    segment_ids_src.append(0)\n",
        "  \n",
        "\n",
        "    tokens_tgt = []\n",
        "    segment_ids_tgt = []\n",
        "    tokens_tgt.append(\"[CLS]\")\n",
        "    #segment_ids_tgt.append(0)\n",
        "    for token in tokens_b:\n",
        "        tokens_tgt.append(token)\n",
        "        #segment_ids_tgt.append(0)\n",
        "    tokens_tgt.append(\"[SEP]\")\n",
        "    #segment_ids_tgt.append(0)\n",
        "\n",
        "    input_ids_src = tokenizer.convert_tokens_to_ids(tokens_src)\n",
        "   \n",
        "    \n",
        "\n",
        "    input_ids_tgt = tokenizer.convert_tokens_to_ids(tokens_tgt)\n",
        "    \n",
        "    #Adding begiining and end token\n",
        "    input_ids_tgt = input_ids_tgt[:-1] \n",
        "    \n",
        "    input_mask_src = [1] * len(input_ids_src)\n",
        "\n",
        "\n",
        "    input_mask_tgt = [1] * len(input_ids_tgt)\n",
        "    \n",
        "    labels_tgt = input_ids_tgt[1:]\n",
        "    \n",
        "    \n",
        "    labels_tgt.append(0)\n",
        "    \n",
        "    #print(len(input_ids_tgt))\n",
        "    #print(len(input_mask_tgt))\n",
        "    #print(len(labels_tgt))\n",
        "    #print(len(segment_ids_tgt))\n",
        "    \n",
        "    while len(input_ids_src) < max_seq_length_src:\n",
        "        input_ids_src.append(0)\n",
        "        input_mask_src.append(0)\n",
        "        segment_ids_src.append(0)\n",
        "\n",
        "    while len(input_ids_tgt) < max_seq_length_tgt:\n",
        "        input_ids_tgt.append(0)\n",
        "        input_mask_tgt.append(0)\n",
        "        segment_ids_tgt.append(0)\n",
        "        labels_tgt.append(0)\n",
        "\n",
        "    feature = InputFeatures( src_input_ids=input_ids_src,src_input_mask=input_mask_src,src_segment_ids=segment_ids_src,\n",
        "        tgt_input_ids=input_ids_tgt,tgt_input_mask=input_mask_tgt,tgt_labels=labels_tgt)\n",
        "\n",
        "    \n",
        "    return feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOjT1YHfVg5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNDailymail(DataProcessor):\n",
        "    \"\"\"Processor for the CoLA data set (GLUE version).\"\"\"\n",
        "    \n",
        "    def get_article(self, art_file, data_dir):\n",
        "        return self._read_file(os.path.join('./', art_file))\n",
        "      \n",
        "    def get_summary(self, summary_file, data_dir):\n",
        "        return self._read_file(os.path.join('./', summary_file))\n",
        "      \n",
        "    def get_train_examples(self, summary, article):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        #Provide an article and a summary, read the file which returns a list of lines and create target and source in an example\n",
        "        \n",
        "        return self._create_examples(article, summary, \"train\")\n",
        "\n",
        "    def get_dev_examples(self, sum_file_name, art_file_name, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        summary = self._read_file(os.path.join('./', sum_file_name))\n",
        "        article = self._read_file(os.path.join('./', art_file_name))\n",
        "        return self._create_examples(article, summary, \"dev\")\n",
        "\n",
        "    def get_test_examples(self, sum_file_name, art_file_name, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        summary = self._read_file(os.path.join('./', sum_file_name))\n",
        "        article = self._read_file(os.path.join('./', art_file_name))\n",
        "        return self._create_examples(article, summary, \"test\")\n",
        "\n",
        "    def _create_examples(self, src_lines,tgt_lines,set_type):\n",
        "        examples = [] \n",
        "        for i,data in enumerate(zip(src_lines,tgt_lines)):\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            if set_type == \"test\" and i == 0:\n",
        "                continue\n",
        "            else:\n",
        "                #print(data)\n",
        "                if len(data[0])==0 or len(data[1])==0:\n",
        "                  continue\n",
        "                src_lines = tokenization.convert_to_unicode(data[0][0])\n",
        "                tgt_lines = tokenization.convert_to_unicode(data[1][0])\n",
        "                examples.append(InputExample(guid=guid, text_a=src_lines,\n",
        "                                         text_b=tgt_lines))\n",
        "        return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVUcvlcIVHhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def file_based_convert_examples_to_features(\n",
        "        examples, max_seq_length_src,max_seq_length_tgt,tokenizer, output_file):\n",
        "    \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
        "\n",
        "    writer = tf.python_io.TFRecordWriter(output_file)\n",
        "\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        #print(\"ex_index\",ex_index)\n",
        "\n",
        "        if (ex_index+1) %1000 == 0 :\n",
        "          print(\"------------processed..{}...examples\".format(ex_index))\n",
        "          \n",
        "        feature = convert_single_example(ex_index, example,\n",
        "                                         max_seq_length_src,max_seq_length_tgt,tokenizer)\n",
        "\n",
        "        def create_int_feature(values):\n",
        "            return tf.train.Feature(\n",
        "                int64_list=tf.train.Int64List(value=list(values)))\n",
        "\n",
        "        features = collections.OrderedDict()\n",
        "        features[\"src_input_ids\"] = create_int_feature(feature.src_input_ids)\n",
        "        features[\"src_input_mask\"] = create_int_feature(feature.src_input_mask)\n",
        "        features[\"src_segment_ids\"] = create_int_feature(feature.src_segment_ids)\n",
        "\n",
        "        features[\"tgt_input_ids\"] = create_int_feature(feature.tgt_input_ids)\n",
        "        features[\"tgt_input_mask\"] = create_int_feature(feature.tgt_input_mask)\n",
        "        features['tgt_labels'] = create_int_feature(feature.tgt_labels)\n",
        "        \n",
        "        \n",
        "        \n",
        "        #print(feature.tgt_labels)\n",
        "        \n",
        "\n",
        "        tf_example = tf.train.Example(\n",
        "            features=tf.train.Features(feature=features))\n",
        "        writer.write(tf_example.SerializeToString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2FO2ISDThzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def read_lines(file):\n",
        "    \"\"\"Gets the lines from a file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        the lines of text of the input file\n",
        "    \"\"\"\n",
        "    with open(file, 'rb') as fd:\n",
        "        first_lines = fd.readlines()\n",
        "    return first_lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_WUn_R8Ytnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def merge_art_sum(art_folder_path, sum_folder_path, category):\n",
        "    \"\"\"Merges first lines of text files in one folder, and\n",
        "    writes combined lines into new output file\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    folder_path : str\n",
        "        String representation of the folder path containing the text files.\n",
        "    output_filename : str\n",
        "        Name of the output file the merged lines will be written to.\n",
        "    \"\"\"\n",
        "    # get all text files\n",
        "    art_txt_files = glob.glob(art_folder_path + \"*.txt\")\n",
        "    sum_txt_files = glob.glob(sum_folder_path + \"*.txt\")\n",
        "    # get first lines; map to each text file (sorted)\n",
        "    art_arr = []\n",
        "    sum_arr = []\n",
        "    for fle in art_txt_files:\n",
        "      # open the file and then call .read() to get the text\n",
        "      art_arr.append(read_lines(fle))\n",
        "    for fl in sum_txt_files:\n",
        "      sum_arr.append(read_lines(fl))\n",
        "    # create dir\n",
        "    #if not os.path.exists(output_dir) : os.makedirs(output_dir)\n",
        "    # return dataframe\n",
        "    art_df = pd.DataFrame({'article':art_arr})\n",
        "    sum_df = pd.DataFrame({'summary':sum_arr})\n",
        "    df = pd.merge(sum_df,art_df,left_index=True,right_index=True)\n",
        "    return df.to_csv(category+'.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty8AQfpPYwT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categories = ['business', 'entertainment', 'politics', 'sport', 'tech']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0RR8rgHYx8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def art_or_sum(category):\n",
        "  artpath = './BBC News Summary/News Articles/' + category +'/'\n",
        "  sumpath = './BBC News Summary/Summaries/' + category +'/'\n",
        "  return artpath, sumpath"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSJ9myvXY4q3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in categories:\n",
        "  art, summ = art_or_sum(i)\n",
        "  merge_art_sum(art, summ, i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj0gmrlDZcaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "politic = pd.read_csv('politics.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy-NnhTebH1m",
        "colab_type": "code",
        "outputId": "594d2333-71ec-4d5f-b27a-c86973679ba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "politic.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>summary</th>\n",
              "      <th>article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[b'A Labour spokesman later said their removal...</td>\n",
              "      <td>[b'Milburn defends poster campaign\\n', b'\\n', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[b\"Climate change could be completely out of c...</td>\n",
              "      <td>[b'MSPs hear renewed climate warning\\n', b'\\n'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[b'\"Limiting the number of regional casinos to...</td>\n",
              "      <td>[b'Jowell confirms casino climbdown\\n', b'\\n',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[b'BBC political editor Andrew Marr said that ...</td>\n",
              "      <td>[b'Brown outlines third term vision\\n', b'\\n',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[b'A deal bringing Turkey a step closer to EU ...</td>\n",
              "      <td>[b\"Turkey deal 'to help world peace'\\n\", b'\\n'...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                            summary  \\\n",
              "0           0  [b'A Labour spokesman later said their removal...   \n",
              "1           1  [b\"Climate change could be completely out of c...   \n",
              "2           2  [b'\"Limiting the number of regional casinos to...   \n",
              "3           3  [b'BBC political editor Andrew Marr said that ...   \n",
              "4           4  [b'A deal bringing Turkey a step closer to EU ...   \n",
              "\n",
              "                                             article  \n",
              "0  [b'Milburn defends poster campaign\\n', b'\\n', ...  \n",
              "1  [b'MSPs hear renewed climate warning\\n', b'\\n'...  \n",
              "2  [b'Jowell confirms casino climbdown\\n', b'\\n',...  \n",
              "3  [b'Brown outlines third term vision\\n', b'\\n',...  \n",
              "4  [b\"Turkey deal 'to help world peace'\\n\", b'\\n'...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIWJ3Cr0afip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processor = CNNDailymail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxaOE3ZGarLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examps = []\n",
        "for (i, s) in enumerate(politic.iterrows()):\n",
        "  examps.append(processor.get_train_examples(politic['summary'][i], politic['article'][i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2BUGM0bcfVq",
        "colab_type": "code",
        "outputId": "b5067793-58d9-43ba-df28-16f69c8b4acb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(examps), len(examps[0]), len(examps[1]), len(examps[2]), len(examps[3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(417, 1019, 702, 1010, 912)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oyru0kt3cvQB",
        "colab_type": "code",
        "outputId": "f20f89c9-45ec-4657-f4cb-1c21cdb04eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import numpy as np\n",
        "a = np.array(examps)\n",
        "a.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(417,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GNb-81wdDMj",
        "colab_type": "code",
        "outputId": "cf384520-55ed-458b-f805-371678e5c128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "for each in examps[:5]:\n",
        "  file_based_convert_examples_to_features(each, max_seq_length_src,max_seq_length_tgt, tokenizer, os.path.join('./', \"train.tf_record\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------processed..999...examples\n",
            "------------processed..999...examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uImTPpxeE5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = tf.data.TFRecordDataset('train.tf_record')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP2qaHSmeGsH",
        "colab_type": "code",
        "outputId": "40d76870-f9b0-42d2-ee3b-e628a041e8e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "d"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TFRecordDatasetV1 shapes: (), types: tf.string>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03lGaEpWdsvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = file_based_input_fn_builder(\n",
        "            input_file='train.tf_record',\n",
        "            max_seq_length_src=max_seq_length_src,\n",
        "            max_seq_length_tgt =max_seq_length_tgt,\n",
        "            is_training=True,\n",
        "            drop_remainder=True,\n",
        "            is_distributed=False)({'batch_size': batch_size})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLeIb2f1VtCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset(processor,\n",
        "                tokenizer,\n",
        "                data_dir,\n",
        "                sum_file,\n",
        "                art_file,\n",
        "                max_seq_length_src,\n",
        "                max_seq_length_tgt,\n",
        "                batch_size,\n",
        "                mode,\n",
        "                output_dir,\n",
        "                is_distributed=False):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        processor: Data Preprocessor, must have get_lables,\n",
        "            get_train/dev/test/examples methods defined.\n",
        "        tokenizer: The Sentence Tokenizer. Generally should be\n",
        "            SentencePiece Model.\n",
        "        data_dir: The input data directory.\n",
        "        max_seq_length: Max sequence length.\n",
        "        batch_size: mini-batch size.\n",
        "        model: `train`, `eval` or `test`.\n",
        "        output_dir: The directory to save the TFRecords in.\n",
        "    \"\"\"\n",
        "    #label_list = processor.get_labels()\n",
        "    if mode == 'train':\n",
        "      #fetches a source and target example\n",
        "        train_examples = processor.get_train_examples(data_dir, sum_file, art_file)\n",
        "        train_file = os.path.join(output_dir, \"train.tf_record\")\n",
        "        #train_file = \"./train.tf_record\"\n",
        "        file_based_convert_examples_to_features(\n",
        "            train_examples, max_seq_length_src,max_seq_length_tgt,\n",
        "            tokenizer, train_file)\n",
        "        dataset = file_based_input_fn_builder(\n",
        "            input_file=train_file,\n",
        "            max_seq_length_src=max_seq_length_src,\n",
        "            max_seq_length_tgt =max_seq_length_tgt,\n",
        "            is_training=True,\n",
        "            drop_remainder=True,\n",
        "            is_distributed=is_distributed)({'batch_size': batch_size})\n",
        "    elif mode == 'eval':\n",
        "        eval_examples = processor.get_dev_examples(data_dir, sum_file, art_file)\n",
        "        eval_file = os.path.join(output_dir, \"eval.tf_record\")\n",
        "        #eval_file = \"gs://bert_summ/eval.tf_record\"\n",
        "        file_based_convert_examples_to_features(\n",
        "            eval_examples, max_seq_length_src,max_seq_length_tgt,\n",
        "            tokenizer, eval_file)\n",
        "        dataset = file_based_input_fn_builder(\n",
        "            input_file=eval_file,\n",
        "            max_seq_length_src=max_seq_length_src,\n",
        "            max_seq_length_tgt =max_seq_length_tgt,\n",
        "            is_training=False,\n",
        "            drop_remainder=True,\n",
        "            is_distributed=is_distributed)({'batch_size': batch_size})\n",
        "    elif mode == 'test':\n",
        "      \n",
        "        test_examples = processor.get_test_examples(data_dir, sum_file, art_file)\n",
        "        test_file = os.path.join(output_dir, \"predict.tf_record\")\n",
        "        #test_file = \"gs://bert_summ/predict.tf_record\"\n",
        "        \n",
        "        file_based_convert_examples_to_features(\n",
        "            test_examples, max_seq_length_src,max_seq_length_tgt,\n",
        "            tokenizer, test_file)\n",
        "        dataset = file_based_input_fn_builder(\n",
        "            input_file=test_file,\n",
        "            max_seq_length_src=max_seq_length_src,\n",
        "            max_seq_length_tgt =max_seq_length_tgt,\n",
        "            is_training=False,\n",
        "            drop_remainder=True,\n",
        "            is_distributed=is_distributed)({'batch_size': batch_size})\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhpMI0gNV25F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_config = model_utils.transform_bert_to_texar_config(\n",
        "            os.path.join(bert_pretrain_dir, 'bert_config.json'))\n",
        "\n",
        "tokenizer = tokenization.FullTokenizer(\n",
        "        vocab_file=os.path.join(bert_pretrain_dir, 'vocab.txt'),\n",
        "        do_lower_case=True)\n",
        "\n",
        "vocab_size = len(tokenizer.vocab)\n",
        "\n",
        "processor = CNNDailymail()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjoVlG-6QPeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = get_dataset(processor,\n",
        "                            tokenizer,\n",
        "                            \"./BBC News Summary/\",\n",
        "                            \"News Articles/business\",\n",
        "                            \"Summaries/business\",\n",
        "                            0.8,\n",
        "                            max_seq_length_src,\n",
        "                            max_seq_length_tgt,\n",
        "                            4,\n",
        "                            'train',\n",
        "                            \"./\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeSMSY2B8QGg",
        "colab_type": "code",
        "outputId": "0958bc5b-d6b4-4228-c259-9d9d17d0463b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "train_dataset = get_dataset(processor,tokenizer,\"./\",\"001.txt\",\"001 (1).txt\",max_seq_length_src,max_seq_length_tgt,4,'train',\"./\")\n",
        "eval_dataset = get_dataset(processor,tokenizer,\"./\",\"002.txt\", \"002 (1).txt\",max_seq_length_src,max_seq_length_tgt,4,'eval',\"./\")\n",
        "test_dataset = get_dataset(processor,tokenizer,\"./\", \"003.txt\", \"003 (1).txt\",max_seq_length_src,max_seq_length_tgt,4,'test',\"./\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:0' shape=(512,) dtype=int64>, 'src_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:1' shape=(512,) dtype=int64>, 'src_segment_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:2' shape=(512,) dtype=int64>, 'tgt_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:3' shape=(400,) dtype=int64>, 'tgt_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:4' shape=(400,) dtype=int64>, 'tgt_labels': <tf.Tensor 'ParseSingleExample/ParseSingleExample:5' shape=(400,) dtype=int64>}\n",
            "dict_keys(['src_input_ids', 'src_input_mask', 'src_segment_ids', 'tgt_input_ids', 'tgt_input_mask', 'tgt_labels'])\n",
            "{'src_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:0' shape=(512,) dtype=int64>, 'src_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:1' shape=(512,) dtype=int64>, 'src_segment_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:2' shape=(512,) dtype=int64>, 'tgt_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:3' shape=(400,) dtype=int64>, 'tgt_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:4' shape=(400,) dtype=int64>, 'tgt_labels': <tf.Tensor 'ParseSingleExample/ParseSingleExample:5' shape=(400,) dtype=int64>}\n",
            "dict_keys(['src_input_ids', 'src_input_mask', 'src_segment_ids', 'tgt_input_ids', 'tgt_input_mask', 'tgt_labels'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSVTUve0jUS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eggs = testy.get_train_examples('001.txt', '001 (1).txt', './')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaNe2VuTj4NK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_based_convert_examples_to_features(\n",
        "            eggs, max_seq_length_src,max_seq_length_tgt,\n",
        "            tokenizer,\"train.tf_record\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUYYTnRehDfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = get_dataset(processor, tokenizer,\"./\",\"001.txt\",\"001 (1).txt\",max_seq_length_src,max_seq_length_tgt,4,'train',\"./\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLn9cVIH7bDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tests = processor.get_train_examples('002.txt', '002 (1).txt', './')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrrU7Kfs7j0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_based_convert_examples_to_features(\n",
        "            tests, max_seq_length_src,max_seq_length_tgt,\n",
        "            tokenizer,\"predict.tf_record\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPFBX4Oo74xV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evally = processor.get_train_examples('003.txt', '003 (1).txt', './')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KoeYYpb79Wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_based_convert_examples_to_features(\n",
        "            evally, max_seq_length_src,max_seq_length_tgt,\n",
        "            tokenizer,\"eval.tf_record\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89sdwotyVWKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def file_based_input_fn_builder(input_file, max_seq_length_src,max_seq_length_tgt, is_training,\n",
        "                                drop_remainder, is_distributed=False):\n",
        "    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "    name_to_features = {\n",
        "        \"src_input_ids\": tf.FixedLenFeature([max_seq_length_src], tf.int64),\n",
        "        \"src_input_mask\": tf.FixedLenFeature([max_seq_length_src], tf.int64),\n",
        "        \"src_segment_ids\": tf.FixedLenFeature([max_seq_length_src], tf.int64),\n",
        "        \"tgt_input_ids\": tf.FixedLenFeature([max_seq_length_tgt], tf.int64),\n",
        "        \"tgt_input_mask\": tf.FixedLenFeature([max_seq_length_tgt], tf.int64),\n",
        "        \"tgt_labels\" : tf.FixedLenFeature([max_seq_length_tgt], tf.int64),\n",
        "        \n",
        "        \n",
        "    }\n",
        "\n",
        "    def _decode_record(record, name_to_features):\n",
        "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "        example = tf.parse_single_example(record, name_to_features)\n",
        "        print(example)\n",
        "        print(example.keys())\n",
        "\n",
        "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "        # So cast all int64 to int32.\n",
        "        for name in list(example.keys()):\n",
        "            t = example[name]\n",
        "            if t.dtype == tf.int64:\n",
        "                t = tf.to_int32(t)\n",
        "            example[name] = t\n",
        "\n",
        "        return example\n",
        "\n",
        "    def input_fn(params):\n",
        "        \"\"\"The actual input function.\"\"\"\n",
        "        batch_size = params[\"batch_size\"]\n",
        "\n",
        "        # For training, we want a lot of parallel reading and shuffling.\n",
        "        # For eval, we want no shuffling and parallel reading doesn't matter.\n",
        "        d = tf.data.TFRecordDataset(input_file)\n",
        "        if is_training:\n",
        "\n",
        "            if is_distributed:\n",
        "                import horovod.tensorflow as hvd\n",
        "                tf.logging.info('distributed mode is enabled.'\n",
        "                                'size:{} rank:{}'.format(hvd.size(), hvd.rank()))\n",
        "                # https://github.com/uber/horovod/issues/223\n",
        "                d = d.shard(hvd.size(), hvd.rank())\n",
        "\n",
        "                d = d.repeat()\n",
        "                d = d.shuffle(buffer_size=100)\n",
        "                d = d.apply(\n",
        "                    tf.contrib.data.map_and_batch(\n",
        "                        lambda record: _decode_record(record, name_to_features),\n",
        "                        batch_size=batch_size//hvd.size(),\n",
        "                        drop_remainder=drop_remainder))\n",
        "            else:\n",
        "                tf.logging.info('distributed mode is not enabled.')\n",
        "                d = d.repeat()\n",
        "                d = d.shuffle(buffer_size=100)\n",
        "                d = d.apply(\n",
        "                    tf.contrib.data.map_and_batch(\n",
        "                        lambda record: _decode_record(record, name_to_features),\n",
        "                        batch_size=batch_size,\n",
        "                        drop_remainder=drop_remainder))\n",
        "\n",
        "        else:\n",
        "            d = d.apply(\n",
        "                tf.contrib.data.map_and_batch(\n",
        "                    lambda record: _decode_record(record, name_to_features),\n",
        "                    batch_size=batch_size,\n",
        "                    drop_remainder=drop_remainder))\n",
        "\n",
        "        return d\n",
        "    return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVhbalK3ggox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "asfm = os.path.join(\"./\", \"001 (1).txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOnjjVhugl_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.gfile.Open(asfm, \"r\") as f:\n",
        "  reader = csv.reader(f, delimiter=\"\\n\")\n",
        "  lines = []\n",
        "  i = 0\n",
        "  for line in reader:\n",
        "    lines.append(line)\n",
        "  asfma = lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fy5FCmNpHMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#inputs to the model\n",
        "src_input_ids = tf.placeholder(tf.int64, shape=(None, None))\n",
        "src_segment_ids = tf.placeholder(tf.int64, shape=(None, None))\n",
        "tgt_input_ids = tf.placeholder(tf.int64, shape=(None, None))\n",
        "tgt_segment_ids = tf.placeholder(tf.int64, shape=(None, None))\n",
        "\n",
        "batch_size = tf.shape(src_input_ids)[0]\n",
        "\n",
        "src_input_length = tf.reduce_sum(1 - tf.to_int32(tf.equal(src_input_ids, 0)),\n",
        "                             axis=1)\n",
        "tgt_input_length = tf.reduce_sum(1 - tf.to_int32(tf.equal(src_input_ids, 0)),\n",
        "                             axis=1)\n",
        "\n",
        "labels = tf.placeholder(tf.int64, shape=(None, None))\n",
        "is_target = tf.to_float(tf.not_equal(labels, 0))\n",
        "\n",
        "\n",
        "global_step = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
        "learning_rate = tf.placeholder(tf.float64, shape=(), name='lr')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M8iXqnNkahF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create the iterator \n",
        "iterator = tx.data.FeedableDataIterator({\n",
        "        'train': tf.data.TFRecordDataset('train.tf_record'), 'eval': tf.data.TFRecordDataset('eval.tf_record'), 'test': tf.data.TFRecordDataset('predict.tf_record')})\n",
        "\n",
        "batch = iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6MsvxOO9eZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encoder Bert model\n",
        "print(\"Intializing the Bert Encoder Graph\")\n",
        "with tf.variable_scope('bert'):\n",
        "        embedder = tx.modules.WordEmbedder(\n",
        "            vocab_size=bert_config.vocab_size,\n",
        "            hparams=bert_config.embed)\n",
        "        word_embeds = embedder(src_input_ids)\n",
        "\n",
        "        # Creates segment embeddings for each type of tokens.\n",
        "        segment_embedder = tx.modules.WordEmbedder(\n",
        "            vocab_size=bert_config.type_vocab_size,\n",
        "            hparams=bert_config.segment_embed)\n",
        "        segment_embeds = segment_embedder(src_segment_ids)\n",
        "\n",
        "        input_embeds = word_embeds + segment_embeds\n",
        "\n",
        "        # The BERT model (a TransformerEncoder)\n",
        "        encoder = tx.modules.TransformerEncoder(hparams=bert_config.encoder)\n",
        "        encoder_output = encoder(input_embeds, src_input_length)\n",
        "        \n",
        "        # Builds layers for downstream classification, which is also initialized\n",
        "        # with BERT pre-trained checkpoint.\n",
        "        with tf.variable_scope(\"pooler\"):\n",
        "            # Uses the projection of the 1st-step hidden vector of BERT output\n",
        "            # as the representation of the sentence\n",
        "            bert_sent_hidden = tf.squeeze(encoder_output[:, 0:1, :], axis=1)\n",
        "            bert_sent_output = tf.layers.dense(\n",
        "                bert_sent_hidden, config_downstream.hidden_dim,\n",
        "                activation=tf.tanh)\n",
        "            output = tf.layers.dropout(\n",
        "                bert_sent_output, rate=0.1, training=tx.global_mode_train())\n",
        "\n",
        "\n",
        "print(\"loading the bert pretrained weights\")\n",
        "# Loads pretrained BERT model parameters\n",
        "init_checkpoint = os.path.join(bert_pretrain_dir, 'bert_model.ckpt')\n",
        "#init_checkpoint = \"gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/bert_model.ckpt\"\n",
        "model_utils.init_bert_checkpoint(init_checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aIoq71x9mrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#decoder part and mle losss\n",
        "tgt_embedding = tf.concat(\n",
        "    [tf.zeros(shape=[1, embedder.dim]), embedder.embedding[1:, :]], axis=0)\n",
        "\n",
        "decoder = tx.modules.TransformerDecoder(embedding=tgt_embedding,\n",
        "                             hparams=dcoder_config)\n",
        "# For training\n",
        "outputs = decoder(\n",
        "    memory=encoder_output,\n",
        "    memory_sequence_length=src_input_length,\n",
        "    inputs=embedder(tgt_input_ids),\n",
        "    sequence_length=tgt_input_length,\n",
        "    decoding_strategy='train_greedy',\n",
        "    mode=tf.estimator.ModeKeys.TRAIN\n",
        ")\n",
        "\n",
        "mle_loss = transformer_utils.smoothing_cross_entropy(\n",
        "        outputs.logits, labels, vocab_size, loss_label_confidence)\n",
        "mle_loss = tf.reduce_sum(mle_loss * is_target) / tf.reduce_sum(is_target)\n",
        "\n",
        "train_op = tx.core.get_train_op(\n",
        "        mle_loss,\n",
        "        learning_rate=learning_rate,\n",
        "        global_step=global_step,\n",
        "        hparams=opt)\n",
        "\n",
        "tf.summary.scalar('lr', learning_rate)\n",
        "tf.summary.scalar('mle_loss', mle_loss)\n",
        "summary_merged = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U97q5KK9q-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prediction \n",
        "start_tokens = tf.fill([tx.utils.get_batch_size(src_input_ids)],\n",
        "                       bos_token_id)\n",
        "predictions = decoder(\n",
        "    memory=encoder_output,\n",
        "    memory_sequence_length=src_input_length,\n",
        "    decoding_strategy='infer_greedy',\n",
        "    beam_width=beam_width,\n",
        "    start_tokens=start_tokens,\n",
        "    end_token=eos_token_id,\n",
        "    max_decoding_length=400,\n",
        "    mode=tf.estimator.ModeKeys.PREDICT\n",
        ")\n",
        "if beam_width <= 1:\n",
        "    inferred_ids = predictions[0].sample_id\n",
        "else:\n",
        "    # Uses the best sample by beam search\n",
        "    inferred_ids = predictions['sample_id'][:, :, 0]\n",
        "\n",
        "\n",
        "saver = tf.train.Saver(max_to_keep=5)\n",
        "best_results = {'score': 0, 'epoch': -1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5BvdMFF9yQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _train_epoch(sess, epoch, step, smry_writer):\n",
        "        \n",
        "            \n",
        "        fetches = {\n",
        "            'step': global_step,\n",
        "            'train_op': train_op,\n",
        "            'smry': summary_merged,\n",
        "            'loss': mle_loss,\n",
        "        }\n",
        "        \n",
        "        while True:\n",
        "            try:\n",
        "              feed_dict = {\n",
        "                iterator.handle: iterator.get_handle(sess, 'train'),\n",
        "                tx.global_mode(): tf.estimator.ModeKeys.TRAIN,\n",
        "              }\n",
        "              op = sess.run([batch],feed_dict)\n",
        "              feed_dict = {\n",
        "                   src_input_ids:op[0]['src_input_ids'],\n",
        "                   src_segment_ids : op[0]['src_segment_ids'],\n",
        "                   tgt_input_ids:op[0]['tgt_input_ids'],\n",
        "\n",
        "                   labels:op[0]['tgt_labels'],\n",
        "                   learning_rate: utils.get_lr(step, lr),\n",
        "                   tx.global_mode(): tf.estimator.ModeKeys.TRAIN\n",
        "                }\n",
        "\n",
        "\n",
        "              fetches_ = sess.run(fetches, feed_dict=feed_dict)\n",
        "              step, loss = fetches_['step'], fetches_['loss']\n",
        "              if step and step % display_steps == 0:\n",
        "                  logger.info('step: %d, loss: %.4f', step, loss)\n",
        "                  print('step: %d, loss: %.4f' % (step, loss))\n",
        "                  smry_writer.add_summary(fetches_['smry'], global_step=step)\n",
        "\n",
        "              if step and step % 1000 == 0:\n",
        "                  model_path = \"./models/model_\"+str(step)+\".ckpt\"\n",
        "                  logger.info('saving model to %s', model_path)\n",
        "                  print('saving model to %s' % model_path)\n",
        "                  saver.save(sess, model_path)\n",
        "              if step and step % eval_steps == 0:\n",
        "                  _eval_epoch(sess, epoch, mode='eval')\n",
        "            except tf.errors.OutOfRangeError:\n",
        "                break\n",
        "\n",
        "        return step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6R8-nl594Qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _eval_epoch(sess, epoch, mode):\n",
        "\n",
        "        references, hypotheses = [], []\n",
        "        bsize = test_batch_size\n",
        "        fetches = {\n",
        "                'inferred_ids': inferred_ids,\n",
        "            }\n",
        "        bno=0\n",
        "        while True:\n",
        "            \n",
        "            #print(\"Temp\",temp)\n",
        "            try:\n",
        "              print(\"Batch\",bno)\n",
        "              feed_dict = {\n",
        "              iterator.handle: iterator.get_handle(sess, 'eval'),\n",
        "              tx.global_mode(): tf.estimator.ModeKeys.EVAL,\n",
        "              }\n",
        "              op = sess.run([batch],feed_dict)\n",
        "              feed_dict = {\n",
        "                   src_input_ids:op[0]['src_input_ids'],\n",
        "                   src_segment_ids : op[0]['src_segment_ids'],\n",
        "                   tx.global_mode(): tf.estimator.ModeKeys.EVAL\n",
        "              }\n",
        "              fetches_ = sess.run(fetches, feed_dict=feed_dict)\n",
        "              labels = op[0]['tgt_labels']\n",
        "              hypotheses.extend(h.tolist() for h in fetches_['inferred_ids'])\n",
        "              references.extend(r.tolist() for r in labels)\n",
        "              hypotheses = utils.list_strip_eos(hypotheses, eos_token_id)\n",
        "              references = utils.list_strip_eos(references, eos_token_id)\n",
        "              bno = bno+1\n",
        "              \n",
        "            except tf.errors.OutOfRangeError:\n",
        "                break\n",
        "\n",
        "\n",
        "        if mode == 'eval':\n",
        "            # Writes results to files to evaluate BLEU\n",
        "            # For 'eval' mode, the BLEU is based on token ids (rather than\n",
        "            # text tokens) and serves only as a surrogate metric to monitor\n",
        "            # the training process\n",
        "            fname = os.path.join(model_dir, 'tmp.eval')\n",
        "            #fname = \"./tmp.eval\"\n",
        "            #fname = \"gs://bert_summ/models/tmp.eval\"\n",
        "            hypotheses = tx.utils.str_join(hypotheses)\n",
        "            references = tx.utils.str_join(references)\n",
        "            hyp_fn, ref_fn = tx.utils.write_paired_text(\n",
        "                hypotheses, references, fname, mode='s')\n",
        "            eval_bleu = bleu_wrapper(ref_fn, hyp_fn, case_sensitive=True)\n",
        "            eval_bleu = 100. * eval_bleu\n",
        "            logger.info('epoch: %d, eval_bleu %.4f', epoch, eval_bleu)\n",
        "            print('epoch: %d, eval_bleu %.4f' % (epoch, eval_bleu))\n",
        "\n",
        "            if eval_bleu > best_results['score']:\n",
        "                logger.info('epoch: %d, best bleu: %.4f', epoch, eval_bleu)\n",
        "                best_results['score'] = eval_bleu\n",
        "                best_results['epoch'] = epoch\n",
        "                model_path = os.path.join(model_dir, 'best-model.ckpt')\n",
        "                #model_path = \"gs://bert_summ/models/best-model.ckpt\"\n",
        "                logger.info('saving model to %s', model_path)\n",
        "                print('saving model to %s' % model_path)\n",
        "                saver.save(sess, model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wTIv7p_CFpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx.utils.maybe_create_dir(model_dir)\n",
        "logging_file = os.path.join('./', 'logging.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj1KjPVa96UP",
        "colab_type": "code",
        "outputId": "1620e84c-ec56-49ab-dc56-d3a0c998b138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "#tx.utils.maybe_create_dir(model_dir)\n",
        "#logging_file = os.path.join(model_dir, 'logging.txt')\n",
        "\n",
        "#model_dir = \"gs://bert_summ/models/\"\n",
        "logging_file= \"logging.txt\"\n",
        "logger = utils.get_logger(logging_file)\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "\n",
        "    smry_writer = tf.summary.FileWriter(model_dir, graph=sess.graph)\n",
        "\n",
        "    if run_mode == 'train_and_evaluate':\n",
        "        logger.info('Begin running with train_and_evaluate mode')\n",
        "        print('Training has begun...')\n",
        "\n",
        "        if tf.train.latest_checkpoint(model_dir) is not None:\n",
        "            logger.info('Restore latest checkpoint in %s' % model_dir)\n",
        "            print('Restoring latest checkpoint in %s ' % model_dir)\n",
        "            saver.restore(sess, tf.train.latest_checkpoint(model_dir))\n",
        "        \n",
        "        iterator.initialize_dataset(sess)\n",
        "\n",
        "        \n",
        "        for epoch in range(max_train_epoch):\n",
        "          \n",
        "          iterator.restart_dataset(sess, 'train')\n",
        "          step = _train_epoch(sess, epoch, step, smry_writer)\n",
        "          print(step)\n",
        "        print('Training has ended')\n",
        "    elif run_mode == 'test':\n",
        "        logger.info('Begin running with test mode')\n",
        "\n",
        "        logger.info('Restore latest checkpoint in %s' % model_dir)\n",
        "        saver.restore(sess, tf.train.latest_checkpoint(model_dir))\n",
        "\n",
        "        _eval_epoch(sess, 0, mode='test')\n",
        "\n",
        "    else:\n",
        "        raise ValueError('Unknown mode: {}'.format(run_mode))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training has begun...\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "Training has ended\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTezNkdEFrHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}