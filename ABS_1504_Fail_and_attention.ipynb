{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ABS-1504-Fail-and-attention.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paruliansaragi/Abstractive-Text-Summarization/blob/master/ABS_1504_Fail_and_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdeX1DKyVKIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!unzip './bbc-news-summary.zip'\n",
        "#!unrar x 'BBC News Summary.rar'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap8hy6mnrY3v",
        "colab_type": "text"
      },
      "source": [
        "##Attention\n",
        "\n",
        "We use an encoder (usually a bi-dir GRU/LSTM) that encodes a sentence (word-embeddings of a sequence of tokens) and produces a series of hidden states that captures the meaning of a word in a given (potentially, global) context. I prefer to use pre-trained word embeddings rather than start from scratch. The next hidden state is calcualted from the previous state plus the new input embedding. The forward GRU will only know previous states and this is flipped for the backward GRU and by concatenating these two hidden states we are able to create a global contextual representation of a word in a given context. The decoder is the same as the encoder but also takes in a context vector. The attention mechanism selects the salient parts of the source sentence (from encoder) to predict the next word. This is done by comparing the last decoder state with each source hidden state, then this decoder state is put through a softmax to give us a prediction distribution. The loss is usually the cross-entropy loss which compares the correct distribution against the predicted one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNZSY0zSuxBN",
        "colab_type": "text"
      },
      "source": [
        "Since sequences will have variable lengths we need to use these PyTorch helper functions called `pack_padded_sequence` and `pad_packed_sequence`.\n",
        "These functions take care of masking and padding, so that the resulting word representations are simply zeros after a sentence stops.\n",
        "\n",
        "The code below reads in a source sentence (a sequence of word embeddings) and produces the hidden states.\n",
        "It also returns a final vector, a summary of the complete sentence, by concatenating the first and the last hidden states (they have both seen the whole sentence, each in a different direction). We will use the final vector to initialize the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5hrNjFx6G49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8y_HRYS6H6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = torch.nn.GRU(10, 20, 2)\n",
        "input = torch.randn(5, 3, 10)\n",
        "h0 = torch.randn(2, 3, 20)\n",
        "output, hn = rnn(input, h0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2Q8JuBv8bCv",
        "colab_type": "text"
      },
      "source": [
        "The view function is meant to reshape the tensor. Say you have a tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iMCnQfF8OCi",
        "colab_type": "code",
        "outputId": "2b29cbe2-03da-40a4-97f8-c4e8ee9e5080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "a = torch.range(1, 16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y7dJUuy8gov",
        "colab_type": "text"
      },
      "source": [
        "a is a tensor that has 16 elements from 1 to 16(included). If you want to reshape this tensor to make it a 4 x 4 tensor then you can use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpFKyUew8lof",
        "colab_type": "code",
        "outputId": "d172bcc0-0974-4f39-f588-a92f9fbdbd63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "a = a.view(1, 1, -1)\n",
        "a.shape, a#creates a long vector"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 16]),\n",
              " tensor([[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.,\n",
              "           15., 16.]]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dco4zlJ18oIq",
        "colab_type": "text"
      },
      "source": [
        "Now a will be a 4 x 4 tensor. Note that after the reshape the total number of elements need to remain the same. Reshaping the tensor a to a 3 x 5 tensor would not be appropriate. \n",
        "\n",
        "What is the meaning of parameter -1?\n",
        "\n",
        "If there is any situation that you don't know how many rows you want but are sure of the number of columns, then you can specify this with a -1. (Note that you can extend this to tensors with more dimensions. Only one of the axis value can be -1). This is a way of telling the library: \"give me a tensor that has these many columns and you compute the appropriate number of rows that is necessary to make this happen\".\n",
        "\n",
        "This can be seen in the neural network code that you have given above. After the line x = self.pool(F.relu(self.conv2(x))) in the forward function, you will have a 16 depth feature map. You have to flatten this to give it to the fully connected layer. So you tell pytorch to reshape the tensor you obtained to have specific number of columns and tell it to decide the number of rows by itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOTngOWUhjlr",
        "colab_type": "code",
        "outputId": "efdfb41e-c7cf-4aa2-8fbd-253413d9fe2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "g = (training_pairs[0]).view(1, 1, -1)\n",
        "g.shape[2], len(g)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(456, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mKhzOEdivf1",
        "colab_type": "code",
        "outputId": "7a36dffe-9cf9-40d6-f6e9-79e3da9ee8af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(training_pairs[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpv8H0od8AQ7",
        "colab_type": "code",
        "outputId": "4c6da9de-3840-4cde-b856-5dc3ba8f1624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "embedding = nn.Embedding(voc.n_words, hidden_size)\n",
        "#so this reshapes an embedding to 1 by 1 by as many rows\n",
        "embedding"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5029743b30d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#so this reshapes an embedding to 1 by 1 by as many rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'voc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ewnG4JAhRdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc = EncoderRNN(voc.n_words, hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caDUTnJFj99S",
        "colab_type": "code",
        "outputId": "8938494f-f3a8-40ca-a83c-63c647b93a9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "emb = a.view(1, 1, -1)\n",
        "emb.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 456])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBKI4PHsmExL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder1 = EncoderRNN(voc.n_words, hidden_size).to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbe-cDIeqhwK",
        "colab_type": "text"
      },
      "source": [
        " Args:\n",
        "        input_size: The number of expected features in the input `x`\n",
        "        hidden_size: The number of features in the hidden state `h`\n",
        "        num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n",
        "            would mean stacking two RNNs together to form a `stacked RNN`,\n",
        "            with the second RNN taking in outputs of the first RNN and\n",
        "            computing the final results. Default: 1\n",
        "        nonlinearity: The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\n",
        "        bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n",
        "            Default: ``True``\n",
        "        batch_first: If ``True``, then the input and output tensors are provided\n",
        "            as `(batch, seq, feature)`. Default: ``False``\n",
        "        dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n",
        "            RNN layer except the last layer, with dropout probability equal to\n",
        "            :attr:`dropout`. Default: 0\n",
        "        bidirectional: If ``True``, becomes a bidirectional RNN. Default: ``False``\n",
        "\n",
        "    Inputs: input, h_0\n",
        "        - **input** of shape `(seq_len, batch, input_size)`: tensor containing the features\n",
        "          of the input sequence. The input can also be a packed variable length\n",
        "          sequence. See :func:`torch.nn.utils.rnn.pack_padded_sequence`\n",
        "          or :func:`torch.nn.utils.rnn.pack_sequence`\n",
        "          for details.\n",
        "        - **h_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
        "          containing the initial hidden state for each element in the batch.\n",
        "          Defaults to zero if not provided. If the RNN is bidirectional,\n",
        "          num_directions should be 2, else it should be 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu2NRYHjoHnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_output, encoder_hidden = encoder1(emb, enc_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLAvhfbWpnlm",
        "colab_type": "code",
        "outputId": "fb8c0141-393c-41d9-f018-04c356387ba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "emb.size(-1), enc_in.size(-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(456, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH-FuU_CqlEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = torch.nn.GRU(456, 1, 256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1SFalrprEmH",
        "colab_type": "code",
        "outputId": "caf935af-3c54-4750-d702-4be829d53d05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "nn.Embedding()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 456])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5Z_Ia0KrIKD",
        "colab_type": "code",
        "outputId": "9138184a-8a0d-4dfd-8b11-f956899dbe84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "enc_in.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl5FSQR6qwF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output, hn = rnn(emb, enc_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTQqIr3WnRQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = torch.nn.GRU(10, 20, 2)\n",
        "input = torch.randn(5, 3, 10)\n",
        "h0 = torch.randn(2, 3, 20)\n",
        "output, hn = rnn(input, h0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7-wBtu5oxuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e4kf9fcnwhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input of shape (seq_len, batch, input_size): \n",
        "#tensor containing the features of the input sequence.\n",
        "rnn = torch.nn.GRU(1, 1, 456)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E1llSSkpakN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out, hid = rnn()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACcW6SxsoeNN",
        "colab_type": "code",
        "outputId": "682eaeb1-785c-4933-e12b-67837df5ffa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "a.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoRFpUHNjmV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc_in = encoder1.initHidden()\n",
        "\n",
        "enc_hid, encout = encoder1(256, enc_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIcdtM0e9ikB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ThisEncoderRNN(nn.Module):\n",
        "  \n",
        "  def __init__(self, embedded, embedding_dim, vocab_size, hidden_size):\n",
        "    super(ThisEncoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size #Weight matrix of hidden is [hidden_size*hidden_size]\n",
        "    \n",
        "    self.embedding = nn.Embedding(embedding_dim, vocab_size)#Embedding_size, hidden_size\n",
        "    self.gru = nn.GRU(vocab_size, hidden_size)\n",
        "     #torch.nn.GRU(voc.n_words, hidden_size)\n",
        "    \n",
        "  def forward(self, input, hidden):\n",
        "    #embedded = self.embedding(input).view(1, 1, -1)\n",
        "    output = embedded\n",
        "    output, hidden  = self.gru(output, hidden)#Feed GRU embedding, initial hidden state\n",
        "    return output, hidden \n",
        "  \n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size)#, device=DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUMVLURs0qt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc1 = ThisEncoderRNN(embedded, embedding_dim, voc.n_words, 256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1Bn7yuB1DO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inut: shape (seq_len, batch, input_size): tensor\n",
        "# h_0: shape (num_layers * num_directions, batch, hidden_size): tensor \n",
        "out, hid = enc1(s, in_hid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRE5LtdEl4fc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#embedded: (src seq len, batch size, embed size)\n",
        "#num_embeddings (int): size of the dictionary of embeddings\n",
        "#embedding_dim (int): the size of each embedding vector\n",
        "#padding_idx (int, optional): If given, pads the output with the embedding \n",
        "#vector at :attr:`padding_idx`\n",
        "embedding_dim = 50\n",
        "vocab_size = voc.n_words\n",
        "embs = nn.Embedding(embedding_dim, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76A4g0FN6K8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = embs(src)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbOiCtuM6dwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "lengths = len(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU6A3cuA7ItG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "T, B, C = 100, 10, 4  # longest sequence, batch_size, feature_size\n",
        "input = torch.randn(T, B, C, device='cuda')\n",
        "lengths, _ = torch.sort(torch.randint(1, 256, (1,)), descending=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owMhlEEd6PzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = pack_padded_sequence(s, lengths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cMyrZBo7XaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZljKG1TovtR9",
        "colab_type": "code",
        "outputId": "957f8900-3b73-4720-b51e-0c6d013e17ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# we will use CUDA if it is available\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE=torch.device('cuda:0') # or set to 'cpu'\n",
        "print(\"CUDA:\", USE_CUDA)\n",
        "print(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA: True\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlbRKL6Ex92V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = src.view(1, 1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5iUWURQvVZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedded = embs(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHnekvYhVsYb",
        "colab_type": "code",
        "outputId": "13bc25b8-078e-42bf-b668-2d33265a3ec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "embedded.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 6, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M927nzbP1luw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_hid = torch.zeros(1, 1, 256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHDvJd632AkJ",
        "colab_type": "code",
        "outputId": "afe1c632-91e3-4aab-f003-df4fe5818a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "src.shape, embedded.shape, in_hid.shape, s.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([6, 1]),\n",
              " torch.Size([1, 1, 6, 256]),\n",
              " torch.Size([1, 1, 256]),\n",
              " torch.Size([1, 1, 6]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izGZdYEa3GFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "packed = pack_padded_sequence(src, [6], batch_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5djPqG5ux83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = torch.nn.GRU(voc.n_words, hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK07nPQu8TCV",
        "colab_type": "code",
        "outputId": "c9bb5ec4-8b43-406c-ed25-09a4cfdd5fee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "s.shape, in_hid.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([6, 1, 8]), torch.Size([1, 1, 256]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4bEkFpW05C5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out, hid = rnn(s, in_hid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_hwwH4E-XME",
        "colab_type": "code",
        "outputId": "44cfe956-2c53-4089-bde6-c85781a7144b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "s.dtype, in_hid.dtype"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.float32, torch.float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLSs6BNf7wxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out, hid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfYQNu3gtmvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "some_src = ['This is some source text']\n",
        "some_tgt = ['This is some target text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzsZ2QiVuHa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src = \" \".join(some_src)\n",
        "tgt = \" \".join(some_tgt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioVaCqEquiIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voc = Vocab()\n",
        "voc.addSentence(src)\n",
        "voc.addSentence(tgt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjpFhA_6ttDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#voc.addSentence(some_src), voc.addSentence(some_tgt)\n",
        "src, tgt = tensorFromSentence(voc, src), tensorFromSentence(voc, tgt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDcGXdwme1v7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc1 = EncoderRNN(a.size(0), hidden_size).to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX_WOpyjlbbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc_out, enc_hid = enc1.forward(a, enc.initHidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXARiJhLLjWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "  \n",
        "  def __init__(self, hidden_size, output_size):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "    \n",
        "  def forward(self, input, hidden):\n",
        "    output = self.embedding(input).view(1, 1, -1)\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    output = self.softmax(self.out(output[0]))\n",
        "    return output, hidden\n",
        "  \n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size, device=DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Koz63YEhQ3Qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size, dropout=0.1, max_length=MAX_LENGTH):\n",
        "    super(AttnDecoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.dropout = dropout\n",
        "    self.max_length = max_length\n",
        "    \n",
        "    self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "    self.attn = nn.Linear(self.hidden_size *2, self.max_length)#*2 since its bidir\n",
        "    self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "    self.dropout = nn.Dropout(self.dropout)\n",
        "    self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "    self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "    \n",
        "  def forward(self, input, hidden, encoder_outputs):\n",
        "    embedded = self.embedding(input).view(1, 1, -1)\n",
        "    embedded = self.dropout(embedded)\n",
        "    \n",
        "    attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "    # Unsqueeze:Returns a new tensor with a dimension of size one \n",
        "    # inserted at the specified position.\n",
        "    attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                            encoder_outputs.unsqueeze(0))\n",
        "    output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "    output = self.attn_combine(output).unsqueeze(0)\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "    return output, hidden, attn_weights\n",
        "  \n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size, device=DEVICE)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5PJlKzGUTf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('./BBC News Summary/News Articles/politics/001.txt') as f:\n",
        "  article = f.readlines()\n",
        "with open('./BBC News Summary/Summaries/politics/001.txt') as f:\n",
        "  summary = f.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJBBo8B9WGup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article = \" \".join(article)\n",
        "article = article.replace('\\n', '')\n",
        "summary = \" \".join(summary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_FZg91jTizg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Vocab:\n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "    self.n_words = 2#Count the sos and eos tokens\n",
        "  \n",
        "  def addSentence(self, sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "    \n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1\n",
        "\n",
        "def indexesFromSentence(Vocab, sentence):\n",
        "    return [Vocab.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(Vocab, sentence):\n",
        "    indexes = indexesFromSentence(Vocab, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIZLvzGIV6fX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voc = Vocab()\n",
        "voc.addSentence(article)\n",
        "voc.addSentence(summary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlRS0ocSi_cH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a, s = tensorFromSentence(voc, article), tensorFromSentence(voc, summary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjVf3PoDjE65",
        "colab_type": "code",
        "outputId": "17c41c49-0ef5-41f7-add2-df17a03f687b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(s.unique()), len(a.unique()), voc.n_words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(120, 235, 243)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCPJ_FovgRtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc = EncoderRNN(voc.n_words, hidden_size).to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7lNlZr5gykw",
        "colab_type": "code",
        "outputId": "efa1f775-2c66-4550-87b7-245f309846fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "g.size(-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0602J_IJg8AH",
        "colab_type": "code",
        "outputId": "726e7536-244e-43ab-d1b0-6139d48c14f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "f = nn.Embedding(input_size, hidden_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([456, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTnteN2ZXeVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing = 0.5\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optim, decoder_optim,\n",
        "         criterion, max_length=MAX_LENGTH):\n",
        "  encoder_hidden = encoder.initHidden()\n",
        "  encoder_optim.zero_grad()\n",
        "  decoder_optim.zero_grad()\n",
        "  \n",
        "  input_length = input_tensor.size(0)\n",
        "  target_length = target_tensor.size(0)\n",
        "  \n",
        "  encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=DEVICE)\n",
        "  \n",
        "  loss = 0\n",
        "  #accumulate encoder outputs \n",
        "  for i in range(input_length):\n",
        "    encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
        "    encoder_outputs[i] = encoder_output[0, 0]\n",
        "    \n",
        "  decoder_input = torch.tensor([[SOS_token]], device=DEVICE)\n",
        "  \n",
        "  decoder_hidden = encoder_hidden\n",
        "  \n",
        "  use_teacher_forcing = True if random.random() < teacher_forcing else False\n",
        "  \n",
        "  if use_teacher_forcing:\n",
        "    #Feed target embedding as next input\n",
        "    for i in range(target_length):\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input,\n",
        "                                                                 decoder_hidden,\n",
        "                                                                 encoder_outputs)\n",
        "      loss += criterion(decoder_output, target_tensor[i])\n",
        "      decoder_input = target_tensor[i]\n",
        "  else:\n",
        "    #Use predictions as input\n",
        "    for i in range(target_length):\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input,\n",
        "                                                                 decoder_hidden,\n",
        "                                                                 encoder_outputs)\n",
        "      #like np.argmax, index of highest probability\n",
        "      topv, topi = decoder_output.topk(1)\n",
        "      decoder_input = topi.squeeze().detach() #detach from history as input\n",
        "      if decoder_input.item() == EOS_token:\n",
        "        break\n",
        "  \n",
        "  loss.backward()\n",
        "  \n",
        "  encoder_optim.step()\n",
        "  decoder_optim.step()\n",
        "  \n",
        "  return loss.item() / target_length#average the losses over all time steps/ preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quGHsXEtdydf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepareData(src, tgt, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJwtA06DeOBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor = tensorFromSentence(voc, article)\n",
        "input_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gq3vQU5Z7rC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, lr=0.01):\n",
        "  plot_losses = []\n",
        "  print_loss_total = 0\n",
        "  plot_loss_total = 0\n",
        "  \n",
        "  encoder_optimizer = optim.SGD(encoder.parameters(), lr=lr)\n",
        "  decoder_optimizer = optim.SGD(decoder.parameters(), lr=lr)\n",
        "  training_pairs = [tensorFromSentence(voc, article), tensorFromSentence(voc, summary)]\n",
        "  criterion = nn.NLLLoss()\n",
        "  \n",
        "  for iter in range(1):\n",
        "    #training_pair = training_pairs#[iter-1]\n",
        "    input_tensor = tensorFromSentence(voc, article)\n",
        "    target_tensor = tensorFromSentence(voc, summary)\n",
        "    \n",
        "    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer,\n",
        "                decoder_optimizer, criterion)\n",
        "    \n",
        "    print_loss_total += loss\n",
        "    plot_loss_total += loss\n",
        "    if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH46CjpobKMq",
        "colab_type": "text"
      },
      "source": [
        "Evaluation is the same as training but without targets so we simply feed the decoders preds back to itself for each time step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-rxp2btbRUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "  with torch.no_grad():\n",
        "    input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "    input_length = input_tensor.size()[0]\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    \n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=DEVICE)\n",
        "    \n",
        "    for i in range(input_length):\n",
        "      encoder_output, encoder_hidden = encoder(input_tensor[i],\n",
        "                                              encoder_hidden)\n",
        "      encoder_outputs[i] += encoder_output[0, 10]\n",
        "    \n",
        "    decoder_input = torch.tensor([[SOS_token]], device=DEVICE)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoded_words = []\n",
        "    decoder_attentions = torch.zeros(max_length, max_length)\n",
        "    \n",
        "    for di in range(max_length):\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input,\n",
        "                                                                 decoder_hidden,\n",
        "                                                                 encoder_outputs)\n",
        "      decoder_attentions[di] = decoder_attention.data\n",
        "      topv, topi = decoder_output.data.topk(1)\n",
        "      if topi.item() == EOS_token:\n",
        "        break\n",
        "      else:\n",
        "        decoded_words.append(voc.index2word[topi.item()])\n",
        "        \n",
        "      decoder_input = topi.squeeze().detach()\n",
        "    return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXDMVzJIc8bo",
        "colab_type": "code",
        "outputId": "f37feeff-91cc-4248-bc53-94169c8f78f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        }
      },
      "source": [
        "from torch import optim\n",
        "import random\n",
        "\n",
        "\n",
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(voc.n_words, hidden_size).to(DEVICE)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, voc.n_words, dropout=0.1).to(DEVICE)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 5, 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-189f38882144>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-134-4c1953af5f3c>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, lr)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer,\n\u001b[0;32m---> 17\u001b[0;31m                 decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-f397cd6213a3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optim, decoder_optim, criterion, max_length)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSOS_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for dimension 0 with size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s37lxaWPe43v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOqywF7krVfr",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Encoder { form-width: \"25px\" }\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n",
        "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        \n",
        "    def forward(self, x, mask, lengths):\n",
        "        \"\"\"\n",
        "        Applies a bidirectional GRU to sequence of embeddings x.\n",
        "        The input mini-batch x needs to be sorted by length.\n",
        "        x should have dimensions [batch, time, dim].\n",
        "        \"\"\"\n",
        "        #https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence\n",
        "        #takes an input (Tensor) – padded batch of variable length sequences.\n",
        "        #lengths (Tensor) – list of sequences lengths of each batch element.\n",
        "        #batch_first (bool, optional) – if True, the input is expected in B x T x * format.\n",
        "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
        "        output, final = self.rnn(packed)#output from GRU is: output-tensor containing output features from last layer of GRU for each timestep,\n",
        "        #and a tensor containing the hidden state for t = seq_len\n",
        "        #https://pytorch.org/docs/stable/nn.html#pad-packed-sequence\n",
        "        #Pads a packed batch of variable length sequences.\n",
        "        #It is an inverse operation to pack_padded_sequence().\n",
        "        #takes as input: sequence (PackedSequence) – batch to pad\n",
        "        #batch_first (bool, optional) – if True, the output will be in B x T x * format.\n",
        "        #returns: a Tuple of Tensor containing the padded sequence, \n",
        "        #and a Tensor containing the list of lengths of each sequence in the batch.\n",
        "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
        "\n",
        "        # we need to manually concatenate the final states for both directions\n",
        "        fwd_final = final[0:final.size(0):2]\n",
        "        bwd_final = final[1:final.size(0):2]\n",
        "        final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n",
        "\n",
        "        return output, final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsY3zae-6q_5",
        "colab_type": "text"
      },
      "source": [
        "The decoder is a conditional GRU which means that rather than starting from an empty state, its initial hidden state results from a projection of the encoder final vector. We use teacher forcing during trainng which is the trg_embed. We feed the correct previous target word embedding straight to the GRU at each time step. The `forward` returns all decoder hidden states and pre-output vectors used to compure the loss. After predicting a word from the pre-output vector we call it again but supplying the word embedding of the previously predicted word and the last state.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I92sgOuTAxB7",
        "colab_type": "text"
      },
      "source": [
        "You can use pretrained embeddings or learn them or fine tune them..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyF3rman8gP1",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Show Attention Map\n",
        "def show_attention_map(src_words, pred_words, attention, pointer_ratio=None):\n",
        "  fig, ax = plt.subplots(figsize=(16, 4))\n",
        "  im = plt.pcolormesh(np.flipud(attention), cmap=\"GnBu\")\n",
        "  # set ticks and labels\n",
        "  ax.set_xticks(np.arange(len(src_words)) + 0.5)\n",
        "  ax.set_xticklabels(src_words, fontsize=14)\n",
        "  ax.set_yticks(np.arange(len(pred_words)) + 0.5)\n",
        "  ax.set_yticklabels(reversed(pred_words), fontsize=14)\n",
        "  if pointer_ratio is not None:\n",
        "    ax1 = ax.twinx()\n",
        "    ax1.set_yticks(np.concatenate([np.arange(0.5, len(pred_words)), [len(pred_words)]]))\n",
        "    ax1.set_yticklabels('%.3f' % v for v in np.flipud(pointer_ratio))\n",
        "    ax1.set_ylabel('Copy probability', rotation=-90, va=\"bottom\")\n",
        "  # let the horizontal axes labelling appear on top\n",
        "  ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
        "  # rotate the tick labels and set their alignment\n",
        "  plt.setp(ax.get_xticklabels(), rotation=-45, ha=\"right\", rotation_mode=\"anchor\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBINCgsd7cdJ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Decoder { form-width: \"25px\" }\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
        "    \n",
        "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
        "                 bridge=True):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.attention = attention\n",
        "        self.dropout = dropout\n",
        "                 \n",
        "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n",
        "                          batch_first=True, dropout=dropout)\n",
        "                 \n",
        "        # to initialize from the final encoder state\n",
        "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
        "\n",
        "        self.dropout_layer = nn.Dropout(p=dropout)\n",
        "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n",
        "                                          hidden_size, bias=False)\n",
        "        \n",
        "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
        "        \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
        "\n",
        "        # compute context vector using attention mechanism\n",
        "        #somelist[-1] get the last element of a list\n",
        "        #nn.Unsqueeze is useful and lets us insert the dimension without \n",
        "        #explicitly being aware of the other dimensions when writing the code.\n",
        "        query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n",
        "        context, attn_probs = self.attention(\n",
        "            query=query, proj_key=proj_key,\n",
        "            value=encoder_hidden, mask=src_mask)\n",
        "\n",
        "        # update rnn hidden state\n",
        "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "        #Concatenates the given sequence of seq tensors in the given dimension.\n",
        "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
        "        pre_output = self.dropout_layer(pre_output)\n",
        "        #\n",
        "        pre_output = self.pre_output_layer(pre_output)\n",
        "\n",
        "        return output, hidden, pre_output\n",
        "    \n",
        "    def forward(self, trg_embed, encoder_hidden, encoder_final, \n",
        "                src_mask, trg_mask, hidden=None, max_len=None):\n",
        "        \"\"\"Unroll the decoder one step at a time.\"\"\"\n",
        "                                         \n",
        "        # the maximum number of steps to unroll the RNN\n",
        "        if max_len is None:\n",
        "            max_len = trg_mask.size(-1)\n",
        "\n",
        "        # initialize decoder hidden state with final encoder hidden states\n",
        "        if hidden is None:\n",
        "            hidden = self.init_hidden(encoder_final)\n",
        "        \n",
        "        # pre-compute projected encoder hidden states\n",
        "        # (the \"keys\" for the attention mechanism)\n",
        "        # this is only done for efficiency\n",
        "        proj_key = self.attention.key_layer(encoder_hidden)\n",
        "        \n",
        "        # here we store all intermediate hidden states and pre-output vectors\n",
        "        decoder_states = []\n",
        "        pre_output_vectors = []\n",
        "        \n",
        "        # unroll the decoder RNN for max_len steps\n",
        "        for i in range(max_len):\n",
        "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
        "            output, hidden, pre_output = self.forward_step(\n",
        "              prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n",
        "            decoder_states.append(output)\n",
        "            pre_output_vectors.append(pre_output)\n",
        "\n",
        "        decoder_states = torch.cat(decoder_states, dim=1)\n",
        "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
        "        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n",
        "\n",
        "    def init_hidden(self, encoder_final):\n",
        "        \"\"\"Returns the initial decoder state,\n",
        "        conditioned on the final encoder state.\"\"\"\n",
        "\n",
        "        if encoder_final is None:\n",
        "            return None  # start with zeros\n",
        "\n",
        "        return torch.tanh(self.bridge(encoder_final))            \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZNYfBQr-aDy",
        "colab_type": "text"
      },
      "source": [
        "At each time step, the decoder has access to all source word representations. The attention mechanism allows the model to focus on the currently most relevant part of the source sentence. The state of the decoder is given by the GRU hidden state si. So if we want to know which source words hj are most relevant we need something with two inputs. We use a multilayer perceptron with tanh to both the current decoder state si (the query) and each encoder state hj(the key), and then project it into a single value (scalar) to get the attention energy eij. Once all energies are computed, we normalize them with softmax. Then the context vector for time step i is the weighted sum of the encoder hidden states (the values).\n",
        "\n",
        "$$\\mathbf{c}_i = \\sum_j \\alpha_{ij} \\mathbf{h}_j$$\n",
        "\n",
        "So you've essentially put a weight on each encoder hidden state (hj, the words) that we want to pay attention to. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KAkbmRSA9Hj",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "class BahdanauAttention(nn.Module):\n",
        "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        \n",
        "        # We assume a bi-directional encoder so key_size is 2*hidden_size\n",
        "        key_size = 2 * hidden_size if key_size is None else key_size\n",
        "        query_size = hidden_size if query_size is None else query_size\n",
        "\n",
        "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
        "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
        "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
        "        \n",
        "        # to store attention scores\n",
        "        self.alphas = None\n",
        "        \n",
        "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
        "        assert mask is not None, \"mask is required\"\n",
        "\n",
        "        # We first project the query (the decoder state).\n",
        "        # the query can be seen as asking what do we want to pay attention to?\n",
        "        # The projected keys (the encoder states) were already pre-computated.\n",
        "        query = self.query_layer(query)\n",
        "        \n",
        "        # Calculate energy scores.\n",
        "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "        \n",
        "        # Mask out invalid positions.\n",
        "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
        "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
        "        \n",
        "        # Turn scores to probabilities.\n",
        "        alphas = F.softmax(scores, dim=-1)\n",
        "        self.alphas = alphas        \n",
        "        \n",
        "        # The context vector is the weighted sum of the values.\n",
        "        #a batch matrix-matrix product of matrices stored in batch1 and batch2.\n",
        "        context = torch.bmm(alphas, value)\n",
        "        \n",
        "        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n",
        "        return context, alphas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ledtG3AsWA2y",
        "colab_type": "code",
        "outputId": "e9290e97-051d-47cb-9411-bd54b1b85e7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "embedded.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5fa56864cd3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'embedded' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtjIW79CD1hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyEncoderRNN(nn.Module):\n",
        "  \n",
        "  def __init__(self, embed_size, hidden_size, dropout: float=0.5):\n",
        "    super(MyEncoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embed_size = embed_size\n",
        "    self.rnn = nn.GRU(embed_size, hidden_size, bidirectional=2, dropout=dropout)\n",
        "    \n",
        "  def forward(self, embedding, hidden, inp_length=None):\n",
        "    \"\"\"\n",
        "    Applies bidir GRU to sequqnece of embeddings.\n",
        "    :param embedding: needs to be [src seq length x batchsize x embedding size]\n",
        "    :param hidden: needs to be [number of directions(2) x batch size x encoder hidden size]\n",
        "    \"\"\"\n",
        "    embedding = pack_padded_sequence(embedding, torch.LongTensor(1))\n",
        "    #https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence\n",
        "    #takes an input (Tensor) – padded batch of variable length sequences.\n",
        "    #lengths (Tensor) – list of sequences lengths of each batch element.\n",
        "    #batch_first (bool, optional) – if True, the input is expected in B x T x * format.\n",
        "        \n",
        "    output, hidden = self.rnn(embedding, [1])\n",
        "    #output from GRU is: output-tensor containing output features from last \n",
        "    #layer of GRU for each timestep,\n",
        "    #and a tensor containing the hidden state for t = seq_len\n",
        "    \n",
        "    #https://pytorch.org/docs/stable/nn.html#pad-packed-sequence\n",
        "    #Pads a packed batch of variable length sequences.\n",
        "    #It is an inverse operation to pack_padded_sequence().\n",
        "    #where T is the length of the longest sequence and B is the batch size. \n",
        "    #If batch_first is True, the data will be transposed into B x T x * format.\n",
        "    #takes as input: sequence (PackedSequence) – batch to pad\n",
        "    #batch_first (bool, optional) – if True, the output will be in B x T x * format.\n",
        "    #returns: a Tuple of Tensor containing the padded sequence, \n",
        "    #and a Tensor containing the list of lengths of each sequence in the batch.\n",
        "    output, _ = pad_packed_sequence(output)\n",
        "    \n",
        "    return output, hidden\n",
        "  \n",
        "  def initHidden(self):\n",
        "    #bidirection x batch size x hidden size \n",
        "    return torch.zeros(2, batch_size, self.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anjq5wcFVUzB",
        "colab_type": "code",
        "outputId": "714b4de7-c31b-4504-efcd-563e70a85ef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "my_enc = MyEncoderRNN(50, 256)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ype1QQdWVocc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out, hid = my_enc(embedded, my_enc.initHidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjFKqgunMfyP",
        "colab_type": "text"
      },
      "source": [
        "The decoder is simply another RNN that takes encoder output vector and outputs a sequence of words. In its simplest form it takes the context vector (last encoder output) as the initial hidden state of the decoder. At every time step, the decoder is given the embedding of a token and a hidden state (that'd be the last encoder output if its the first time step). NB: usually its a \"sos\" token. \n",
        "\n",
        "There is a problem with transduction bottlenecks when we try to encode the entire sentence with a single vector output from the encoder. The attention mechanism can overcome this by allowing us to focus on different parts of the encoder output for each decoding time step. We do so by calculating the attention weights. Then multiply these with the encoder output vectors to create a weighted sum `attn_applied`, that contains the information about that specific part of the input sequence. \n",
        "\n",
        "![alt text](https://i.imgur.com/1152PYf.png)\n",
        "\n",
        "As mentioned, we do this via a feed-forward layer `attn` (decoder inputs and hidden state as inputs) but since sequences are of variable length we need a max sentence length. \n",
        "\n",
        "![alt text](https://pytorch.org/tutorials/_images/attention-decoder-network.png)\n",
        "\n",
        "The encoder reads an input sequence one item at a time, and outputs a vector at each step. The final output of the encoder is kept as the context vector. The decoder uses this context vector to produce a sequence of outputs one step at a time. The fixed-length vector carries the burden of encoding the the entire \"meaning\" of the input sequence, no matter how long that may be. The attention mechanism introduced by Bahdanau et al. addresses this by giving the decoder a way to \"pay attention\" to parts of the input, rather than relying on a single vector. For every step the decoder can select a different part of the input sentence to consider. Attention is calculated with another feedforward layer in the decoder. This layer will use the current input and hidden state to create a new vector, which is the same size as the input sequence (in practice, a fixed maximum length). This vector is processed through softmax to create attention weights, which are multiplied by the encoders' outputs to create a new context vector, which is then used to predict the next output.\n",
        "\n",
        "Attention assigns context elements attention weights which define a weighted sum over context representations. This is known as content-based attention where attention weights are computed by comparing the current time-step to all elements in the context. Computing unlimited context size with attetnion is quadratic in complexity in the input length. \n",
        "Lightweight convolutions may overcome this which increase linearly with input size. Weights are shared across the channel dimension. This reduces the amount of weights compared to standard non-separable convolutions. Lightweight convs reuse the same weights for context elements regardless of timestep. \n",
        "\n",
        "Dynamic convs build on this by predicting different conv kernel at each time step. It is a function of only the current time step.\n",
        "\n",
        "self attention of Vaswani et al. 2017 applies three projection to the input to obtain a key (K), value (V) representations, where n is the number of time steps, d the input/output dimension. H is the number of heads where each head can learn separate attention weights over dk features and attend to different positions. It compures dot-products between key/query pairs, scales to stabilize in training and softmax normalizes the result. Finally, computes a weighted sum using the output of the value projection (V). \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOjWwdSAScRw",
        "colab_type": "text"
      },
      "source": [
        "BERT, instead of reading a text left to right or right to left bidirectionally, it reads the entire sequence at once, allowing the model to capture the context of a word. You feed the transformer a sequence of token embeddings and it outputs a sequence of vectors of size H (each vector corresponds to an input token with the same index). It is used to train a language model whereby you try predict the next word (conditional probability) given a previous word or words (and you do the product over this). But BERT masks tokens before training and this forces bert to predict the original value of the masked word based on non-masked words. This means there is a classification layer ontop of the encoder output. Then multiplying the output vectors by the embedding matrix, transforming them into the fixed vocabulary dimensions. Then calculating the probability of each word in the vocab with softmax. BERT's loss only takes the masked word predictions and is slower to converge but adds more contextual awareness. \n",
        "\n",
        "In training Bert, the model has pairs of sentences as input and tries to predict if the second sentence in the pair is the subsequent sentence in the original. We split inputs 50/50 as pairs that are and are not subsequent sentences. The input is processed with a [CLS] token at the beginning of the first sentence and a [SEP] token at the end of each sentence. A sentence embedding to indicate if it is sentence A or B to each token. A positional embedding is added to each token to indicate its position in the sequence. To predict: the entire input sequence goes through the transformer, the output of the [CLS] token is transformed into a 2by1 vector, using a classification layer. Then calculating the probability of is it the next sentence with softmax. The Masked LM and Next Sentence Prediction are trained together with the goal of minimizing the combined loss function of the two. \n",
        "\n",
        "BERT can be used for a wide variety of language tasks and finetuned: Classification by simply adding a classfication layer on top of the transformer output for the [CLS] token. \n",
        "BERT is huge with 345 million parameters, taking 16 TPUs 4 days. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yybuIco7LUeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyAttentionDecoderRNN(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(MyAttentionDecoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_emb_size = embed_size\n",
        "    self.dropout_perc = dropout_perc\n",
        "    \n",
        "    self.attn = nn.Linear(self.hidden_size * 2, self.max_length)#*2 for bidir\n",
        "    self.attn_comb = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "    \n",
        "    self.dropout = nn.Dropout(self.dropout_perc)\n",
        "    self.gru = nn.GRU(embed_size, hidden_size, dropout=dropout)\n",
        "    self.max_length = max_length\n",
        "    self.out = nn.Linear()\n",
        "    # provide coverage as input when computing enc attn?\n",
        "    self.enc_attn_cover = enc_attn_cover\n",
        "    # pointer net\n",
        "    self.pointer = pointer\n",
        "    \n",
        "    #using glove embedding so its vocab size by embedding dim\n",
        "    \n",
        "  def forward(self, embedding, input, encoder_output_vector):\n",
        "    # Get the embedding of the current input word (last output word)\n",
        "    embedded = self.embedding(input).view(1, 1, -1)# S=1 x B x N\n",
        "    embedded = self.dropout(embedded)\n",
        "    \n",
        "    attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "    attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                            encoder_outputs.unsqueeze(0))\n",
        "    \n",
        "    \n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp0CgjGIuAHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embed_size, hidden_size, *, enc_attn=True, dec_attn=True,\n",
        "               enc_attn_cover=True, pointer=True, tied_embedding=None, out_embed_size=None,\n",
        "               in_drop: float=0, rnn_drop: float=0, out_drop: float=0, enc_hidden_size=None):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.combined_size = self.hidden_size\n",
        "    self.enc_attn = enc_attn\n",
        "    self.dec_attn = dec_attn\n",
        "    self.enc_attn_cover = enc_attn_cover\n",
        "    self.pointer = pointer\n",
        "    self.out_embed_size = out_embed_size\n",
        "    if tied_embedding is not None and self.out_embed_size and embed_size != self.out_embed_size:\n",
        "      print(\"Warning: Output embedding size %d is overriden by its tied embedding size %d.\"\n",
        "            % (self.out_embed_size, embed_size))\n",
        "      self.out_embed_size = embed_size\n",
        "\n",
        "    self.in_drop = nn.Dropout(in_drop) if in_drop > 0 else None\n",
        "    self.gru = nn.GRU(embed_size, self.hidden_size, dropout=rnn_drop)\n",
        "\n",
        "    if enc_attn:\n",
        "      if not enc_hidden_size: enc_hidden_size = self.hidden_size\n",
        "      self.enc_bilinear = nn.Bilinear(self.hidden_size, enc_hidden_size, 1)\n",
        "      self.combined_size += enc_hidden_size\n",
        "      if enc_attn_cover:\n",
        "        self.cover_weight = nn.Parameter(torch.rand(1))\n",
        "\n",
        "    if dec_attn:\n",
        "      self.dec_bilinear = nn.Bilinear(self.hidden_size, self.hidden_size, 1)\n",
        "      self.combined_size += self.hidden_size\n",
        "\n",
        "    self.out_drop = nn.Dropout(out_drop) if out_drop > 0 else None\n",
        "    if pointer:\n",
        "      self.ptr = nn.Linear(self.combined_size, 1)\n",
        "\n",
        "    if tied_embedding is not None and embed_size != self.combined_size:\n",
        "      # use pre_out layer if combined size is different from embedding size\n",
        "      self.out_embed_size = embed_size\n",
        "\n",
        "    if self.out_embed_size:  # use pre_out layer\n",
        "      self.pre_out = nn.Linear(self.combined_size, self.out_embed_size)\n",
        "      size_before_output = self.out_embed_size\n",
        "    else:  # don't use pre_out layer\n",
        "      size_before_output = self.combined_size\n",
        "\n",
        "    self.out = nn.Linear(size_before_output, vocab_size)\n",
        "    if tied_embedding is not None:\n",
        "      self.out.weight = tied_embedding.weight\n",
        "\n",
        "  def forward(self, embedded, hidden, encoder_states=None, decoder_states=None, coverage_vector=None, *,\n",
        "              encoder_word_idx=None, ext_vocab_size: int=None, log_prob: bool=True):\n",
        "    \"\"\"\n",
        "    :param embedded: (batch size, embed size)\n",
        "    :param hidden: (1, batch size, decoder hidden size)\n",
        "    :param encoder_states: (src seq len, batch size, hidden size), for attention mechanism\n",
        "    :param decoder_states: (past dec steps, batch size, hidden size), for attention mechanism\n",
        "    :param encoder_word_idx: (src seq len, batch size), for pointer network\n",
        "    :param ext_vocab_size: the dynamic vocab size, determined by the max num of OOV words contained\n",
        "                           in any src seq in this batch, for pointer network\n",
        "    :param log_prob: return log probability instead of probability\n",
        "    :return: tuple of four things:\n",
        "             1. word prob or log word prob, (batch size, dynamic vocab size);\n",
        "             2. RNN hidden state after this step, (1, batch size, decoder hidden size);\n",
        "             3. attention weights over encoder states, (batch size, src seq len);\n",
        "             4. prob of copying by pointing as opposed to generating, (batch size, 1)\n",
        "    Perform single-step decoding.\n",
        "    \"\"\"\n",
        "    batch_size = embedded.size(0)\n",
        "    combined = torch.zeros(batch_size, self.combined_size, device=DEVICE)\n",
        "\n",
        "    if self.in_drop: embedded = self.in_drop(embedded)\n",
        "\n",
        "    output, hidden = self.gru(embedded.unsqueeze(0), hidden)  \n",
        "    # unsqueeze and squeeze are necessary\n",
        "    combined[:, :self.hidden_size] = output.squeeze(0)        \n",
        "    # as RNN expects a 3D tensor (step=1)\n",
        "    offset = self.hidden_size\n",
        "    enc_attn, prob_ptr = None, None  # for visualization\n",
        "\n",
        "    if self.enc_attn or self.pointer:\n",
        "      # energy and attention: (num encoder states, batch size, 1)\n",
        "      num_enc_steps = encoder_states.size(0)\n",
        "      enc_total_size = encoder_states.size(2)\n",
        "      enc_energy = self.enc_bilinear(hidden.expand(num_enc_steps, batch_size, -1).contiguous(),\n",
        "                                     encoder_states)\n",
        "      if self.enc_attn_cover and coverage_vector is not None:\n",
        "        enc_energy += self.cover_weight * torch.log(coverage_vector.transpose(0, 1).unsqueeze(2) + eps)\n",
        "      # transpose => (batch size, num encoder states, 1)\n",
        "      enc_attn = F.softmax(enc_energy, dim=0).transpose(0, 1)\n",
        "      if self.enc_attn:\n",
        "        # context: (batch size, encoder hidden size, 1)\n",
        "        enc_context = torch.bmm(encoder_states.permute(1, 2, 0), enc_attn)\n",
        "        combined[:, offset:offset+enc_total_size] = enc_context.squeeze(2)\n",
        "        offset += enc_total_size\n",
        "      enc_attn = enc_attn.squeeze(2)\n",
        "\n",
        "    if self.dec_attn:\n",
        "      if decoder_states is not None and len(decoder_states) > 0:\n",
        "        dec_energy = self.dec_bilinear(hidden.expand_as(decoder_states).contiguous(),\n",
        "                                       decoder_states)\n",
        "        dec_attn = F.softmax(dec_energy, dim=0).transpose(0, 1)\n",
        "        dec_context = torch.bmm(decoder_states.permute(1, 2, 0), dec_attn)\n",
        "        combined[:, offset:offset + self.hidden_size] = dec_context.squeeze(2)\n",
        "      offset += self.hidden_size\n",
        "\n",
        "    if self.out_drop: combined = self.out_drop(combined)\n",
        "\n",
        "    # generator\n",
        "    if self.out_embed_size:\n",
        "      out_embed = self.pre_out(combined)\n",
        "    else:\n",
        "      out_embed = combined\n",
        "    logits = self.out(out_embed)  # (batch size, vocab size)\n",
        "\n",
        "    # pointer\n",
        "    if self.pointer:\n",
        "      output = torch.zeros(batch_size, ext_vocab_size, device=DEVICE)\n",
        "      # distribute probabilities between generator and pointer\n",
        "      prob_ptr = F.sigmoid(self.ptr(combined))  # (batch size, 1)\n",
        "      #prob_ptr = torch.sigmoid(self.ptr(combined))\n",
        "      prob_gen = 1 - prob_ptr\n",
        "      # add generator probabilities to output\n",
        "      gen_output = F.softmax(logits, dim=1)  # can't use log_softmax due to adding probabilities\n",
        "      output[:, :self.vocab_size] = prob_gen * gen_output\n",
        "      # add pointer probabilities to output\n",
        "      ptr_output = enc_attn\n",
        "      output.scatter_add_(1, encoder_word_idx.transpose(0, 1), prob_ptr * ptr_output)\n",
        "      if log_prob: output = torch.log(output + eps)\n",
        "    else:\n",
        "      if log_prob: output = F.log_softmax(logits, dim=1)\n",
        "      else: output = F.softmax(logits, dim=1)\n",
        "\n",
        "    return output, hidden, enc_attn, prob_ptr"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}