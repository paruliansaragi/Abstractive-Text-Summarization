{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ABS-0904-Fail.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paruliansaragi/Abstractive-Text-Summarization/blob/master/ABS_0904_Fail.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvIKko-de9ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip ./bbc-news-summary.zip\n",
        "!unrar x \"./BBC News Summary.rar\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKQmrm3Jehb-",
        "colab_type": "code",
        "outputId": "78851297-8092-4f45-9fda-5dd6816887f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n",
            "\r\u001b[K    8% |██▉                             | 10kB 14.8MB/s eta 0:00:01\r\u001b[K    17% |█████▊                          | 20kB 3.3MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 30kB 4.7MB/s eta 0:00:01\r\u001b[K    35% |███████████▌                    | 40kB 3.1MB/s eta 0:00:01\r\u001b[K    44% |██████████████▍                 | 51kB 3.8MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▏              | 61kB 4.5MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 71kB 5.1MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 81kB 5.8MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 92kB 6.4MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▊   | 102kB 5.1MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 112kB 5.1MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 122kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2018.1.10)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.128)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.0.1.post2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.18.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.6)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.128 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.128)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.6)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.128->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.128->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.128->boto3->pytorch-pretrained-bert) (1.11.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEO1gVh0Akw8",
        "colab_type": "code",
        "outputId": "8c88fdd9-85ef-4af0-ed94-7772b2e41dbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "!mkdir bert_pretrained_models\n",
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip -P bert_pretrained_models/;\n",
        "!unzip bert_pretrained_models/uncased_L-12_H-768_A-12.zip -d bert_pretrained_models/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-10 07:07:59--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 2607:f8b0:400e:c08::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘bert_pretrained_models/uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   182MB/s    in 2.1s    \n",
            "\n",
            "2019-04-10 07:08:02 (182 MB/s) - ‘bert_pretrained_models/uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  bert_pretrained_models/uncased_L-12_H-768_A-12.zip\n",
            "   creating: bert_pretrained_models/uncased_L-12_H-768_A-12/\n",
            "  inflating: bert_pretrained_models/uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: bert_pretrained_models/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: bert_pretrained_models/uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: bert_pretrained_models/uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: bert_pretrained_models/uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgoJ0zK3AnC3",
        "colab_type": "code",
        "outputId": "ad721ae3-2992-4cd7-a485-fc14c8243ffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "!test -d texar_repo || git clone https://github.com/asyml/texar.git texar_repo\n",
        "if not 'texar_repo' in sys.path:\n",
        "  sys.path += ['texar_repo']\n",
        "!pip install funcsigs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'texar_repo'...\n",
            "remote: Enumerating objects: 162, done.\u001b[K\n",
            "remote: Counting objects: 100% (162/162), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 11144 (delta 97), reused 129 (delta 84), pack-reused 10982\u001b[K\n",
            "Receiving objects: 100% (11144/11144), 2.59 MiB | 7.35 MiB/s, done.\n",
            "Resolving deltas: 100% (8471/8471), done.\n",
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Installing collected packages: funcsigs\n",
            "Successfully installed funcsigs-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGcW8p9UArvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import collections\n",
        "import sys\n",
        "from texar_repo.examples.bert.utils import data_utils, model_utils, tokenization\n",
        "import importlib\n",
        "import tensorflow as tf\n",
        "import texar as tx \n",
        "from texar_repo.examples.bert import config_classifier as config_downstream\n",
        "from texar_repo.texar.utils import transformer_utils\n",
        "from texar_repo.examples.transformer.utils import data_utils, utils\n",
        "from texar_repo.examples.transformer.bleu_tool import bleu_wrapper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS-VO_1jAtZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#config\n",
        "\n",
        "dcoder_config = {\n",
        "    'dim': 768,\n",
        "    'num_blocks': 6,\n",
        "    'multihead_attention': {\n",
        "        'num_heads': 8,\n",
        "        'output_dim': 768\n",
        "        # See documentation for more optional hyperparameters\n",
        "    },\n",
        "    'position_embedder_hparams': {\n",
        "        'dim': 768\n",
        "    },\n",
        "    'initializer': {\n",
        "        'type': 'variance_scaling_initializer',\n",
        "        'kwargs': {\n",
        "            'scale': 1.0,\n",
        "            'mode': 'fan_avg',\n",
        "            'distribution': 'uniform',\n",
        "        },\n",
        "    },\n",
        "    'poswise_feedforward': tx.modules.default_transformer_poswise_net_hparams(\n",
        "        output_dim=768)\n",
        "}\n",
        "\n",
        "loss_label_confidence = 0.9\n",
        "\n",
        "random_seed = 1234\n",
        "beam_width = 5\n",
        "alpha = 0.6\n",
        "hidden_dim = 768\n",
        "\n",
        "\n",
        "opt = {\n",
        "    'optimizer': {\n",
        "        'type': 'AdamOptimizer',\n",
        "        'kwargs': {\n",
        "            'beta1': 0.9,\n",
        "            'beta2': 0.997,\n",
        "            'epsilon': 1e-9\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "lr = {\n",
        "    'learning_rate_schedule': 'constant.linear_warmup.rsqrt_decay.rsqrt_depth',\n",
        "    'lr_constant': 2 * (hidden_dim ** -0.5),\n",
        "    'static_lr': 1e-3,\n",
        "    'warmup_steps': 2000,\n",
        "}\n",
        "\n",
        "bos_token_id =101\n",
        "eos_token_id = 102\n",
        "\n",
        "model_dir= \"./models\"\n",
        "run_mode= \"train_and_evaluate\"\n",
        "batch_size = 32\n",
        "test_batch_size = 32\n",
        "\n",
        "max_train_epoch = 20\n",
        "display_steps = 100\n",
        "eval_steps = 100000\n",
        "\n",
        "max_decoding_length = 400\n",
        "\n",
        "max_seq_length_src = 512\n",
        "max_seq_length_tgt = 400\n",
        "\n",
        "bert_pretrain_dir = 'bert_pretrained_models/uncased_L-12_H-768_A-12'\n",
        "#config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGQwbPaOuFRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample():\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence.\n",
        "                For single sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second\n",
        "                sequence. Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "                specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.src_txt = text_a\n",
        "        self.tgt_txt = text_b\n",
        "        \n",
        "class InputFeatures():\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, src_input_ids,src_input_mask,src_segment_ids,tgt_input_ids,tgt_input_mask,tgt_labels):\n",
        "        self.src_input_ids = src_input_ids\n",
        "        self.src_input_mask = src_input_mask\n",
        "        self.src_segment_ids = src_segment_ids\n",
        "        self.tgt_input_ids = tgt_input_ids\n",
        "        self.tgt_input_mask = tgt_input_mask \n",
        "        self.tgt_labels = tgt_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPdaem_TuODe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_single_example(ex_index, example, max_seq_length_src,max_seq_length_tgt,\n",
        "                           tokenizer):\n",
        "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "    \"\"\"\n",
        "    label_map = {}\n",
        "    for (i, label) in enumerate(label_list):\n",
        "        label_map[label] = i\n",
        "    \"\"\"\n",
        "    tokens_a = tokenizer.tokenize(example.src_txt)\n",
        "    tokens_b = tokenizer.tokenize(example.tgt_txt)\n",
        "\n",
        "\n",
        "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "    # length is less than the specified length.\n",
        "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "    if len(tokens_a) > max_seq_length_src - 2:\n",
        "            tokens_a = tokens_a[0:(max_seq_length_src - 2)]\n",
        "    \n",
        "    if len(tokens_b) > max_seq_length_tgt - 2:\n",
        "            tokens_b = tokens_b[0:(max_seq_length_tgt - 2)]\n",
        "\n",
        "    \n",
        "    tokens_src = []\n",
        "    segment_ids_src = []\n",
        "    tokens_src.append(\"[CLS]\")\n",
        "    segment_ids_src.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens_src.append(token)\n",
        "        segment_ids_src.append(0)\n",
        "    tokens_src.append(\"[SEP]\")\n",
        "    segment_ids_src.append(0)\n",
        "  \n",
        "\n",
        "    tokens_tgt = []\n",
        "    segment_ids_tgt = []\n",
        "    tokens_tgt.append(\"[CLS]\")\n",
        "    #segment_ids_tgt.append(0)\n",
        "    for token in tokens_b:\n",
        "        tokens_tgt.append(token)\n",
        "        #segment_ids_tgt.append(0)\n",
        "    tokens_tgt.append(\"[SEP]\")\n",
        "    #segment_ids_tgt.append(0)\n",
        "\n",
        "    input_ids_src = tokenizer.convert_tokens_to_ids(tokens_src)\n",
        "   \n",
        "    \n",
        "\n",
        "    input_ids_tgt = tokenizer.convert_tokens_to_ids(tokens_tgt)\n",
        "    \n",
        "    #Adding begiining and end token\n",
        "    input_ids_tgt = input_ids_tgt[:-1] \n",
        "    \n",
        "    input_mask_src = [1] * len(input_ids_src)\n",
        "\n",
        "\n",
        "    input_mask_tgt = [1] * len(input_ids_tgt)\n",
        "    \n",
        "    labels_tgt = input_ids_tgt[1:]\n",
        "    \n",
        "    \n",
        "    labels_tgt.append(0)\n",
        "    \n",
        "    #print(len(input_ids_tgt))\n",
        "    #print(len(input_mask_tgt))\n",
        "    #print(len(labels_tgt))\n",
        "    #print(len(segment_ids_tgt))\n",
        "    \n",
        "    while len(input_ids_src) < max_seq_length_src:\n",
        "        input_ids_src.append(0)\n",
        "        input_mask_src.append(0)\n",
        "        segment_ids_src.append(0)\n",
        "\n",
        "    while len(input_ids_tgt) < max_seq_length_tgt:\n",
        "        input_ids_tgt.append(0)\n",
        "        input_mask_tgt.append(0)\n",
        "        segment_ids_tgt.append(0)\n",
        "        labels_tgt.append(0)\n",
        "\n",
        "    feature = InputFeatures( src_input_ids=input_ids_src,src_input_mask=input_mask_src,src_segment_ids=segment_ids_src,\n",
        "        tgt_input_ids=input_ids_tgt,tgt_input_mask=input_mask_tgt,tgt_labels=labels_tgt)\n",
        "\n",
        "    \n",
        "    return feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcuiqehCuLgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with tf.gfile.Open(input_file, \"r\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            i = 0\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "        return lines\n",
        "\n",
        "    #Amended Texar here by adding a read file option to read text files\n",
        "    @classmethod\n",
        "    def _read_file(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a text file.\"\"\"\n",
        "        with tf.gfile.Open(input_file, \"r\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\n\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            i = 0\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "        return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quEzfQQkBgyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNDailymail(DataProcessor):\n",
        "    \"\"\"Processor for the CoLA data set (GLUE version).\"\"\"\n",
        "    \n",
        "    def get_article(self, art_file):\n",
        "        return self._read_file(art_file)\n",
        "      \n",
        "    def get_summary(self, summary_file):\n",
        "        return self._read_file(summary_file)\n",
        "      \n",
        "    def get_train_examples(self, summary, article):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        #Provide an article and a summary, read the file which returns a list of lines and create target and source in an example\n",
        "        \n",
        "        return self._create_examples(article, summary, \"train\")\n",
        "\n",
        "    def get_dev_examples(self, summary, article):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(article, summary, \"dev\")\n",
        "\n",
        "    def get_test_examples(self, summary, article):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(article, summary, \"test\")\n",
        "\n",
        "    def _create_examples(self, src_lines,tgt_lines,set_type):\n",
        "        examples = [] \n",
        "        for i,data in enumerate(zip(src_lines,tgt_lines)):\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            if set_type == \"test\" and i == 0:\n",
        "                continue\n",
        "            else:\n",
        "                #print(data)\n",
        "                if len(data[0])==0 or len(data[1])==0:\n",
        "                  continue\n",
        "                src_lines = tokenization.convert_to_unicode(data[0][0])\n",
        "                tgt_lines = tokenization.convert_to_unicode(data[1][0])\n",
        "                examples.append(InputExample(guid=guid, text_a=src_lines,\n",
        "                                         text_b=tgt_lines))\n",
        "        return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj68nsXRogXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
        "                                 tokenizer, output_mode):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 10000 == 0:\n",
        "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids: 0   0   0   0  0     0 0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens += tokens_b + [\"[SEP]\"]\n",
        "            segment_ids += [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        if output_mode == \"classification\":\n",
        "            label_id = label_map[example.label]\n",
        "        elif output_mode == \"regression\":\n",
        "            label_id = float(example.label)\n",
        "        else:\n",
        "            raise KeyError(output_mode)\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join(\n",
        "                    [str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\n",
        "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                              label_id=label_id))\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRjD723KCUwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def file_based_convert_examples_to_features(\n",
        "        examples, max_seq_length_src,max_seq_length_tgt,tokenizer, output_file):\n",
        "    \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
        "\n",
        "    writer = tf.python_io.TFRecordWriter(output_file)\n",
        "\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        #print(\"ex_index\",ex_index)\n",
        "\n",
        "        if (ex_index+1) %1000 == 0 :\n",
        "          print(\"------------processed..{}...examples\".format(ex_index))\n",
        "          \n",
        "        feature = convert_single_example(ex_index, example,\n",
        "                                         max_seq_length_src,max_seq_length_tgt,tokenizer)\n",
        "\n",
        "        def create_int_feature(values):\n",
        "            return tf.train.Feature(\n",
        "                int64_list=tf.train.Int64List(value=list(values)))\n",
        "\n",
        "        features = collections.OrderedDict()\n",
        "        features[\"src_input_ids\"] = create_int_feature(feature.src_input_ids)\n",
        "        features[\"src_input_mask\"] = create_int_feature(feature.src_input_mask)\n",
        "        features[\"src_segment_ids\"] = create_int_feature(feature.src_segment_ids)\n",
        "\n",
        "        features[\"tgt_input_ids\"] = create_int_feature(feature.tgt_input_ids)\n",
        "        features[\"tgt_input_mask\"] = create_int_feature(feature.tgt_input_mask)\n",
        "        features['tgt_labels'] = create_int_feature(feature.tgt_labels)\n",
        "        \n",
        "        \n",
        "        \n",
        "        #print(feature.tgt_labels)\n",
        "        \n",
        "\n",
        "        tf_example = tf.train.Example(\n",
        "            features=tf.train.Features(feature=features))\n",
        "        writer.write(tf_example.SerializeToString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqQNnhJ4B8y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def file_based_input_fn_builder(input_file, max_seq_length_src,max_seq_length_tgt, is_training,\n",
        "                                drop_remainder, is_distributed=False):\n",
        "    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "    name_to_features = {\n",
        "        \"src_input_ids\": tf.FixedLenFeature([max_seq_length_src], tf.int64),\n",
        "        \"src_input_mask\": tf.FixedLenFeature([max_seq_length_src], tf.int64),\n",
        "        \"src_segment_ids\": tf.FixedLenFeature([max_seq_length_src], tf.int64),\n",
        "        \"tgt_input_ids\": tf.FixedLenFeature([max_seq_length_tgt], tf.int64),\n",
        "        \"tgt_input_mask\": tf.FixedLenFeature([max_seq_length_tgt], tf.int64),\n",
        "        \"tgt_labels\" : tf.FixedLenFeature([max_seq_length_tgt], tf.int64),\n",
        "        \n",
        "        \n",
        "    }\n",
        "\n",
        "    def _decode_record(record, name_to_features):\n",
        "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "        example = tf.parse_single_example(record, name_to_features)\n",
        "        print(example)\n",
        "        print(example.keys())\n",
        "\n",
        "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "        # So cast all int64 to int32.\n",
        "        for name in list(example.keys()):\n",
        "            t = example[name]\n",
        "            if t.dtype == tf.int64:\n",
        "                t = tf.to_int32(t)\n",
        "            example[name] = t\n",
        "\n",
        "        return example\n",
        "\n",
        "    def input_fn(params):\n",
        "        \"\"\"The actual input function.\"\"\"\n",
        "        batch_size = params[\"batch_size\"]\n",
        "\n",
        "        # For training, we want a lot of parallel reading and shuffling.\n",
        "        # For eval, we want no shuffling and parallel reading doesn't matter.\n",
        "        d = tf.data.TFRecordDataset(input_file)\n",
        "        if is_training:\n",
        "\n",
        "            if is_distributed:\n",
        "                import horovod.tensorflow as hvd\n",
        "                tf.logging.info('distributed mode is enabled.'\n",
        "                                'size:{} rank:{}'.format(hvd.size(), hvd.rank()))\n",
        "                # https://github.com/uber/horovod/issues/223\n",
        "                d = d.shard(hvd.size(), hvd.rank())\n",
        "\n",
        "                d = d.repeat()\n",
        "                d = d.shuffle(buffer_size=100)\n",
        "                d = d.apply(\n",
        "                    tf.contrib.data.map_and_batch(\n",
        "                        lambda record: _decode_record(record, name_to_features),\n",
        "                        batch_size=batch_size//hvd.size(),\n",
        "                        drop_remainder=drop_remainder))\n",
        "            else:\n",
        "                tf.logging.info('distributed mode is not enabled.')\n",
        "                d = d.repeat()\n",
        "                d = d.shuffle(buffer_size=100)\n",
        "                d = d.apply(\n",
        "                    tf.contrib.data.map_and_batch(\n",
        "                        lambda record: _decode_record(record, name_to_features),\n",
        "                        batch_size=batch_size,\n",
        "                        drop_remainder=drop_remainder))\n",
        "\n",
        "        else:\n",
        "            d = d.apply(\n",
        "                tf.contrib.data.map_and_batch(\n",
        "                    lambda record: _decode_record(record, name_to_features),\n",
        "                    batch_size=batch_size,\n",
        "                    drop_remainder=drop_remainder))\n",
        "\n",
        "        return d\n",
        "    return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-KvNZc9D2lG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_config = model_utils.transform_bert_to_texar_config(\n",
        "            os.path.join(bert_pretrain_dir, 'bert_config.json'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AgKLkhuCd0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processor = CNNDailymail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqN2IHOZCzRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = tokenization.FullTokenizer(\n",
        "        vocab_file=os.path.join(bert_pretrain_dir, 'vocab.txt'),\n",
        "        do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu9JntKTC4vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(tokenizer.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKzFW074C7P1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "\n",
        "train_examples = None\n",
        "num_train_optimization_steps = None\n",
        "ddir = './BBC News Summary/'\n",
        "art_pol_dir = 'News Articles/politics'\n",
        "sum_pol_dir = 'Summaries/politics/'\n",
        "art_pol_path = glob.glob('./BBC News Summary/News Articles/politics/' +'*.txt')\n",
        "sum_pol_path = glob.glob('./BBC News Summary/Summaries/politics/' +'*.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp-q8oixs_hP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tech_art_path, tech_sum_path = glob.glob('./BBC News Summary/News Articles/tech/' +'*.txt'), glob.glob('./BBC News Summary/Summaries/tech/' +'*.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9FhuXnxEfm5",
        "colab_type": "code",
        "outputId": "41330cdf-9930-42d0-a443-96cef155d414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(tech_art_path), len(tech_sum_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(401, 401)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQKGVzSXJutT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for each filepath get the article\n",
        "trn_art = []\n",
        "for (i, path) in enumerate(tech_art_path):\n",
        "  trn_art.append( processor.get_article(path)  )\n",
        "#for each filepath get the article\n",
        "summ_art = []\n",
        "for (i, path) in enumerate(tech_sum_path):\n",
        "  summ_art.append( processor.get_summary(path)  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FcohAFhtUt_",
        "colab_type": "code",
        "outputId": "62093504-a262-4d53-dc61-6ef1e75fc7bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "summ_art[0], len(trn_art[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['\"The idea was simply to slow spammers\\' sites and this was achieved by the campaign,\" the company said.Lycos Europe\\'s Make Love, Not Spam campaign began in late November but its tactics proved controversial.Many security organisations said users should not participate in the Lycos Europe campaign.\"There is nothing to suggest that Make Love, Not Spam has brought down any of the sites that it has targeted,\" it said.Through the Make Love, Not Spam website, users could download a screensaver that would endlessly request data from the net sites mentioned in many junk mail messages.Some net service firms began blocking access to the Lycos Europe site in protest at the action.A contentious campaign to bump up the bandwidth bills of spammers by flooding their sites with data has been dropped.In a statement from Lycos Europe announcing the scrapping of the scheme, the company denied that this was its fault.']],\n",
              " 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBdbwMeIQ5nP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered = list(filter(None, trn_art[0])) # fastest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSMvuykKQ-XX",
        "colab_type": "code",
        "outputId": "f6437792-381e-4ef3-b651-86fc7340eae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "filtered"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Mobile games come of age'],\n",
              " ['The BBC News website takes a look at how games on mobile phones are maturing. A brief round-up follows but you can skip straight to the reviews by clicking on the links below.'],\n",
              " ['If you think of Snake when some mentions \"mobile games\" then you could be in for a bit of a surprise. This is because mobile games have come a long way in a very short time. Even before Nokia\\'s N-Gage game phone launched in late 2003, many mobile operators were realising that there was an audience looking for something to play on their handset.'],\n",
              " ['And given that many more people own handsets than own portable game playing gadgets such as the GameBoy it could be a very lucrative market. That audience includes commuters wanting something to fill their time on the way home, game fans looking for a bit of variety and hard core gamers who like to play every moment they can. Life for all these types of player has got immeasurably better in the last year as the numbers of titles you can download to your phone has snowballed. Now sites such as Wireless Gaming Review list more than 200 different titles for some UK networks and the ranges suit every possible taste. There are ports of PC and arcade classics such as Space Invaders, Lunar Lander and Bejewelled. There are also versions of titles, such as Colin McRae Rally, that you typically find on PCs and consoles.'],\n",
              " ['There are shoot-em-ups, adventure games, strategy titles and many novel games only found on handsets. Rarely now does an action movie launch without a mobile game tie-in. Increasingly such launches are all part of the promotional campaign for a film, understandable when you realise that a good game can rack up millions of downloads. The returns can be pretty good when you consider that some games cost £5. What has also helped games on mobiles thrive is the fact that it is easier than ever to get hold of them thanks to technology known as Wap push. By sending a text message to a game maker you can have the title downloaded to your handset. Far better than having to navigate through the menus of most mobile operator portals. The number of handsets that can play games has grown hugely too. Almost half of all phones now have Java onboard meaning that they can play the increasingly sophisticated games that are available - even the ones that use 3D graphics.'],\n",
              " ['The minimum technology specifications that phones should adhere to are getting more sophisticated which means that games are too. Now double key presses are possible making familiar tactics such as moving and strafing a real option. The processing power on handsets means that physics on mobile games is getting more convincing and the graphics are improving too. Some game makers are also starting to take advantage of the extra capabilities in a mobile. Many titles, particularly racing games, let you upload your best time to see how you compare to others. Usually you can get hold of their best time and race against a \"ghost\" or \"shadow\" to see if you can beat them. A few games also let you take on people in real time via the network or, if you are sitting close to them, via Bluetooth short-range radio technology. With so much going on it is hard to do justice to the sheer diversity of what is happening. But these two features should help point you in the direction of the game makers and give you an idea of where to look and how to get playing.'],\n",
              " [' TOO FAST TOO FURIOUS (DIGITAL BRIDGES) '],\n",
              " [\"As soon as I start playing this I remember why I never play driving games - because I'm rubbish at them. No matter if I drive the car via joystick or keypad I just cannot get the hang of braking for corners or timing a rush to pass other drivers. The game rewards replay because to advance you have to complete every section within a time limit. Winning gives you cash for upgrades. Graphically the rolling road is a convincing enough evocation of speed as the palm trees and cactus whip by and the city scrolls past in the background. The cars handle pretty well despite my uselessness but it was not clear if the different models of cars were appreciably different on the track. The only niggle was that the interface was a bit confusing especially when using a joystick rather than the keypad to play.\"],\n",
              " [' FATAL FORCE (MACROSPACE) '],\n",
              " ['A futuristic shooter that lets you either play various deathmatch modes against your phone or run through a series of scenarios that involves killing aliens invading Earth. Graphics are a bit cartoon-like but only helps to make clear what is going on and levels are well laid out and encourage you to leap about exploring. Both background music and sounds effects work well. The scenarios are well scripted and you regularly get hints from the Fatal Force commanders. Weapons include flamethrowers, rocket launchers, grenades and at a couple of points you even get chance to use a mech for a short while. With the right power-up you can go into a Matrix-style bullet time to cope with the onslaught of aliens. The game lets you play via Bluetooth if others are in range. Online the game has quite a following with clans, player rankings and even new downloadable maps.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOXKU0VJX9mZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_stuff(art_path, sum_path):\n",
        "  trn_art = []\n",
        "  for (i, path) in enumerate(art_pol_path):\n",
        "    trn_art.append( processor.get_article(path))\n",
        "    \n",
        "  trn_summ = []\n",
        "  for (i, path) in enumerate(sum_pol_path):\n",
        "    trn_summ.append( processor.get_summary(path))\n",
        "    \n",
        "  return trn_summ, trn_art"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZB4vS-9i1nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summ_art, trn_art = get_stuff(art_pol_path, sum_pol_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hI5Aa-zp74q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for (i, words) in enumerate(summ_art):\n",
        "  if i < 333:\n",
        "    trn_examples= processor.get_train_examples(summ_art[i], trn_art[i])\n",
        "  elif i > 333 < 375:\n",
        "    val_examples = processor.get_train_examples(summ_art[i], trn_art[i])\n",
        "  else:\n",
        "    test_examples = processor.get_train_examples(summ_art[i], trn_art[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfGAh0qbqpGT",
        "colab_type": "code",
        "outputId": "297e43be-afbf-4525-e6d2-41e242e7fa54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(trn_examples)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miaorgGCn3Hj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_examples = processor.get_train_examples(list(summ_art[:333]), list(trn_art[:333]))\n",
        "#val_examples = processor.get_test_examples(summ_art[333:375], trn_art[333:375])\n",
        "#test_examples = processor.get_dev_examples(summ_art[375:], trn_art[375:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGnKEVg4Z9qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tf_files(output_fold, splt_pct, test_split, length, trn_sum, trn_art):\n",
        "  #trn_arr, test_arr, val_arr = [], [], []\n",
        "  if not os.path.exists(output_fold):\n",
        "    os.makedirs(output_fold)\n",
        "  \n",
        "  \n",
        "  split_pct = int(length * splt_pct)\n",
        "  print('Splitting percentage {}'.format(split_pct))\n",
        "  test_split = int(length * test_split)\n",
        "  m, o = 0, 0\n",
        "  for (i, words) in enumerate(trn_art):\n",
        "    if i < length:\n",
        "      if i < split_pct:\n",
        "        trn_ex = processor.get_train_examples(trn_art[i], trn_sum[i])\n",
        "        file_based_convert_examples_to_features(\n",
        "            trn_ex, max_seq_length_src,max_seq_length_tgt,\n",
        "            tokenizer, \"{}train{}.tf_record\".format(output_fold,i))\n",
        "      elif i > test_split and m < test_split:\n",
        "        trn_ex = processor.get_train_examples(trn_art[i], trn_sum[i])\n",
        "        file_based_convert_examples_to_features(\n",
        "            trn_ex, max_seq_length_src,max_seq_length_tgt,\n",
        "            tokenizer, \"{}val{}.tf_record\".format(output_fold, m))\n",
        "        m += 1\n",
        "      else:\n",
        "        trn_ex = processor.get_train_examples(trn_art[i], trn_sum[i])\n",
        "        file_based_convert_examples_to_features(\n",
        "            trn_ex, max_seq_length_src,max_seq_length_tgt,\n",
        "            tokenizer, \"{}test{}.tf_record\".format(output_fold, o))\n",
        "        o += 1\n",
        "  return \n",
        "\n",
        "def create_single_tf_file(proc_exmp):\n",
        "  file_based_convert_examples_to_features(\n",
        "            proc_exmp, max_seq_length_src,max_seq_length_tgt,\n",
        "            tokenizer, \"train.tf_record\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz5wymLVsCyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_ = create_single_tf_file(trn_examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihRCn_Rph9YM",
        "colab_type": "code",
        "outputId": "0c0a9bfb-eaf2-40eb-ebcb-fd74e35ff64c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "_ = create_tf_files('tf_recs/', 0.8, 0.1, 10, summ_art, trn_art)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitting percentage 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjgiWntpjNXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r tf_recs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMXv3T-rKaU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trn_ex = []\n",
        "m = 0\n",
        "j = 0\n",
        "e = 0\n",
        "for i in range(5):\n",
        "  trn_ex = processor.get_train_examples(trn_art[i], summ_art[i])\n",
        "  file_based_convert_examples_to_features(\n",
        "    trn_ex, max_seq_length_src,max_seq_length_tgt,\n",
        "    tokenizer, \"train{}.tf_record\".format(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plUhV0caKxXP",
        "colab_type": "code",
        "outputId": "71d2a2eb-4a19-426a-fd12-3db34f6cccb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(trn_ex)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QonqAP-3G1E6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_examples = processor.get_train_examples(train_art, train_summ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5sMcEaxmoP6",
        "colab_type": "code",
        "outputId": "88b7865b-ed4a-43e4-e4d7-91cd434fad4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(train_examples)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grX_mqF_rOwC",
        "colab_type": "text"
      },
      "source": [
        "get train examples \n",
        "\n",
        "convert to a tf file\n",
        "\n",
        "use fn builder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi1bsyJtHYO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crt_exmpls(src_lines,tgt_lines,set_type):\n",
        "        examples = [] \n",
        "        for i,data in enumerate(zip(src_lines,tgt_lines)):\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            if set_type == \"test\" and i == 0:\n",
        "                continue\n",
        "            else:\n",
        "                #print(data)\n",
        "                if len(data[0])==0 or len(data[1])==0:\n",
        "                  continue\n",
        "                src_lines = tokenization.convert_to_unicode(data[0][0])\n",
        "                tgt_lines = tokenization.convert_to_unicode(data[1][0])\n",
        "                examples.append(InputExample(guid=guid, text_a=src_lines,\n",
        "                                         text_b=tgt_lines))\n",
        "        return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej91qT2uLKwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def file_based_input_fn_builder(input_file, max_seq_length_src,max_seq_length_tgt, is_training,\n",
        "                                drop_remainder, is_distributed=False):\n",
        "    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "    name_to_features = {\n",
        "        \"src_input_ids\": tf.FixedLenFeature([max_seq_length_src], tf.int64),\n",
        "        \"src_input_mask\": tf.FixedLenFeature([max_seq_length_src], tf.int64),\n",
        "        \"src_segment_ids\": tf.FixedLenFeature([max_seq_length_src], tf.int64),\n",
        "        \"tgt_input_ids\": tf.FixedLenFeature([max_seq_length_tgt], tf.int64),\n",
        "        \"tgt_input_mask\": tf.FixedLenFeature([max_seq_length_tgt], tf.int64),\n",
        "        \"tgt_labels\" : tf.FixedLenFeature([max_seq_length_tgt], tf.int64),\n",
        "        \n",
        "        \n",
        "    }\n",
        "\n",
        "    def _decode_record(record, name_to_features):\n",
        "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "        example = tf.parse_single_example(record, name_to_features)\n",
        "        print(example)\n",
        "        print(example.keys())\n",
        "\n",
        "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "        # So cast all int64 to int32.\n",
        "        for name in list(example.keys()):\n",
        "            t = example[name]\n",
        "            if t.dtype == tf.int64:\n",
        "                t = tf.to_int32(t)\n",
        "            example[name] = t\n",
        "\n",
        "        return example\n",
        "\n",
        "    def input_fn(params):\n",
        "        \"\"\"The actual input function.\"\"\"\n",
        "        batch_size = params[\"batch_size\"]\n",
        "\n",
        "        # For training, we want a lot of parallel reading and shuffling.\n",
        "        # For eval, we want no shuffling and parallel reading doesn't matter.\n",
        "        d = tf.data.TFRecordDataset(input_file)\n",
        "        if is_training:\n",
        "\n",
        "            if is_distributed:\n",
        "                import horovod.tensorflow as hvd\n",
        "                tf.logging.info('distributed mode is enabled.'\n",
        "                                'size:{} rank:{}'.format(hvd.size(), hvd.rank()))\n",
        "                # https://github.com/uber/horovod/issues/223\n",
        "                d = d.shard(hvd.size(), hvd.rank())\n",
        "\n",
        "                d = d.repeat()\n",
        "                d = d.shuffle(buffer_size=100)\n",
        "                d = d.apply(\n",
        "                    tf.contrib.data.map_and_batch(\n",
        "                        lambda record: _decode_record(record, name_to_features),\n",
        "                        batch_size=batch_size//hvd.size(),\n",
        "                        drop_remainder=drop_remainder))\n",
        "            else:\n",
        "                tf.logging.info('distributed mode is not enabled.')\n",
        "                d = d.repeat()\n",
        "                d = d.shuffle(buffer_size=100)\n",
        "                d = d.apply(\n",
        "                    tf.contrib.data.map_and_batch(\n",
        "                        lambda record: _decode_record(record, name_to_features),\n",
        "                        batch_size=batch_size,\n",
        "                        drop_remainder=drop_remainder))\n",
        "\n",
        "        else:\n",
        "            d = d.apply(\n",
        "                tf.contrib.data.map_and_batch(\n",
        "                    lambda record: _decode_record(record, name_to_features),\n",
        "                    batch_size=batch_size,\n",
        "                    drop_remainder=drop_remainder))\n",
        "\n",
        "        return d\n",
        "    return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toXZBPpVsP0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YolodkX9sKCf",
        "colab_type": "code",
        "outputId": "2c65e46e-c562-4bb1-e46b-5d414ad8d52a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "fake_ds = file_based_input_fn_builder(\n",
        "            input_file=\"./tf_recs/train0.tf_record\",\n",
        "            max_seq_length_src=max_seq_length_src,\n",
        "            max_seq_length_tgt =max_seq_length_tgt,\n",
        "            is_training=True,\n",
        "            drop_remainder=True,\n",
        "            is_distributed=False)({'batch_size': batch_size})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:distributed mode is not enabled.\n",
            "WARNING:tensorflow:From <ipython-input-27-9d1202a3964c>:63: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "{'src_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:0' shape=(512,) dtype=int64>, 'src_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:1' shape=(512,) dtype=int64>, 'src_segment_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:2' shape=(512,) dtype=int64>, 'tgt_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:3' shape=(400,) dtype=int64>, 'tgt_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:4' shape=(400,) dtype=int64>, 'tgt_labels': <tf.Tensor 'ParseSingleExample/ParseSingleExample:5' shape=(400,) dtype=int64>}\n",
            "dict_keys(['src_input_ids', 'src_input_mask', 'src_segment_ids', 'tgt_input_ids', 'tgt_input_mask', 'tgt_labels'])\n",
            "WARNING:tensorflow:From <ipython-input-27-9d1202a3964c>:27: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jxSc5k4DLRs",
        "colab_type": "code",
        "outputId": "9c067a14-4ee0-45b0-93aa-ae0c75e7dfba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "fake_val_ds = file_based_input_fn_builder(\n",
        "            input_file=\"./tf_recs/val0.tf_record\",\n",
        "            max_seq_length_src=max_seq_length_src,\n",
        "            max_seq_length_tgt =max_seq_length_tgt,\n",
        "            is_training=True,\n",
        "            drop_remainder=True,\n",
        "            is_distributed=False)({'batch_size': batch_size})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:distributed mode is not enabled.\n",
            "{'src_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:0' shape=(512,) dtype=int64>, 'src_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:1' shape=(512,) dtype=int64>, 'src_segment_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:2' shape=(512,) dtype=int64>, 'tgt_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:3' shape=(400,) dtype=int64>, 'tgt_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:4' shape=(400,) dtype=int64>, 'tgt_labels': <tf.Tensor 'ParseSingleExample/ParseSingleExample:5' shape=(400,) dtype=int64>}\n",
            "dict_keys(['src_input_ids', 'src_input_mask', 'src_segment_ids', 'tgt_input_ids', 'tgt_input_mask', 'tgt_labels'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwDIxWk-DPkQ",
        "colab_type": "code",
        "outputId": "3a6a017d-ea38-49cb-c00f-9f0f2646f509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "fake_test_ds = file_based_input_fn_builder(\n",
        "            input_file=\"./tf_recs/test0.tf_record\",\n",
        "            max_seq_length_src=max_seq_length_src,\n",
        "            max_seq_length_tgt =max_seq_length_tgt,\n",
        "            is_training=True,\n",
        "            drop_remainder=True,\n",
        "            is_distributed=False)({'batch_size': batch_size})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:distributed mode is not enabled.\n",
            "{'src_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:0' shape=(512,) dtype=int64>, 'src_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:1' shape=(512,) dtype=int64>, 'src_segment_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:2' shape=(512,) dtype=int64>, 'tgt_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:3' shape=(400,) dtype=int64>, 'tgt_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:4' shape=(400,) dtype=int64>, 'tgt_labels': <tf.Tensor 'ParseSingleExample/ParseSingleExample:5' shape=(400,) dtype=int64>}\n",
            "dict_keys(['src_input_ids', 'src_input_mask', 'src_segment_ids', 'tgt_input_ids', 'tgt_input_mask', 'tgt_labels'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgI7k2jjVg0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(5):\n",
        "  dataset = file_based_input_fn_builder(\n",
        "            input_file=\"train{}.tf_record\".format(i),\n",
        "            max_seq_length_src=max_seq_length_src,\n",
        "            max_seq_length_tgt =max_seq_length_tgt,\n",
        "            is_training=True,\n",
        "            drop_remainder=True,\n",
        "            is_distributed=False)({'batch_size': batch_size})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJx96erLYpxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tf_files(input_file):\n",
        "  dataset = file_based_input_fn_builder(\n",
        "                input_file=input_file,\n",
        "                max_seq_length_src=max_seq_length_src,\n",
        "                max_seq_length_tgt =max_seq_length_tgt,\n",
        "                is_training=True,\n",
        "                drop_remainder=True,\n",
        "                is_distributed=False)({'batch_size': batch_size})\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN1XNU3jkjBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(8):\n",
        "  train_dataset = get_tf_files(\"tf_recs/train{}.tf_record\".format(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1XV_6aLWDym",
        "colab_type": "code",
        "outputId": "f0035ba7-3a11-40fa-872b-779e9e0f7a8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "#inputs to the model\n",
        "src_input_ids = tf.placeholder(tf.int64, shape=(None, None))\n",
        "src_segment_ids = tf.placeholder(tf.int64, shape=(None, None))\n",
        "tgt_input_ids = tf.placeholder(tf.int64, shape=(None, None))\n",
        "tgt_segment_ids = tf.placeholder(tf.int64, shape=(None, None))\n",
        "\n",
        "batch_size = tf.shape(src_input_ids)[0]\n",
        "\n",
        "src_input_length = tf.reduce_sum(1 - tf.to_int32(tf.equal(src_input_ids, 0)),\n",
        "                             axis=1)\n",
        "tgt_input_length = tf.reduce_sum(1 - tf.to_int32(tf.equal(src_input_ids, 0)),\n",
        "                             axis=1)\n",
        "\n",
        "labels = tf.placeholder(tf.int64, shape=(None, None))\n",
        "is_target = tf.to_float(tf.not_equal(labels, 0))\n",
        "\n",
        "\n",
        "global_step = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
        "learning_rate = tf.placeholder(tf.float64, shape=(), name='lr')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-32-54824d78f153>:14: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpTCd7aWWH1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create the iterator \n",
        "iterator = tx.data.FeedableDataIterator({\n",
        "        'train': fake_ds, 'eval': fake_val_ds, 'test': fake_test_ds})\n",
        "\n",
        "batch = iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A4XOh7LDqpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encoder Bert model\n",
        "print(\"Intializing the Bert Encoder Graph\")\n",
        "with tf.variable_scope('bert'):\n",
        "        embedder = tx.modules.WordEmbedder(\n",
        "            vocab_size=bert_config.vocab_size,\n",
        "            hparams=bert_config.embed)\n",
        "        word_embeds = embedder(src_input_ids)\n",
        "\n",
        "        # Creates segment embeddings for each type of tokens.\n",
        "        segment_embedder = tx.modules.WordEmbedder(\n",
        "            vocab_size=bert_config.type_vocab_size,\n",
        "            hparams=bert_config.segment_embed)\n",
        "        segment_embeds = segment_embedder(src_segment_ids)\n",
        "\n",
        "        input_embeds = word_embeds + segment_embeds\n",
        "\n",
        "        # The BERT model (a TransformerEncoder)\n",
        "        encoder = tx.modules.TransformerEncoder(hparams=bert_config.encoder)\n",
        "        encoder_output = encoder(input_embeds, src_input_length)\n",
        "        \n",
        "        # Builds layers for downstream classification, which is also initialized\n",
        "        # with BERT pre-trained checkpoint.\n",
        "        with tf.variable_scope(\"pooler\"):\n",
        "            # Uses the projection of the 1st-step hidden vector of BERT output\n",
        "            # as the representation of the sentence\n",
        "            bert_sent_hidden = tf.squeeze(encoder_output[:, 0:1, :], axis=1)\n",
        "            bert_sent_output = tf.layers.dense(\n",
        "                bert_sent_hidden, config_downstream.hidden_dim,\n",
        "                activation=tf.tanh)\n",
        "            output = tf.layers.dropout(\n",
        "                bert_sent_output, rate=0.1, training=tx.global_mode_train())\n",
        "\n",
        "\n",
        "print(\"loading the bert pretrained weights\")\n",
        "# Loads pretrained BERT model parameters\n",
        "init_checkpoint = os.path.join(bert_pretrain_dir, 'bert_model.ckpt')\n",
        "#init_checkpoint = \"gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/bert_model.ckpt\"\n",
        "model_utils.init_bert_checkpoint(init_checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f-BbuKeDtDN",
        "colab_type": "code",
        "outputId": "ece5ab9c-e9a0-4a1f-9068-a9432b912e41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "#decoder part and mle losss\n",
        "tgt_embedding = tf.concat(\n",
        "    [tf.zeros(shape=[1, embedder.dim]), embedder.embedding[1:, :]], axis=0)\n",
        "\n",
        "decoder = tx.modules.TransformerDecoder(embedding=tgt_embedding,\n",
        "                             hparams=dcoder_config)\n",
        "# For training\n",
        "outputs = decoder(\n",
        "    memory=encoder_output,\n",
        "    memory_sequence_length=src_input_length,\n",
        "    inputs=embedder(tgt_input_ids),\n",
        "    sequence_length=tgt_input_length,\n",
        "    decoding_strategy='train_greedy',\n",
        "    mode=tf.estimator.ModeKeys.TRAIN\n",
        ")\n",
        "\n",
        "mle_loss = transformer_utils.smoothing_cross_entropy(\n",
        "        outputs.logits, labels, vocab_size, loss_label_confidence)\n",
        "mle_loss = tf.reduce_sum(mle_loss * is_target) / tf.reduce_sum(is_target)\n",
        "\n",
        "train_op = tx.core.get_train_op(\n",
        "        mle_loss,\n",
        "        learning_rate=learning_rate,\n",
        "        global_step=global_step,\n",
        "        hparams=opt)\n",
        "\n",
        "tf.summary.scalar('lr', learning_rate)\n",
        "tf.summary.scalar('mle_loss', mle_loss)\n",
        "summary_merged = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-a9fce4ecefb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m decoder = tx.modules.TransformerDecoder(embedding=tgt_embedding,\n\u001b[0;32m----> 5\u001b[0;31m                              hparams=dcoder_config)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# For training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m outputs = decoder(\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'embedding'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuuVLO5UFcAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}